{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ ÊîπËøõÁâàText-to-Image GAN - ÊâßË°åÊåáÂçó\n",
    "\n",
    "## üìã ÊåâÈ°∫Â∫èËøêË°å‰ª•‰∏ãcellsÔºö\n",
    "\n",
    "### Èò∂ÊÆµ1ÔºöÁéØÂ¢ÉÂíåÊï∞ÊçÆÂáÜÂ§á\n",
    "1. Cell 2 - ÊåÇËΩΩGoogle Drive\n",
    "2. Cell 3 - ÂØºÂÖ•Â∫ìÂíåGPUÈÖçÁΩÆ  \n",
    "3. Cell 4 - Âä†ËΩΩËØçÊ±áË°®\n",
    "4. Cell 5 - ÊñáÊú¨È¢ÑÂ§ÑÁêÜÂáΩÊï∞\n",
    "5. Cell 6 - Âä†ËΩΩËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "\n",
    "### Èò∂ÊÆµ2ÔºöÊï∞ÊçÆÂ¢ûÂº∫ÂíåÊï∞ÊçÆÈõÜ\n",
    "6. Cell 7 - ÂÆö‰πâÊï∞ÊçÆÁîüÊàêÂô®ÂíåÂ¢ûÂº∫ÂáΩÊï∞\n",
    "\n",
    "### Èò∂ÊÆµ3ÔºöÊîπËøõÁöÑÊ®°ÂûãÂÆö‰πâ\n",
    "7. Cell 8 - Êù°‰ª∂Â¢ûÂº∫ÂíåÊîπËøõÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®\n",
    "8. Cell 9 - ÊîπËøõÁöÑÁîüÊàêÂô®ÔºàÂ∏¶Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºâ\n",
    "9. Cell 10 - ÊîπËøõÁöÑÂà§Âà´Âô®ÔºàÂ∏¶Ëá™Ê≥®ÊÑèÂäõÔºâ\n",
    "10. Cell 11 - Ë∂ÖÂèÇÊï∞ÈÖçÁΩÆ\n",
    "11. Cell 12 - ÂàõÂª∫Êï∞ÊçÆÈõÜ\n",
    "\n",
    "### Èò∂ÊÆµ4ÔºöÊçüÂ§±ÂáΩÊï∞ÂíåÂàùÂßãÂåñ\n",
    "12. Cell 13 - ÂÆö‰πâÂ¢ûÂº∫ÁöÑÊçüÂ§±ÂáΩÊï∞\n",
    "13. Cell 14 - ÂàùÂßãÂåñÊîπËøõÁöÑÊ®°Âûã\n",
    "\n",
    "### Èò∂ÊÆµ5ÔºöËÆ≠ÁªÉ\n",
    "14. Cell 15 - ÂÆö‰πâÊîπËøõÁöÑËÆ≠ÁªÉÊ≠•È™§\n",
    "15. Cell 16 - ÂÆö‰πâÊîπËøõÁöÑËÆ≠ÁªÉÂæ™ÁéØ\n",
    "16. Cell 17 - ËæÖÂä©ÂáΩÊï∞ÔºàÁîüÊàêÂíå‰øùÂ≠òÂõæÂÉèÔºâ\n",
    "17. Cell 18 - ÂáÜÂ§áÊ†∑Êú¨Êï∞ÊçÆ\n",
    "18. Cell 19 - **üî• ÂºÄÂßãËÆ≠ÁªÉ**ÔºàÈ¢ÑËÆ°6-10Â∞èÊó∂Ôºâ\n",
    "\n",
    "### Èò∂ÊÆµ6ÔºöÂèØËßÜÂåñÂíåÊé®ÁêÜÔºàËÆ≠ÁªÉÂÆåÊàêÂêéÔºâ\n",
    "19. Cell 20 - ÂèØËßÜÂåñËÆ≠ÁªÉÂéÜÂè≤\n",
    "20. Cell 21 - ÂÆö‰πâÊîπËøõÁâàÊé®ÁêÜÂáΩÊï∞\n",
    "21. Cell 22 - Âä†ËΩΩÊµãËØïÊï∞ÊçÆ\n",
    "22. Cell 23 - Âä†ËΩΩÊúÄ‰Ω≥Ê®°Âûã\n",
    "23. Cell 24 - ËøêË°åÊé®ÁêÜÁîüÊàêÊµãËØïÂõæÂÉè\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ÂÖ≥ÈîÆÊîπËøõÔºö\n",
    "‚úÖ ËØçÁ∫ßÊ≥®ÊÑèÂäõÊú∫Âà∂  \n",
    "‚úÖ ÊÆãÂ∑ÆËøûÊé•ÂíåËá™Ê≥®ÊÑèÂäõÂ±Ç  \n",
    "‚úÖ ÁâπÂæÅÂåπÈÖçÊçüÂ§±  \n",
    "‚úÖ È¢úËâ≤‰∏ÄËá¥ÊÄßÂíåÂ§öÊ†∑ÊÄßÊ≠£ÂàôÂåñ  \n",
    "‚úÖ Ëá™Âä®‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã  \n",
    "‚úÖ Êó©ÂÅúÊú∫Âà∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-sY702fwXxY"
   },
   "source": [
    "# ÊîπËøõÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°Âûã (Text-to-Image GAN)\n",
    "\n",
    "## ‰∏ªË¶ÅÊîπËøõÁÇπ\n",
    "\n",
    "Êú¨notebookÂü∫‰∫éÊØîËµõPDF‰∏≠ÁöÑhintsËøõË°å‰∫Ü‰ª•‰∏ãÊîπËøõÔºö\n",
    "\n",
    "### 1. Êï∞ÊçÆÂ¢ûÂº∫ (Data Augmentation)\n",
    "- ÈöèÊú∫Ë£ÅÂâ™ÂíåÁøªËΩ¨\n",
    "- ‰∫ÆÂ∫¶ÂíåÂØπÊØîÂ∫¶Ë∞ÉÊï¥\n",
    "- È¢úËâ≤ÊäñÂä®\n",
    "- ÊèêÈ´òÊ®°ÂûãÊ≥õÂåñËÉΩÂäõÔºåÈò≤Ê≠¢ËøáÊãüÂêà\n",
    "\n",
    "### 2. ÊîπËøõÁöÑÊ®°ÂûãÊû∂ÊûÑ\n",
    "- **Text Encoder**: ‰ΩøÁî®ÂèåÂêëGRU + Â§öÂ±ÇRNNÁªìÊûÑÔºåÊõ¥Â•ΩÂú∞ÊçïÊçâÊñáÊú¨ËØ≠‰πâ\n",
    "- **Generator**: ÈááÁî®DCGANÊû∂ÊûÑÔºå‰ΩøÁî®ËΩ¨ÁΩÆÂç∑ÁßØÈÄêÊ≠•ÁîüÊàê64x64ÂõæÂÉè\n",
    "- **Discriminator**: CNNÊû∂ÊûÑ + Dropout + Batch NormalizationÔºåÊèêÈ´òÂà§Âà´ËÉΩÂäõ\n",
    "\n",
    "### 3. Êõ¥Â§çÊùÇÁöÑÊçüÂ§±ÂáΩÊï∞\n",
    "- Ê†áÁ≠æÂπ≥Êªë (Label Smoothing) - Èò≤Ê≠¢Âà§Âà´Âô®Ëøá‰∫éËá™‰ø°\n",
    "- WGAN-GPÈÄâÈ°π - Êõ¥Á®≥ÂÆöÁöÑËÆ≠ÁªÉÔºàÂèØÈÄâÔºâ\n",
    "- Ê¢ØÂ∫¶ÊÉ©ÁΩö (Gradient Penalty) - ÊîπÂñÑËÆ≠ÁªÉÁ®≥ÂÆöÊÄß\n",
    "\n",
    "### 4. ËÆ≠ÁªÉÊäÄÂ∑ß\n",
    "- Ê¢ØÂ∫¶Ë£ÅÂâ™ - Èò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏\n",
    "- Â≠¶‰π†ÁéáË∞ÉÂ∫¶ - Âä®ÊÄÅË∞ÉÊï¥Â≠¶‰π†Áéá\n",
    "- ÊúÄ‰Ω≥Ê®°Âûã‰øùÂ≠ò - ‰øùÂ≠òË°®Áé∞ÊúÄÂ•ΩÁöÑÊ®°Âûã\n",
    "- ËÆ≠ÁªÉÂéÜÂè≤ÂèØËßÜÂåñ - ÁõëÊéßËÆ≠ÁªÉËøáÁ®ã\n",
    "\n",
    "### 5. ÂèØÈÄâÁöÑÈ´òÁ∫ßÊäÄÊúØ\n",
    "- È¢ÑËÆ≠ÁªÉËØçÂµåÂÖ• (Word2Vec/GloVe)\n",
    "- Â§öÊ†∑ÊÄßÊ£ÄÊü• - Èò≤Ê≠¢Ê®°ÂºèÂ¥©Ê∫É\n",
    "- Êõ¥Âº∫ÁöÑÊï∞ÊçÆÂ¢ûÂº∫\n",
    "\n",
    "## ‰ΩøÁî®ËØ¥Êòé\n",
    "\n",
    "1. ÊåâÈ°∫Â∫èËøêË°åÊâÄÊúâcell\n",
    "2. ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰ºöÂÆöÊúü‰øùÂ≠òcheckpointÂíåÁîüÊàêÊ†∑Êú¨ÂõæÂÉè\n",
    "3. ÂèØ‰ª•Ë∞ÉÊï¥ `hparas` ‰∏≠ÁöÑË∂ÖÂèÇÊï∞ËøõË°åÂÆûÈ™å\n",
    "4. Â¶ÇÊûúËÆ≠ÁªÉ‰∏çÁ®≥ÂÆöÔºåÂèØ‰ª•Â∞ùËØïÂêØÁî®WGAN-GP (`USE_WGAN_GP = True`)\n",
    "\n",
    "## ÂèÇËÄÉËÆ∫Êñá\n",
    "- Generative Adversarial Text to Image Synthesis\n",
    "- DCGAN: Unsupervised Representation Learning with Deep Convolutional GANs\n",
    "- Improved Training of Wasserstein GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRB8j2wez71r",
    "outputId": "d0f3595f-2c94-4612-beb5-80d6ceffaf5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "islve_lE51yC"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KCVwdp456Ta",
    "outputId": "cc8b45f4-80c8-4b6c-c7fd-5c036d820356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ1GwGjS6AfM"
   },
   "source": [
    "Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-bLeJOw5__R",
    "outputId": "59d419a1-4f17-4940-ece1-aff69eb68641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuHfeMac70N2",
    "outputId": "c7bf0328-6477-4313-ff80-c569b0e6db2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "[np.str_('9'), np.str_('1'), np.str_('82'), np.str_('5'), np.str_('11'), np.str_('70'), np.str_('20'), np.str_('31'), np.str_('3'), np.str_('29'), np.str_('20'), np.str_('2'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427')]\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "\n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "\n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngI0VhKu756m"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rbp0zk-e8ABG",
    "outputId": "b9fd4368-dcc0-4c09-b698-345505fb04d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "FbeBceM58JlP",
    "outputId": "9406bf26-eb2a-4999-fc3f-9ceb0f1bd21c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7370,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2360,\n        \"min\": 1,\n        \"max\": 8188,\n        \"num_unique_values\": 7370,\n        \"samples\": [\n          2488,\n          3823,\n          5489\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Captions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ImagePath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7370,\n        \"samples\": [\n          \"./102flowers/image_02488.jpg\",\n          \"./102flowers/image_03823.jpg\",\n          \"./102flowers/image_05489.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ef17995b-8e37-4359-8bf2-a0a71eecfa73\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef17995b-8e37-4359-8bf2-a0a71eecfa73')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ef17995b-8e37-4359-8bf2-a0a71eecfa73 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ef17995b-8e37-4359-8bf2-a0a71eecfa73');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-ecaa2ddf-73fb-4e1d-9d95-3cbb5f2e0383\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ecaa2ddf-73fb-4e1d-9d95-3cbb5f2e0383')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-ecaa2ddf-73fb-4e1d-9d95-3cbb5f2e0383 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vq39KJ58Pke"
   },
   "source": [
    "Create Dataset by Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5LmYPj7o8RRX"
   },
   "outputs": [],
   "source": [
    "# ÊîπËøõÁöÑÊï∞ÊçÆÁîüÊàêÂô® - Ê∑ªÂä†Êï∞ÊçÆÂ¢ûÂº∫\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "\n",
    "    # Êï∞ÊçÆÂ¢ûÂº∫ - ÈöèÊú∫Ë£ÅÂâ™ÂíåÁøªËΩ¨\n",
    "    img = tf.image.resize(img, size=[72, 72])  # ÂÖàÊîæÂ§ß‰∏ÄÁÇπ\n",
    "    img = tf.image.random_crop(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption).astype(int)\n",
    "\n",
    "    # È°πÁõÆÊ†πÁõÆÂΩïÔºàÊúâ 102flowers ÈÇ£‰∏ÄÂ±ÇÁöÑÈÇ£‰∏™ÁõÆÂΩïÔºâ\n",
    "    PROJECT_ROOT = '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption'\n",
    "\n",
    "    # ÂéüÊù•ÁöÑ ImagePath ÈáåÂ§ßÊ¶ÇÁéáÊòØÂÉè \"./102flowers/image_06734.jpg\" ËøôÊ†∑ÁöÑÁõ∏ÂØπË∑ØÂæÑ\n",
    "    # ÊääÂÆÉÂèòÊàêÁªùÂØπË∑ØÂæÑ\n",
    "    image_path_series = df['ImagePath'].apply(\n",
    "        lambda p: os.path.join(PROJECT_ROOT, p.lstrip('./'))\n",
    "    )\n",
    "    image_path = image_path_series.values\n",
    "\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_z912VArJJrC",
    "outputId": "2a50f78b-0f99-43d5-873f-91c82c9aceb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\n",
      "6734    ./102flowers/image_06734.jpg\n",
      "6736    ./102flowers/image_06736.jpg\n",
      "6737    ./102flowers/image_06737.jpg\n",
      "6738    ./102flowers/image_06738.jpg\n",
      "6739    ./102flowers/image_06739.jpg\n",
      "Name: ImagePath, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['ImagePath'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "efazdiBj8YON"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0sS1Ukt9IVZ"
   },
   "source": [
    "Conditional GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= È´òÁ∫ß‰ºòÂåñË∂ÖÂèÇÊï∞ =============\n",
    "hparas_advanced = {\n",
    "    'MAX_SEQ_LENGTH': 20,\n",
    "    'EMBED_DIM': 512,                         # Â¢ûÂä†Âà∞512‰ª•Ëé∑ÂæóÊõ¥‰∏∞ÂØåÁöÑÊñáÊú¨Ë°®Á§∫\n",
    "    'VOCAB_SIZE': len(word2Id_dict),\n",
    "    'RNN_HIDDEN_SIZE': 512,                   # Â¢ûÂä†Âà∞512\n",
    "    'Z_DIM': 128,                             # Â¢ûÂä†Âô™Â£∞Áª¥Â∫¶Âà∞128\n",
    "    'TEXT_DIM': 256,                          # Êù°‰ª∂Â¢ûÂº∫ÂêéÁöÑÊñáÊú¨Áª¥Â∫¶\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'BATCH_SIZE': 32,                         # Èôç‰Ωébatch size‰ª•ÊîØÊåÅÊõ¥Â§ßÊ®°Âûã\n",
    "    'LR_G': 2e-4,\n",
    "    'LR_D': 2e-4,                             # ‰∏§ËÄÖ‰ΩøÁî®Áõ∏ÂêåÂ≠¶‰π†Áéá\n",
    "    'BETA_1': 0.5,\n",
    "    'BETA_2': 0.999,\n",
    "    'N_EPOCH': 300,                           # Â¢ûÂä†ËÆ≠ÁªÉËΩÆÊï∞\n",
    "    'N_SAMPLE': num_training_sample,\n",
    "    'CHECKPOINTS_DIR': '/content/drive/MyDrive/Colab Notebooks/dl/comp3/checkpoints/stackgan',\n",
    "    'PRINT_FREQ': 1,\n",
    "    'SAVE_FREQ': 10,\n",
    "    'LAMBDA_GP': 10,\n",
    "    'LAMBDA_PERCEPTUAL': 0.1,                 # ÊÑüÁü•ÊçüÂ§±ÊùÉÈáç\n",
    "    'LAMBDA_KL': 2.0,                         # Êù°‰ª∂Â¢ûÂº∫KLÊï£Â∫¶ÊùÉÈáç\n",
    "    'CA_GAMMA': 1.0,
    'LAMBDA_DAMSM': 10.0,                     # DAMSMÊçüÂ§±ÊùÉÈáç (ÂÖ≥ÈîÆ)                          # Êù°‰ª∂Â¢ûÂº∫ÂèÇÊï∞\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Êù°‰ª∂Â¢ûÂº∫ (Conditioning Augmentation) =============\n",
    "class ConditioningAugmentation(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ËÆ∫Êñá: Learning Deep Representations of Fine-Grained Visual Descriptions\n",
    "    Âú®ÊñáÊú¨ÂµåÂÖ•‰∏≠ÂºïÂÖ•ÈöèÊú∫ÊÄßÔºåÂ¢ûÂä†ÁîüÊàêÁöÑÂ§öÊ†∑ÊÄß\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, **kwargs):\n",
    "        super(ConditioningAugmentation, self).__init__(**kwargs)\n",
    "        self.text_dim = text_dim\n",
    "        self.fc_mu = tf.keras.layers.Dense(text_dim)\n",
    "        self.fc_sigma = tf.keras.layers.Dense(text_dim)\n",
    "    \n",
    "    def call(self, text_embedding, training=True):\n",
    "        mu = self.fc_mu(text_embedding)\n",
    "        log_sigma = self.fc_sigma(text_embedding)\n",
    "        \n",
    "        if training:\n",
    "            # ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ß\n",
    "            epsilon = tf.random.normal(shape=tf.shape(mu))\n",
    "            c_code = mu + tf.exp(log_sigma) * epsilon\n",
    "        else:\n",
    "            c_code = mu\n",
    "        \n",
    "        return c_code, mu, log_sigma\n",
    "\n",
    "# ============= ÊîπËøõÁöÑÊñáÊú¨ÁºñÁ†ÅÂô® (Hybrid CNN-RNN) =============\n",
    "class AdvancedTextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    ÁªìÂêàÂ≠óÁ¨¶Á∫ßCNNÂíåÂèåÂêëLSTMÔºåÊèêÂèñÊõ¥‰∏∞ÂØåÁöÑÊñáÊú¨ÁâπÂæÅ\n",
    "    ÂèÇËÄÉ: Learning Deep Representations of Fine-Grained Visual Descriptions\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(AdvancedTextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = hparas['BATCH_SIZE']\n",
    "        \n",
    "        # EmbeddingÂ±Ç\n",
    "        self.embedding = layers.Embedding(hparas['VOCAB_SIZE'], hparas['EMBED_DIM'])\n",
    "        self.dropout_embed = layers.Dropout(0.2)\n",
    "        \n",
    "        # ÂèåÂêëLSTMÔºà‰∏§Â±ÇÔºâ\n",
    "        self.bilstm1 = layers.Bidirectional(\n",
    "            layers.LSTM(hparas['RNN_HIDDEN_SIZE'], \n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.2,\n",
    "                       return_state=False)\n",
    "        )\n",
    "        \n",
    "        self.bilstm2 = layers.Bidirectional(\n",
    "            layers.LSTM(hparas['RNN_HIDDEN_SIZE'] // 2,\n",
    "                       return_sequences=False,\n",
    "                       recurrent_dropout=0.2)\n",
    "        )\n",
    "        \n",
    "        # Êù°‰ª∂Â¢ûÂº∫\n",
    "        self.ca = ConditioningAugmentation(hparas['TEXT_DIM'])\n",
    "        \n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "    \n",
    "    def call(self, text, training=True):\n",
    "        # Embedding\n",
    "        x = self.embedding(text)\n",
    "        x = self.dropout_embed(x, training=training)\n",
    "        \n",
    "        # ÂèåÂêëLSTMÂ§ÑÁêÜ\n",
    "        x = self.bilstm1(x, training=training)\n",
    "        sentence_embedding = self.bilstm2(x, training=training)\n",
    "        \n",
    "        # Êù°‰ª∂Â¢ûÂº∫\n",
    "        c_code, mu, log_sigma = self.ca(sentence_embedding, training=training)\n",
    "        c_code = self.dropout(c_code, training=training)\n",
    "        \n",
    "        return c_code, mu, log_sigma\n",
    "    \n",
    "    def kl_loss(self, mu, log_sigma):\n",
    "        \"\"\"KLÊï£Â∫¶ÊçüÂ§±ÔºåÁî®‰∫éÊ≠£ÂàôÂåñÊù°‰ª∂Â¢ûÂº∫\"\"\"\n",
    "        kl = -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu ** 2 - tf.exp(2 * log_sigma), axis=1)\n",
    "        return tf.reduce_mean(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊÆãÂ∑ÆÂùó (Áî®‰∫éÁîüÊàêÂô®ÂíåÂà§Âà´Âô®) =============\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"ÊÆãÂ∑ÆÂùóÊèêÂçáÁîüÊàêË¥®Èáè\"\"\"\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(filters, 3, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(filters, 3, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        residual = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        \n",
    "        x = x + residual\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# ============= Stage-I Generator (ÁîüÊàê64x64ÂõæÂÉè) =============\n",
    "class StageIGenerator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Stage-I: ÁîüÊàê64x64ÁöÑ‰ΩéÂàÜËæ®ÁéáÂõæÂÉè\n",
    "    ‰ΩøÁî®Êù°‰ª∂Â¢ûÂº∫ÁöÑÊñáÊú¨ÁâπÂæÅ\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(StageIGenerator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        ngf = 128  # generator feature mapÊï∞Èáè\n",
    "        \n",
    "        # ÊñáÊú¨ÂíåÂô™Â£∞ËûçÂêà\n",
    "        self.fc = layers.Dense(ngf * 8 * 4 * 4, use_bias=False)\n",
    "        self.bn_fc = layers.BatchNormalization()\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Â±Ç: 4x4 -> 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        self.upsample1 = layers.Conv2DTranspose(ngf * 4, 4, 2, padding='same', use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        self.upsample2 = layers.Conv2DTranspose(ngf * 2, 4, 2, padding='same', use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        self.upsample3 = layers.Conv2DTranspose(ngf, 4, 2, padding='same', use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        \n",
    "        self.upsample4 = layers.Conv2DTranspose(ngf // 2, 4, 2, padding='same', use_bias=False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        \n",
    "        # ËæìÂá∫Â±Ç\n",
    "        self.conv_out = layers.Conv2D(3, 3, padding='same')\n",
    "    \n",
    "    def call(self, text_embedding, noise, training=True):\n",
    "        # ËûçÂêàÊñáÊú¨ÂíåÂô™Â£∞\n",
    "        x = tf.concat([text_embedding, noise], axis=1)\n",
    "        \n",
    "        # ÂÖ®ËøûÊé•Â±Ç\n",
    "        x = self.fc(x)\n",
    "        x = self.bn_fc(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ÈáçÂ°ë‰∏∫4x4ÁâπÂæÅÂõæ\n",
    "        x = tf.reshape(x, [-1, 4, 4, self.hparas['RNN_HIDDEN_SIZE']])\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âà∞8x8\n",
    "        x = self.upsample1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âà∞16x16\n",
    "        x = self.upsample2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âà∞32x32\n",
    "        x = self.upsample3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âà∞64x64\n",
    "        x = self.upsample4(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ËæìÂá∫RGBÂõæÂÉè\n",
    "        x = self.conv_out(x)\n",
    "        output = tf.nn.tanh(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Stage-I Discriminator =============\n",
    "class StageIDiscriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Stage-IÂà§Âà´Âô®: Âà§Âà´64x64ÂõæÂÉèÁöÑÁúüÂÅá\n",
    "    ‰ΩøÁî®Ë∞±ÂΩí‰∏ÄÂåñÊèêÂçáËÆ≠ÁªÉÁ®≥ÂÆöÊÄß\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(StageIDiscriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        ndf = 64\n",
    "        \n",
    "        # ÂõæÂÉèÁºñÁ†Å: 64x64 -> 32x32 -> 16x16 -> 8x8 -> 4x4\n",
    "        self.conv1 = layers.Conv2D(ndf, 4, 2, padding='same')\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(ndf * 2, 4, 2, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(ndf * 4, 4, 2, padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv4 = layers.Conv2D(ndf * 8, 4, 2, padding='same')\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        \n",
    "        # ÊñáÊú¨ËûçÂêà\n",
    "        self.text_fc = layers.Dense(ndf * 8)\n",
    "        self.text_bn = layers.BatchNormalization()\n",
    "        \n",
    "        # ÊÆãÂ∑ÆÂùó\n",
    "        self.res_block = ResidualBlock(ndf * 8)\n",
    "        \n",
    "        # ËæìÂá∫Â±Ç\n",
    "        self.conv_out = layers.Conv2D(1, 4, 1, padding='valid')\n",
    "    \n",
    "    def call(self, image, text_embedding, training=True):\n",
    "        # ÂõæÂÉèÁâπÂæÅÊèêÂèñ\n",
    "        x = self.conv1(image)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "        \n",
    "        # ÊñáÊú¨ÁâπÂæÅÂ§ÑÁêÜ\n",
    "        text_feat = self.text_fc(text_embedding)\n",
    "        text_feat = self.text_bn(text_feat, training=training)\n",
    "        text_feat = tf.nn.leaky_relu(text_feat, 0.2)\n",
    "        \n",
    "        # Á©∫Èó¥Â§çÂà∂ÊñáÊú¨ÁâπÂæÅÂπ∂ËûçÂêà\n",
    "        text_feat = tf.reshape(text_feat, [-1, 1, 1, text_feat.shape[-1]])\n",
    "        text_feat = tf.tile(text_feat, [1, 4, 4, 1])\n",
    "        \n",
    "        # ËûçÂêàÂõæÂÉèÂíåÊñáÊú¨\n",
    "        x = tf.concat([x, text_feat], axis=-1)\n",
    "        x = layers.Conv2D(self.hparas['RNN_HIDDEN_SIZE'], 1)(x)\n",
    "        \n",
    "        # ÊÆãÂ∑ÆÂùó\n",
    "        x = self.res_block(x, training=training)\n",
    "        \n",
    "        # ËæìÂá∫\n",
    "        logits = self.conv_out(x)\n",
    "        logits = tf.squeeze(logits, [1, 2])\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Êõ¥Âº∫ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ =============\n",
    "def advanced_data_augmentation(caption, image_path):\n",
    "    \"\"\"\n",
    "    ÂÆûÁé∞hintÂª∫ËÆÆÁöÑÂº∫Â§ßÊï∞ÊçÆÂ¢ûÂº∫\n",
    "    ÂåÖÊã¨: ÈöèÊú∫Ë£ÅÂâ™„ÄÅÁøªËΩ¨„ÄÅÈ¢úËâ≤ÊäñÂä®„ÄÅÂØπÊØîÂ∫¶„ÄÅ‰∫ÆÂ∫¶„ÄÅÈ•±ÂíåÂ∫¶Ë∞ÉÊï¥\n",
    "    \"\"\"\n",
    "    # ËØªÂèñÂõæÂÉè\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    \n",
    "    # 1. ÈöèÊú∫Áº©Êîæ (Ê®°Êãü‰∏çÂêåË∑ùÁ¶ª)\n",
    "    scale = tf.random.uniform([], 0.8, 1.2)\n",
    "    new_size = tf.cast(tf.cast([64, 64], tf.float32) * scale, tf.int32)\n",
    "    img = tf.image.resize(img, new_size)\n",
    "    \n",
    "    # 2. ÈöèÊú∫Ë£ÅÂâ™Âà∞64x64\n",
    "    img = tf.image.resize_with_crop_or_pad(img, 80, 80)\n",
    "    img = tf.image.random_crop(img, [64, 64, 3])\n",
    "    \n",
    "    # 3. ÈöèÊú∫Ê∞¥Âπ≥ÁøªËΩ¨\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    # 4. È¢úËâ≤Â¢ûÂº∫\n",
    "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "    img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "    img = tf.image.random_hue(img, max_delta=0.1)\n",
    "    \n",
    "    # 5. ÈöèÊú∫ÁÅ∞Â∫¶Âåñ (5%Ê¶ÇÁéá)\n",
    "    if tf.random.uniform([]) < 0.05:\n",
    "        img = tf.image.rgb_to_grayscale(img)\n",
    "        img = tf.image.grayscale_to_rgb(img)\n",
    "    \n",
    "    # 6. Ê∑ªÂä†ËΩªÂæÆÂô™Â£∞\n",
    "    noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.02)\n",
    "    img = img + noise\n",
    "    \n",
    "    # Ë£ÅÂâ™Âà∞[0, 1]\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    img.set_shape([64, 64, 3])\n",
    "    \n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    return img, caption\n",
    "\n",
    "# Êõ¥Êñ∞Êï∞ÊçÆÈõÜÁîüÊàêÂô®‰ΩøÁî®Êñ∞ÁöÑÂ¢ûÂº∫\n",
    "def create_advanced_dataset(filenames, batch_size):\n",
    "    \"\"\"ÂàõÂª∫‰ΩøÁî®È´òÁ∫ßÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊï∞ÊçÆÈõÜ\"\"\"\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    \n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption).astype(int)\n",
    "    \n",
    "    PROJECT_ROOT = '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption'\n",
    "    image_path_series = df['ImagePath'].apply(\n",
    "        lambda p: os.path.join(PROJECT_ROOT, p.lstrip('./'))\n",
    "    )\n",
    "    image_path = image_path_series.values\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(advanced_data_augmentation, \n",
    "                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size, drop_remainder=True)  # Áº©Â∞èshuffleÁºìÂÜ≤Âå∫\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊîπËøõÁöÑÊçüÂ§±ÂáΩÊï∞ =============\n",
    "# 1. WGAN-GPÊçüÂ§±\n",
    "def wgan_d_loss(real_logits, fake_logits):\n",
    "    \"\"\"WGANÂà§Âà´Âô®ÊçüÂ§±\"\"\"\n",
    "    return tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n",
    "\n",
    "def wgan_g_loss(fake_logits):\n",
    "    \"\"\"WGANÁîüÊàêÂô®ÊçüÂ§±\"\"\"\n",
    "    return -tf.reduce_mean(fake_logits)\n",
    "\n",
    "# 2. Ê¢ØÂ∫¶ÊÉ©ÁΩö\n",
    "def compute_gradient_penalty(discriminator, real_images, fake_images, text_embedding):\n",
    "    \"\"\"WGAN-GPÁöÑÊ¢ØÂ∫¶ÊÉ©ÁΩö\"\"\"\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    \n",
    "    interpolated = alpha * real_images + (1 - alpha) * fake_images\n",
    "    \n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        pred_logits, _ = discriminator(interpolated, text_embedding, training=True)\n",
    "    \n",
    "    grads = gp_tape.gradient(pred_logits, interpolated)\n",
    "    grads_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "    gp = tf.reduce_mean((grads_norm - 1.0) ** 2)\n",
    "    \n",
    "    return gp\n",
    "\n",
    "# 3. ÊÑüÁü•ÊçüÂ§± (‰ΩøÁî®VGGÁâπÂæÅ)\n",
    "def build_vgg_model():\n",
    "    vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(64, 64, 3))\n",
    "    vgg.trainable = False\n",
    "    \n",
    "    # ÊèêÂèñÂ§öÂ±ÇÁâπÂæÅ\n",
    "    outputs = [vgg.get_layer('block2_conv2').output,\n",
    "               vgg.get_layer('block3_conv3').output,\n",
    "               vgg.get_layer('block4_conv3').output]\n",
    "    \n",
    "    model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def perceptual_loss(vgg_model, real_images, fake_images):\n",
    "    # Â∞ÜÂõæÂÉè‰ªé[-1, 1]ËΩ¨Êç¢Âà∞[0, 1]ÔºåÂÜçÂà∞VGGÈúÄË¶ÅÁöÑËåÉÂõ¥\n",
    "    real = (real_images + 1.0) * 127.5\n",
    "    fake = (fake_images + 1.0) * 127.5\n",
    "    \n",
    "    # VGGÈ¢ÑÂ§ÑÁêÜ\n",
    "    real = tf.keras.applications.vgg16.preprocess_input(real)\n",
    "    fake = tf.keras.applications.vgg16.preprocess_input(fake)\n",
    "    \n",
    "    # ÊèêÂèñÁâπÂæÅ\n",
    "    real_features = vgg_model(real, training=False)\n",
    "    fake_features = vgg_model(fake, training=False)\n",
    "    \n",
    "    # ËÆ°ÁÆóÂêÑÂ±ÇL2ÊçüÂ§±\n",
    "    loss = 0.0\n",
    "    for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "        loss += tf.reduce_mean(tf.square(real_feat - fake_feat))\n",
    "    \n",
    "    return loss / len(real_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= È´òÁ∫ßËÆ≠ÁªÉÊ≠•È™§ =============\n",
    "# Ê≥®ÊÑèÔºöÁßªÈô§@tf.function‰ª•Âä†Âø´È¶ñÊ¨°ÂêØÂä®ÈÄüÂ∫¶\n",
    "def advanced_train_step(real_image, caption, text_encoder, generator, discriminator, \n",
    "                       gen_optimizer, disc_optimizer, vgg_model, hparas):\n",
    "    \"\"\"\n",
    "    ÊîπËøõÁöÑËÆ≠ÁªÉÊ≠•È™§:\n",
    "    1. WGAN-GPÊçüÂ§±\n",
    "    2. Êù°‰ª∂Â¢ûÂº∫ + KLÊï£Â∫¶Ê≠£ÂàôÂåñ\n",
    "    3. ÊÑüÁü•ÊçüÂ§±\n",
    "    4. Ê¢ØÂ∫¶ÊÉ©ÁΩö\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal([hparas['BATCH_SIZE'], hparas['Z_DIM']])\n",
    "    \n",
    "    # ===== ËÆ≠ÁªÉÂà§Âà´Âô® =====\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # ÁºñÁ†ÅÊñáÊú¨ÔºàÂ∏¶Êù°‰ª∂Â¢ûÂº∫Ôºâ\n",
    "        text_embedding, mu, log_sigma = text_encoder(caption, training=True)\n",
    "        \n",
    "        # ÁîüÊàêÂÅáÂõæÂÉè\n",
    "        fake_image = generator(text_embedding, noise, training=True)\n",
    "        \n",
    "        # Âà§Âà´Âô®Âà§Êñ≠\n",
    "        real_logits, _ = discriminator(real_image, text_embedding, training=True)\n",
    "        fake_logits, _ = discriminator(fake_image, text_embedding, training=True)\n",
    "        \n",
    "        # WGANÊçüÂ§±\n",
    "        d_loss = wgan_d_loss(real_logits, fake_logits)\n",
    "        \n",
    "        # Ê¢ØÂ∫¶ÊÉ©ÁΩö\n",
    "        gp = compute_gradient_penalty(discriminator, real_image, fake_image, text_embedding)\n",
    "        d_loss = d_loss + hparas['LAMBDA_GP'] * gp\n",
    "    \n",
    "    # Êõ¥Êñ∞Âà§Âà´Âô®\n",
    "    disc_grads = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    disc_grads, _ = tf.clip_by_global_norm(disc_grads, 5.0)\n",
    "    disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
    "    \n",
    "    # ===== ËÆ≠ÁªÉÁîüÊàêÂô® =====\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # ÈáçÊñ∞ÁºñÁ†ÅÊñáÊú¨\n",
    "        text_embedding, mu, log_sigma = text_encoder(caption, training=True)\n",
    "        \n",
    "        # ÁîüÊàêÂÅáÂõæÂÉè\n",
    "        fake_image = generator(text_embedding, noise, training=True)\n",
    "        \n",
    "        # Âà§Âà´Âô®Âà§Êñ≠\n",
    "        fake_logits, _ = discriminator(fake_image, text_embedding, training=True)\n",
    "        \n",
    "        # WGANÁîüÊàêÂô®ÊçüÂ§±\n",
    "        g_loss = wgan_g_loss(fake_logits)\n",
    "        \n",
    "        # Êù°‰ª∂Â¢ûÂº∫KLÊï£Â∫¶ÊçüÂ§±\n",
    "        kl_loss = text_encoder.kl_loss(mu, log_sigma)\n",
    "        \n",
    "        # ÊÑüÁü•ÊçüÂ§±\n",
    "        p_loss = perceptual_loss(vgg_model, real_image, fake_image)\n",
    "        \n",
    "        # ÊÄªÊçüÂ§±\n",
    "        total_g_loss = g_loss + hparas['LAMBDA_KL'] * kl_loss + hparas['LAMBDA_PERCEPTUAL'] * p_loss\n",
    "    \n",
    "    # Êõ¥Êñ∞ÁîüÊàêÂô®ÂíåÊñáÊú¨ÁºñÁ†ÅÂô®\n",
    "    gen_vars = generator.trainable_variables + text_encoder.trainable_variables\n",
    "    gen_grads = gen_tape.gradient(total_g_loss, gen_vars)\n",
    "    gen_grads, _ = tf.clip_by_global_norm(gen_grads, 5.0)\n",
    "    gen_optimizer.apply_gradients(zip(gen_grads, gen_vars))\n",
    "    \n",
    "    return d_loss, g_loss, kl_loss, p_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Ê≥®ÊÑèÂäõÊú∫Âà∂ =============\n",
    "class WordAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ÂçïËØçÁ∫ßÊ≥®ÊÑèÂäõÊú∫Âà∂ - ËÆ©ÁîüÊàêÂô®ÂÖ≥Ê≥®ÊñáÊú¨‰∏≠ÁöÑÂÖ≥ÈîÆÈÉ®ÂàÜ\n",
    "    ÂèÇËÄÉÔºöAttnGAN: Fine-Grained Text to Image Generation with Attentional GAN\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, hidden_dim):\n",
    "        super(WordAttention, self).__init__()\n",
    "        self.text_dim = text_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # ÊäïÂΩ±Â±Ç\n",
    "        self.text_proj = layers.Dense(hidden_dim)\n",
    "        self.context_proj = layers.Dense(hidden_dim)\n",
    "        self.attention_proj = layers.Dense(1)\n",
    "        \n",
    "    def call(self, word_features, context_vector, training=True):\n",
    "        \"\"\"\n",
    "        word_features: (batch, seq_len, text_dim) - ÊØè‰∏™ÂçïËØçÁöÑÁâπÂæÅ\n",
    "        context_vector: (batch, hidden_dim) - ÂÖ®Â±Ä‰∏ä‰∏ãÊñáÂêëÈáè\n",
    "        \"\"\"\n",
    "        # ÊäïÂΩ±ÊñáÊú¨ÁâπÂæÅ (batch, seq_len, hidden_dim)\n",
    "        text_proj = self.text_proj(word_features)\n",
    "        \n",
    "        # ÊäïÂΩ±‰∏ä‰∏ãÊñáÂêëÈáèÂπ∂Êâ©Â±ïÁª¥Â∫¶ (batch, 1, hidden_dim)\n",
    "        context_proj = self.context_proj(context_vector)\n",
    "        context_proj = tf.expand_dims(context_proj, axis=1)\n",
    "        \n",
    "        # ËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞\n",
    "        # (batch, seq_len, hidden_dim) + (batch, 1, hidden_dim) -> (batch, seq_len, hidden_dim)\n",
    "        combined = tf.nn.tanh(text_proj + context_proj)\n",
    "        \n",
    "        # (batch, seq_len, 1)\n",
    "        attention_scores = self.attention_proj(combined)\n",
    "        \n",
    "        # SoftmaxÂΩí‰∏ÄÂåñ (batch, seq_len, 1)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "        \n",
    "        # Âä†ÊùÉÊ±ÇÂíå (batch, text_dim)\n",
    "        attended_text = tf.reduce_sum(word_features * attention_weights, axis=1)\n",
    "        \n",
    "        return attended_text, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊîπËøõÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®ÔºàÂ∏¶Ê≥®ÊÑèÂäõÔºâ =============\n",
    "class ImprovedTextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    ÊîπËøõÁöÑÊñáÊú¨ÁºñÁ†ÅÂô®Ôºö\n",
    "    1. ÂèåÂêëLSTMÊèêÂèñÂ∫èÂàóÁâπÂæÅ\n",
    "    2. ËæìÂá∫ÊØè‰∏™ÂçïËØçÁöÑÁâπÂæÅÔºàÁî®‰∫éÊ≥®ÊÑèÂäõÔºâ\n",
    "    3. ËæìÂá∫ÂÖ®Â±ÄÂè•Â≠êÁâπÂæÅ\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(ImprovedTextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = hparas['BATCH_SIZE']\n",
    "        \n",
    "        # EmbeddingÂ±Ç\n",
    "        self.embedding = layers.Embedding(hparas['VOCAB_SIZE'], hparas['EMBED_DIM'])\n",
    "        self.dropout_embed = layers.Dropout(0.2)\n",
    "        \n",
    "        # ÂèåÂêëLSTMÔºàËæìÂá∫Â∫èÂàóÂíåÊúÄÁªàÁä∂ÊÄÅÔºâ\n",
    "        self.bilstm = layers.Bidirectional(\n",
    "            layers.LSTM(hparas['RNN_HIDDEN_SIZE'],\n",
    "                       return_sequences=True,  # ËøîÂõûÊØè‰∏™Êó∂Èó¥Ê≠•\n",
    "                       return_state=True,      # ËøîÂõûÊúÄÁªàÁä∂ÊÄÅ\n",
    "                       recurrent_dropout=0.2)\n",
    "        )\n",
    "        \n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        \n",
    "    def call(self, text, training=True):\n",
    "        # Embedding (batch, seq_len, embed_dim)\n",
    "        x = self.embedding(text)\n",
    "        x = self.dropout_embed(x, training=training)\n",
    "        \n",
    "        # ÂèåÂêëLSTM\n",
    "        # word_features: (batch, seq_len, hidden_dim*2)\n",
    "        # states: [forward_h, forward_c, backward_h, backward_c]\n",
    "        outputs = self.bilstm(x, training=training)\n",
    "        word_features = outputs[0]  # ÊØè‰∏™ÂçïËØçÁöÑÁâπÂæÅ\n",
    "        \n",
    "        # ‰ΩøÁî®ÂâçÂêëÂíåÂêéÂêëÁöÑÊúÄÁªàÈöêËóèÁä∂ÊÄÅ‰Ωú‰∏∫Âè•Â≠êË°®Á§∫\n",
    "        forward_h = outputs[1]\n",
    "        backward_h = outputs[3]\n",
    "        sentence_feature = tf.concat([forward_h, backward_h], axis=-1)\n",
    "        \n",
    "        sentence_feature = self.dropout(sentence_feature, training=training)\n",
    "        \n",
    "        return word_features, sentence_feature\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.hparas['RNN_HIDDEN_SIZE'] * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊÆãÂ∑ÆÂùóÔºàResidual BlockÔºâ =============\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ÊÆãÂ∑ÆÂùó - Â∏ÆÂä©Ê¢ØÂ∫¶ÊµÅÂä®Ôºå‰ΩøÁΩëÁªúÊõ¥Ê∑±\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(channels, 3, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(channels, 3, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out, training=training)\n",
    "        out = tf.nn.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out, training=training)\n",
    "        \n",
    "        # ÊÆãÂ∑ÆËøûÊé•\n",
    "        out = out + residual\n",
    "        out = tf.nn.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# ============= Ëá™Ê≥®ÊÑèÂäõÂ±ÇÔºàSelf-AttentionÔºâ =============\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Ëá™Ê≥®ÊÑèÂäõÂ±Ç - ËÆ©ÁîüÊàêÂô®ÂÖ≥Ê≥®ÂõæÂÉèÁöÑ‰∏çÂêåÂå∫Âüü\n",
    "    ÂèÇËÄÉÔºöSelf-Attention GAN (SAGAN)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        \n",
    "        # 1x1Âç∑ÁßØÁî®‰∫éÈôçÁª¥\n",
    "        self.query_conv = layers.Conv2D(channels // 8, 1)\n",
    "        self.key_conv = layers.Conv2D(channels // 8, 1)\n",
    "        self.value_conv = layers.Conv2D(channels, 1)\n",
    "        \n",
    "        # ÂèØÂ≠¶‰π†ÁöÑÁº©ÊîæÂèÇÊï∞\n",
    "        self.gamma = tf.Variable(0.0, trainable=True, dtype=tf.float32)\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        \n",
    "        # Query: (batch, h*w, c//8)\n",
    "        query = self.query_conv(x)\n",
    "        query = tf.reshape(query, [batch_size, -1, self.channels // 8])\n",
    "        \n",
    "        # Key: (batch, c//8, h*w)\n",
    "        key = self.key_conv(x)\n",
    "        key = tf.reshape(key, [batch_size, -1, self.channels // 8])\n",
    "        key = tf.transpose(key, [0, 2, 1])\n",
    "        \n",
    "        # Attention map: (batch, h*w, h*w)\n",
    "        attention = tf.matmul(query, key)\n",
    "        attention = tf.nn.softmax(attention, axis=-1)\n",
    "        \n",
    "        # Value: (batch, h*w, c)\n",
    "        value = self.value_conv(x)\n",
    "        value = tf.reshape(value, [batch_size, -1, channels])\n",
    "        \n",
    "        # Â∫îÁî®Ê≥®ÊÑèÂäõ (batch, h*w, c)\n",
    "        out = tf.matmul(attention, value)\n",
    "        out = tf.reshape(out, [batch_size, height, width, channels])\n",
    "        \n",
    "        # ÊÆãÂ∑ÆËøûÊé•\n",
    "        out = self.gamma * out + x\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊîπËøõÁöÑÁîüÊàêÂô®ÔºàÂ∏¶Ê≥®ÊÑèÂäõÂíåÊÆãÂ∑ÆËøûÊé•Ôºâ =============\n",
    "class ImprovedGenerator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    ÊîπËøõÁöÑÁîüÊàêÂô®Ôºö\n",
    "    1. ËØçÁ∫ßÊ≥®ÊÑèÂäõÊú∫Âà∂\n",
    "    2. ÊÆãÂ∑ÆÂùó\n",
    "    3. Ëá™Ê≥®ÊÑèÂäõÂ±Ç\n",
    "    4. Êõ¥Ê∑±ÁöÑÁΩëÁªúÁªìÊûÑ\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(ImprovedGenerator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # ËØçÁ∫ßÊ≥®ÊÑèÂäõ\n",
    "        self.word_attention = WordAttention(\n",
    "            text_dim=hparas['RNN_HIDDEN_SIZE'] * 2,\n",
    "            hidden_dim=256\n",
    "        )\n",
    "        \n",
    "        # ÊñáÊú¨ÁâπÂæÅËûçÂêà\n",
    "        self.text_fc = layers.Dense(512)\n",
    "        self.text_bn = layers.BatchNormalization()\n",
    "        \n",
    "        # ÂàùÂßãÂÖ®ËøûÊé•Â±ÇÔºöÂô™Â£∞ + ÊñáÊú¨ -> ÁâπÂæÅÂõæ\n",
    "        # ‰øÆÂæ©Ôºö‰ΩøÁî® 512 (text_fc Ëº∏Âá∫) ËÄå‰∏çÊòØ RNN_HIDDEN_SIZE * 2\n",
    "        noise_text_dim = hparas['Z_DIM'] + 512\n",
    "        self.fc1 = layers.Dense(4 * 4 * 512, use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 1: 4x4 -> 8x8\n",
    "        self.deconv1 = layers.Conv2DTranspose(512, 4, strides=2, padding='same', use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.res1 = ResidualBlock(512)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 2: 8x8 -> 16x16\n",
    "        self.deconv2 = layers.Conv2DTranspose(256, 4, strides=2, padding='same', use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.res2 = ResidualBlock(256)\n",
    "        \n",
    "        # Ëá™Ê≥®ÊÑèÂäõÂ±ÇÔºàÂú®16x16ÂàÜËæ®ÁéáÔºâ\n",
    "        self.self_attn = SelfAttention(256)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 3: 16x16 -> 32x32\n",
    "        self.deconv3 = layers.Conv2DTranspose(128, 4, strides=2, padding='same', use_bias=False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 4: 32x32 -> 64x64\n",
    "        self.deconv4 = layers.Conv2DTranspose(64, 4, strides=2, padding='same', use_bias=False)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        \n",
    "        # ÊúÄÁªàÂç∑ÁßØÂ±Ç\n",
    "        self.final_conv = layers.Conv2D(3, 3, padding='same')\n",
    "        \n",
    "    def call(self, word_features, sentence_feature, noise, training=True):\n",
    "        \"\"\"\n",
    "        word_features: (batch, seq_len, hidden_dim*2) - ÊØè‰∏™ÂçïËØçÁöÑÁâπÂæÅ\n",
    "        sentence_feature: (batch, hidden_dim*2) - ÂÖ®Â±ÄÂè•Â≠êÁâπÂæÅ\n",
    "        noise: (batch, z_dim) - ÈöèÊú∫Âô™Â£∞\n",
    "        \"\"\"\n",
    "        # Â∫îÁî®ËØçÁ∫ßÊ≥®ÊÑèÂäõ\n",
    "        attended_text, attention_weights = self.word_attention(\n",
    "            word_features, sentence_feature, training=training\n",
    "        )\n",
    "        \n",
    "        # ËûçÂêàÊ≥®ÊÑèÂäõÁâπÂæÅÂíåÂè•Â≠êÁâπÂæÅ\n",
    "        text_feature = attended_text + sentence_feature\n",
    "        text_feature = self.text_fc(text_feature)\n",
    "        text_feature = self.text_bn(text_feature, training=training)\n",
    "        text_feature = tf.nn.relu(text_feature)\n",
    "        \n",
    "        # ËøûÊé•Âô™Â£∞ÂíåÊñáÊú¨ÁâπÂæÅÔºà‰øÆÂæ©Ôºö‰ΩøÁî®ËûçÂêàÂæåÁöÑ text_feature ËÄå‰∏çÊòØ sentence_featureÔºâ\n",
    "        x = tf.concat([noise, text_feature], axis=1)\n",
    "        \n",
    "        # ÂÖ®ËøûÊé•Â±Ç\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ÈáçÂ°ë‰∏∫ÁâπÂæÅÂõæ (batch, 4, 4, 512)\n",
    "        x = tf.reshape(x, [-1, 4, 4, 512])\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 1: 4x4 -> 8x8\n",
    "        x = self.deconv1(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.res1(x, training=training)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 2: 8x8 -> 16x16\n",
    "        x = self.deconv2(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        \n",
    "        # Ëá™Ê≥®ÊÑèÂäõ\n",
    "        x = self.self_attn(x, training=training)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 3: 16x16 -> 32x32\n",
    "        x = self.deconv3(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        \n",
    "        # ‰∏äÈááÊ†∑Âùó 4: 32x32 -> 64x64\n",
    "        x = self.deconv4(x)\n",
    "        x = self.bn5(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # ÊúÄÁªàËæìÂá∫\n",
    "        x = self.final_conv(x)\n",
    "        output = tf.nn.tanh(x)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊîπËøõÁöÑÂà§Âà´Âô®ÔºàÂ∏¶Ëá™Ê≥®ÊÑèÂäõÔºâ =============\n",
    "class ImprovedDiscriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    ÊîπËøõÁöÑÂà§Âà´Âô®Ôºö\n",
    "    1. Êõ¥Ê∑±ÁöÑÂç∑ÁßØÁΩëÁªú\n",
    "    2. Ëá™Ê≥®ÊÑèÂäõÂ±Ç\n",
    "    3. Ë∞±ÂΩí‰∏ÄÂåñÔºàÂèØÈÄâÔºâ\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(ImprovedDiscriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # ÂõæÂÉèÂç∑ÁßØÂ±Ç\n",
    "        # 64x64 -> 32x32\n",
    "        self.conv1 = layers.Conv2D(64, 4, strides=2, padding='same')\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        \n",
    "        # 32x32 -> 16x16\n",
    "        self.conv2 = layers.Conv2D(128, 4, strides=2, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(0.3)\n",
    "        \n",
    "        # Ëá™Ê≥®ÊÑèÂäõÂ±ÇÔºàÂú®16x16ÂàÜËæ®ÁéáÔºâ\n",
    "        self.self_attn = SelfAttention(128)\n",
    "        \n",
    "        # 16x16 -> 8x8\n",
    "        self.conv3 = layers.Conv2D(256, 4, strides=2, padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.dropout3 = layers.Dropout(0.3)\n",
    "        \n",
    "        # 8x8 -> 4x4\n",
    "        self.conv4 = layers.Conv2D(512, 4, strides=2, padding='same')\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.dropout4 = layers.Dropout(0.3)\n",
    "        \n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        # ÊñáÊú¨Â§ÑÁêÜÂ±Ç\n",
    "        self.text_fc = layers.Dense(512)\n",
    "        self.text_bn = layers.BatchNormalization()\n",
    "        \n",
    "        # ËûçÂêàÂ±Ç\n",
    "        self.fc1 = layers.Dense(1024)\n",
    "        self.fc2 = layers.Dense(1)\n",
    "        \n",
    "    def call(self, img, text, training=True):\n",
    "        # Â§ÑÁêÜÂõæÂÉè\n",
    "        x = self.conv1(img)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        \n",
    "        # Ëá™Ê≥®ÊÑèÂäõ\n",
    "        x = self.self_attn(x, training=training)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout3(x, training=training)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout4(x, training=training)\n",
    "        \n",
    "        # ÊèêÂèñÂõæÂÉèÁâπÂæÅÔºàÁî®‰∫éÁâπÂæÅÂåπÈÖçÔºâ\n",
    "        img_features = tf.identity(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Â§ÑÁêÜÊñáÊú¨\n",
    "        text = self.text_fc(text)\n",
    "        text = self.text_bn(text, training=training)\n",
    "        text = tf.nn.leaky_relu(text, alpha=0.2)\n",
    "        \n",
    "        # ËûçÂêàÂõæÂÉèÂíåÊñáÊú¨ÁâπÂæÅ\n",
    "        combined = tf.concat([x, text], axis=1)\n",
    "        combined = self.fc1(combined)\n",
    "        combined = tf.nn.leaky_relu(combined, alpha=0.2)\n",
    "        \n",
    "        logits = self.fc2(combined)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output, img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Â¢ûÂº∫ÁöÑÊçüÂ§±ÂáΩÊï∞ =============\n",
    "\n",
    "def improved_discriminator_loss(real_logits, fake_logits, real_features, fake_features, \n",
    "                                use_feature_matching=True, feature_weight=0.1):\n",
    "    \"\"\"\n",
    "    ÊîπËøõÁöÑÂà§Âà´Âô®ÊçüÂ§±Ôºö\n",
    "    1. Ê†áÁ≠æÂπ≥Êªë\n",
    "    2. ÂèØÈÄâÁöÑÁâπÂæÅÂåπÈÖçÊçüÂ§±\n",
    "    \"\"\"\n",
    "    # Ê†áÁ≠æÂπ≥Êªë - ÁúüÂÆûÊ†áÁ≠æÂú®[0.9, 1.0]ÔºåÂÅáÊ†áÁ≠æÂú®[0, 0.1]\n",
    "    real_labels = tf.random.uniform(tf.shape(real_logits), minval=0.9, maxval=1.0)\n",
    "    fake_labels = tf.random.uniform(tf.shape(fake_logits), minval=0.0, maxval=0.1)\n",
    "    \n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    real_loss = cross_entropy(real_labels, real_logits)\n",
    "    fake_loss = cross_entropy(fake_labels, fake_logits)\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    # ÂèØÈÄâÔºöÊ∑ªÂä†ÁâπÂæÅÂåπÈÖçÊçüÂ§±ÈºìÂä±Âà§Âà´Âô®Â≠¶‰π†ÊúâÊÑè‰πâÁöÑÁâπÂæÅ\n",
    "    if use_feature_matching:\n",
    "        fm_loss = tf.reduce_mean(tf.abs(\n",
    "            tf.reduce_mean(real_features, axis=0) - \n",
    "            tf.reduce_mean(fake_features, axis=0)\n",
    "        ))\n",
    "        total_loss = total_loss + feature_weight * fm_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def improved_generator_loss(fake_logits, fake_features, real_features, \n",
    "                           use_feature_matching=True, fm_weight=10.0,\n",
    "                           use_diversity=True, diversity_weight=0.01):\n",
    "    \"\"\"\n",
    "    ÊîπËøõÁöÑÁîüÊàêÂô®ÊçüÂ§±Ôºö\n",
    "    1. ÂØπÊäóÊçüÂ§±\n",
    "    2. ÁâπÂæÅÂåπÈÖçÊçüÂ§± - ËÆ©ÁîüÊàêÂõæÂÉèÁöÑÁâπÂæÅÊé•ËøëÁúüÂÆûÂõæÂÉè\n",
    "    3. Â§öÊ†∑ÊÄßÊ≠£ÂàôÂåñ - Èò≤Ê≠¢Ê®°ÂºèÂ¥©Ê∫É\n",
    "    \"\"\"\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # 1. ÂØπÊäóÊçüÂ§± - Ê¨∫È™óÂà§Âà´Âô®\n",
    "    adv_loss = cross_entropy(tf.ones_like(fake_logits), fake_logits)\n",
    "    \n",
    "    total_loss = adv_loss\n",
    "    \n",
    "    # 2. ÁâπÂæÅÂåπÈÖçÊçüÂ§± - ËÆ©ÁîüÊàêÁâπÂæÅÊé•ËøëÁúüÂÆûÁâπÂæÅ\n",
    "    if use_feature_matching:\n",
    "        fm_loss = tf.reduce_mean(tf.square(\n",
    "            tf.reduce_mean(fake_features, axis=0) - \n",
    "            tf.reduce_mean(real_features, axis=0)\n",
    "        ))\n",
    "        total_loss = total_loss + fm_weight * fm_loss\n",
    "    \n",
    "    # 3. Â§öÊ†∑ÊÄßÊçüÂ§± - ÈºìÂä±‰∏çÂêåÁöÑÂô™Â£∞ÁîüÊàê‰∏çÂêåÁöÑÂõæÂÉè\n",
    "    # ËøôÈÉ®ÂàÜÈúÄË¶ÅÂú®ËÆ≠ÁªÉÊ≠•È™§‰∏≠ÂÆûÁé∞ÔºàÊØîËæÉÂêå‰∏ÄÊâπÊ¨°‰∏≠ÁöÑÂõæÂÉèÔºâ\n",
    "    \n",
    "    return total_loss, adv_loss, fm_loss if use_feature_matching else 0.0\n",
    "\n",
    "\n",
    "def color_consistency_loss(generated_images):\n",
    "    \"\"\"\n",
    "    È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§± - ÈºìÂä±ÁîüÊàêÁöÑËä±ÊúµÈ¢úËâ≤Êõ¥Âä†Ëá™ÁÑ∂\n",
    "    ÈÄöËøáÊ£ÄÊü•È¢úËâ≤ÂàÜÂ∏ÉÁöÑÂπ≥ÊªëÊÄß\n",
    "    \"\"\"\n",
    "    # ËÆ°ÁÆóÁõ∏ÈÇªÂÉèÁ¥†ÁöÑÈ¢úËâ≤Â∑ÆÂºÇ\n",
    "    # Ê∞¥Âπ≥ÊñπÂêë\n",
    "    h_diff = tf.reduce_mean(tf.abs(generated_images[:, :, :-1, :] - generated_images[:, :, 1:, :]))\n",
    "    # ÂûÇÁõ¥ÊñπÂêë\n",
    "    v_diff = tf.reduce_mean(tf.abs(generated_images[:, :-1, :, :] - generated_images[:, 1:, :, :]))\n",
    "    \n",
    "    # Â∏åÊúõÈ¢úËâ≤ÂèòÂåñÂπ≥Êªë‰ΩÜ‰∏çÊòØÂÆåÂÖ®‰∏ÄËá¥\n",
    "    smoothness = h_diff + v_diff\n",
    "    \n",
    "    return smoothness\n",
    "\n",
    "\n",
    "def diversity_loss(generated_images, noise):\n",
    "    \"\"\"\n",
    "    Â§öÊ†∑ÊÄßÊçüÂ§± - Á°Æ‰øù‰∏çÂêåÂô™Â£∞ÁîüÊàê‰∏çÂêåÂõæÂÉè\n",
    "    ËÆ°ÁÆóÂõæÂÉèÂ∑ÆÂºÇ‰∏éÂô™Â£∞Â∑ÆÂºÇÁöÑÊØîÁéá\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(generated_images)[0]\n",
    "    \n",
    "    if batch_size < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # ËÆ°ÁÆóÂõæÂÉèÂØπ‰πãÈó¥ÁöÑL2Ë∑ùÁ¶ª\n",
    "    img_flat = tf.reshape(generated_images, [batch_size, -1])\n",
    "    img_diff = tf.norm(img_flat[:-1] - img_flat[1:], axis=1)\n",
    "    \n",
    "    # ËÆ°ÁÆóÂô™Â£∞ÂØπ‰πãÈó¥ÁöÑL2Ë∑ùÁ¶ª\n",
    "    noise_diff = tf.norm(noise[:-1] - noise[1:], axis=1)\n",
    "    \n",
    "    # Â§öÊ†∑ÊÄßÊçüÂ§±ÔºöÈºìÂä±ÂõæÂÉèÂ∑ÆÂºÇ‰∏éÂô™Â£∞Â∑ÆÂºÇÊàêÊ≠£ÊØî\n",
    "    # ‰ΩøÁî®Ë¥üÊï∞Âõ†‰∏∫Êàë‰ª¨ÊÉ≥ÊúÄÂ§ßÂåñËøô‰∏™ÊØîÁéá\n",
    "    diversity = -tf.reduce_mean(img_diff / (noise_diff + 1e-5))\n",
    "    \n",
    "    return diversity\n",
    "\n",
    "    return loss\n",
    "\n",
    "    \n",
    "\n",
    "def text_image_matching_loss(image_features, text_features, labels=None, temperature=0.07):    loss = cross_entropy(mismatch_labels, wrong_logits)\n",
    "\n",
    "    \"\"\"    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    ÊñáÊú¨-ÂõæÂÉèÂåπÈÖçÊçüÂ§± (CLIP-style contrastive loss)    \n",
    "\n",
    "    Á°Æ‰øùÁîüÊàêÁöÑÂõæÂÉèÁâπÂæÅ‰∏éÂØπÂ∫îÊñáÊú¨ÁâπÂæÅÁõ∏‰ººÔºå‰∏é‰∏çÂåπÈÖçÁöÑÊñáÊú¨‰∏çÁõ∏‰ºº    mismatch_labels = tf.zeros_like(wrong_logits)\n",
    "\n",
    "        # ÈîôÈÖçÂØπÂ∫îËØ•Ë¢´Âà§‰∏∫ÂÅáÔºàÊé•Ëøë0Ôºâ\n",
    "\n",
    "    Args:    \n",
    "\n",
    "        image_features: (batch, feature_dim) ÂõæÂÉèÁâπÂæÅ    wrong_logits, wrong_probs, _ = discriminator(real_image, wrong_text, training=True)\n",
    "\n",
    "        text_features: (batch, feature_dim) ÊñáÊú¨ÁâπÂæÅ    # Âà§Âà´Âô®ÂØπÈîôÈÖçÂØπÁöÑÂà§Êñ≠\n",
    "\n",
    "        labels: (batch,) Â¶ÇÊûúÊèê‰æõÔºåË°®Á§∫Âì™‰∫õÊòØÂåπÈÖçÂØπ (1=ÂåπÈÖç, 0=‰∏çÂåπÈÖç)    \"\"\"\n",
    "\n",
    "        temperature: Ê∏©Â∫¶Á≥ªÊï∞ÔºåÊéßÂà∂ÂØπÊØîÂ≠¶‰π†ÁöÑÈöæÂ∫¶        temperature: Ê∏©Â∫¶Á≥ªÊï∞\n",
    "\n",
    "    \"\"\"        discriminator: Âà§Âà´Âô®Ê®°Âûã\n",
    "\n",
    "    batch_size = tf.shape(image_features)[0]        wrong_text: (batch, hidden_dim) ‰∏çÂåπÈÖçÁöÑÊñáÊú¨ÁâπÂæÅÔºàÊù•Ëá™ÂÖ∂‰ªñÊ†∑Êú¨Ôºâ\n",
    "\n",
    "            real_image: (batch, h, w, c) ÁúüÂÆûÂõæÂÉè\n",
    "\n",
    "    # ÂΩí‰∏ÄÂåñÁâπÂæÅ    Args:\n",
    "\n",
    "    image_features = tf.nn.l2_normalize(image_features, axis=-1)    \n",
    "\n",
    "    text_features = tf.nn.l2_normalize(text_features, axis=-1)    ÈÄöËøáÂ∞ÜÁúüÂÆûÂõæÂÉè‰∏éÈîôËØØÁöÑÊñáÊú¨ÈÖçÂØπÔºåËÆ©Âà§Âà´Âô®Â≠¶‰ºöÊãíÁªù‰∏çÂåπÈÖçÁöÑÂØπ\n",
    "\n",
    "        ÈîôÈÖçÊçüÂ§± - ËÆ≠ÁªÉÂà§Âà´Âô®ËØÜÂà´ÊñáÊú¨-ÂõæÂÉè‰∏çÂåπÈÖçÁöÑÊÉÖÂÜµ\n",
    "\n",
    "    # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Áü©Èòµ (batch, batch)    \"\"\"\n",
    "\n",
    "    # logits[i][j] = ÂõæÂÉèi‰∏éÊñáÊú¨jÁöÑÁõ∏‰ººÂ∫¶def mismatch_loss(real_image, wrong_text, discriminator, temperature=1.0):\n",
    "\n",
    "    logits = tf.matmul(image_features, text_features, transpose_b=True) / temperature\n",
    "\n",
    "    \n",
    "\n",
    "    if labels is None:    return tf.reduce_mean(kl_loss)\n",
    "\n",
    "        # ÂÅáËÆæÂØπËßíÁ∫øÊòØÂåπÈÖçÂØπ (ÂõæÂÉèiÂØπÂ∫îÊñáÊú¨i)    \n",
    "\n",
    "        labels = tf.range(batch_size)    )\n",
    "\n",
    "            axis=-1\n",
    "\n",
    "    # ÂèåÂêëÂØπÊØîÊçüÂ§±        word_importance * tf.math.log((word_importance + 1e-10) / (attention_weights + 1e-10)),\n",
    "\n",
    "    # ÂõæÂÉè->ÊñáÊú¨: ÊØè‰∏™ÂõæÂÉèÂ∫îËØ•ÊúÄÂåπÈÖçÂÖ∂ÂØπÂ∫îÁöÑÊñáÊú¨    kl_loss = tf.reduce_sum(\n",
    "\n",
    "    loss_i2t = tf.keras.losses.sparse_categorical_crossentropy(    # KLÊï£Â∫¶: KL(word_importance || attention_weights)\n",
    "\n",
    "        labels, logits, from_logits=True    \n",
    "\n",
    "    )    word_importance = tf.squeeze(word_importance, axis=-1)\n",
    "\n",
    "        attention_weights = tf.squeeze(attention_weights, axis=-1)  # (batch, seq_len)\n",
    "\n",
    "    # ÊñáÊú¨->ÂõæÂÉè: ÊØè‰∏™ÊñáÊú¨Â∫îËØ•ÊúÄÂåπÈÖçÂÖ∂ÂØπÂ∫îÁöÑÂõæÂÉè    # ‰ΩøÁî®KLÊï£Â∫¶Ë°°Èáè‰∏§‰∏™ÂàÜÂ∏ÉÁöÑÂ∑ÆÂºÇ\n",
    "\n",
    "    loss_t2i = tf.keras.losses.sparse_categorical_crossentropy(    # ËÆ©Ê≥®ÊÑèÂäõÊùÉÈáçÊé•ËøëËØçÁöÑÈáçË¶ÅÊÄßÂàÜÂ∏É\n",
    "\n",
    "        labels, tf.transpose(logits), from_logits=True    \n",
    "\n",
    "    )    word_importance = tf.nn.softmax(word_importance / temperature, axis=1)\n",
    "\n",
    "        # ÂΩí‰∏ÄÂåñ‰∏∫Ê¶ÇÁéáÂàÜÂ∏É\n",
    "\n",
    "    # Âπ≥Âùá‰∏§‰∏™ÊñπÂêëÁöÑÊçüÂ§±    \n",
    "\n",
    "    return (tf.reduce_mean(loss_i2t) + tf.reduce_mean(loss_t2i)) / 2.0    )\n",
    "\n",
    "        tf.expand_dims(sentence_feature, axis=-1)\n",
    "\n",
    "        word_features, \n",
    "\n",
    "def word_level_alignment_loss(attention_weights, word_features, sentence_feature, temperature=5.0):    word_importance = tf.matmul(\n",
    "\n",
    "    \"\"\"    # (batch, seq_len, hidden_dim) √ó (batch, hidden_dim, 1) -> (batch, seq_len, 1)\n",
    "\n",
    "    ËØçÁ∫ßÂØπÈΩêÊçüÂ§± - Á°Æ‰øùÊ≥®ÊÑèÂäõÊùÉÈáçÂÖ≥Ê≥®Âà∞ÊñáÊú¨‰∏≠ÊúÄÈáçË¶ÅÁöÑËØç    # ËÆ°ÁÆóÊØè‰∏™ËØç‰∏éÂè•Â≠êÁâπÂæÅÁöÑÁõ∏‰ººÂ∫¶‰Ωú‰∏∫\"ÈáçË¶ÅÊÄß\"ÊåáÊ†á\n",
    "\n",
    "    Âü∫‰∫éËØçÁâπÂæÅ‰∏éÂè•Â≠êÁâπÂæÅÁöÑÁõ∏‰ººÂ∫¶Êù•ÊåáÂØºÊ≥®ÊÑèÂäõ    \"\"\"\n",
    "\n",
    "            temperature: Ê∏©Â∫¶Á≥ªÊï∞\n",
    "\n",
    "    Args:        sentence_feature: (batch, hidden_dim) Âè•Â≠êÂÖ®Â±ÄÁâπÂæÅ\n",
    "\n",
    "        attention_weights: (batch, seq_len, 1) ÁîüÊàêÂô®ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáç        word_features: (batch, seq_len, hidden_dim) ÊØè‰∏™ËØçÁöÑÁâπÂæÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊñáÊú¨-ÂõæÂÉèÂØπÈΩêÊçüÂ§±ÔºàÊèêÈ´òÊèèËø∞Á¨¶ÂêàÂ∫¶Ôºâ=============\n",
    "\n",
    "def text_image_matching_loss(image_features, text_features, labels=None, temperature=0.07):\n",
    "    \"\"\"\n",
    "    ÊñáÊú¨-ÂõæÂÉèÂåπÈÖçÊçüÂ§± (CLIP-style contrastive loss)\n",
    "    Á°Æ‰øùÁîüÊàêÁöÑÂõæÂÉèÁâπÂæÅ‰∏éÂØπÂ∫îÊñáÊú¨ÁâπÂæÅÁõ∏‰ººÔºå‰∏é‰∏çÂåπÈÖçÁöÑÊñáÊú¨‰∏çÁõ∏‰ºº\n",
    "    \n",
    "    Args:\n",
    "        image_features: (batch, feature_dim) ÂõæÂÉèÁâπÂæÅ\n",
    "        text_features: (batch, feature_dim) ÊñáÊú¨ÁâπÂæÅ\n",
    "        labels: (batch,) Â¶ÇÊûúÊèê‰æõÔºåË°®Á§∫Âì™‰∫õÊòØÂåπÈÖçÂØπ (1=ÂåπÈÖç, 0=‰∏çÂåπÈÖç)\n",
    "        temperature: Ê∏©Â∫¶Á≥ªÊï∞ÔºåÊéßÂà∂ÂØπÊØîÂ≠¶‰π†ÁöÑÈöæÂ∫¶\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(image_features)[0]\n",
    "    \n",
    "    # ÂΩí‰∏ÄÂåñÁâπÂæÅ\n",
    "    image_features = tf.nn.l2_normalize(image_features, axis=-1)\n",
    "    text_features = tf.nn.l2_normalize(text_features, axis=-1)\n",
    "    \n",
    "    # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Áü©Èòµ (batch, batch)\n",
    "    # logits[i][j] = ÂõæÂÉèi‰∏éÊñáÊú¨jÁöÑÁõ∏‰ººÂ∫¶\n",
    "    logits = tf.matmul(image_features, text_features, transpose_b=True) / temperature\n",
    "    \n",
    "    if labels is None:\n",
    "        # ÂÅáËÆæÂØπËßíÁ∫øÊòØÂåπÈÖçÂØπ (ÂõæÂÉèiÂØπÂ∫îÊñáÊú¨i)\n",
    "        labels = tf.range(batch_size)\n",
    "    \n",
    "    # ÂèåÂêëÂØπÊØîÊçüÂ§±\n",
    "    # ÂõæÂÉè->ÊñáÊú¨: ÊØè‰∏™ÂõæÂÉèÂ∫îËØ•ÊúÄÂåπÈÖçÂÖ∂ÂØπÂ∫îÁöÑÊñáÊú¨\n",
    "    loss_i2t = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True\n",
    "    )\n",
    "    \n",
    "    # ÊñáÊú¨->ÂõæÂÉè: ÊØè‰∏™ÊñáÊú¨Â∫îËØ•ÊúÄÂåπÈÖçÂÖ∂ÂØπÂ∫îÁöÑÂõæÂÉè\n",
    "    loss_t2i = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, tf.transpose(logits), from_logits=True\n",
    "    )\n",
    "    \n",
    "    # Âπ≥Âùá‰∏§‰∏™ÊñπÂêëÁöÑÊçüÂ§±\n",
    "    return (tf.reduce_mean(loss_i2t) + tf.reduce_mean(loss_t2i)) / 2.0\n",
    "\n",
    "\n",
    "def word_level_alignment_loss(attention_weights, word_features, sentence_feature, temperature=5.0):\n",
    "    \"\"\"\n",
    "    ËØçÁ∫ßÂØπÈΩêÊçüÂ§± - Á°Æ‰øùÊ≥®ÊÑèÂäõÊùÉÈáçÂÖ≥Ê≥®Âà∞ÊñáÊú¨‰∏≠ÊúÄÈáçË¶ÅÁöÑËØç\n",
    "    Âü∫‰∫éËØçÁâπÂæÅ‰∏éÂè•Â≠êÁâπÂæÅÁöÑÁõ∏‰ººÂ∫¶Êù•ÊåáÂØºÊ≥®ÊÑèÂäõ\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: (batch, seq_len, 1) ÁîüÊàêÂô®ÁöÑÊ≥®ÊÑèÂäõÊùÉÈáç\n",
    "        word_features: (batch, seq_len, hidden_dim) ÊØè‰∏™ËØçÁöÑÁâπÂæÅ\n",
    "        sentence_feature: (batch, hidden_dim) Âè•Â≠êÂÖ®Â±ÄÁâπÂæÅ\n",
    "        temperature: Ê∏©Â∫¶Á≥ªÊï∞\n",
    "    \"\"\"\n",
    "    # ËÆ°ÁÆóÊØè‰∏™ËØç‰∏éÂè•Â≠êÁâπÂæÅÁöÑÁõ∏‰ººÂ∫¶‰Ωú‰∏∫\"ÈáçË¶ÅÊÄß\"ÊåáÊ†á\n",
    "    # (batch, seq_len, hidden_dim) √ó (batch, hidden_dim, 1) -> (batch, seq_len, 1)\n",
    "    word_importance = tf.matmul(\n",
    "        word_features, \n",
    "        tf.expand_dims(sentence_feature, axis=-1)\n",
    "    )\n",
    "    \n",
    "    # ÂΩí‰∏ÄÂåñ‰∏∫Ê¶ÇÁéáÂàÜÂ∏É\n",
    "    word_importance = tf.nn.softmax(word_importance / temperature, axis=1)\n",
    "    \n",
    "    # ËÆ©Ê≥®ÊÑèÂäõÊùÉÈáçÊé•ËøëËØçÁöÑÈáçË¶ÅÊÄßÂàÜÂ∏É\n",
    "    # ‰ΩøÁî®KLÊï£Â∫¶Ë°°Èáè‰∏§‰∏™ÂàÜÂ∏ÉÁöÑÂ∑ÆÂºÇ\n",
    "    attention_weights = tf.squeeze(attention_weights, axis=-1)  # (batch, seq_len)\n",
    "    word_importance = tf.squeeze(word_importance, axis=-1)\n",
    "    \n",
    "    # KLÊï£Â∫¶: KL(word_importance || attention_weights)\n",
    "    kl_loss = tf.reduce_sum(\n",
    "        word_importance * tf.math.log((word_importance + 1e-10) / (attention_weights + 1e-10)),\n",
    "        axis=-1\n",
    "    )\n",
    "    \n",
    "    return tf.reduce_mean(kl_loss)\n",
    "\n",
    "\n",
    "def mismatch_loss(real_image, wrong_text, discriminator, temperature=1.0):\n",
    "    \"\"\"\n",
    "    ÈîôÈÖçÊçüÂ§± - ËÆ≠ÁªÉÂà§Âà´Âô®ËØÜÂà´ÊñáÊú¨-ÂõæÂÉè‰∏çÂåπÈÖçÁöÑÊÉÖÂÜµ\n",
    "    ÈÄöËøáÂ∞ÜÁúüÂÆûÂõæÂÉè‰∏éÈîôËØØÁöÑÊñáÊú¨ÈÖçÂØπÔºåËÆ©Âà§Âà´Âô®Â≠¶‰ºöÊãíÁªù‰∏çÂåπÈÖçÁöÑÂØπ\n",
    "    \n",
    "    Args:\n",
    "        real_image: (batch, h, w, c) ÁúüÂÆûÂõæÂÉè\n",
    "        wrong_text: (batch, hidden_dim) ‰∏çÂåπÈÖçÁöÑÊñáÊú¨ÁâπÂæÅÔºàÊù•Ëá™ÂÖ∂‰ªñÊ†∑Êú¨Ôºâ\n",
    "        discriminator: Âà§Âà´Âô®Ê®°Âûã\n",
    "        temperature: Ê∏©Â∫¶Á≥ªÊï∞\n",
    "    \"\"\"\n",
    "    # Âà§Âà´Âô®ÂØπÈîôÈÖçÂØπÁöÑÂà§Êñ≠\n",
    "    wrong_logits, wrong_probs, _ = discriminator(real_image, wrong_text, training=True)\n",
    "    \n",
    "    # ÈîôÈÖçÂØπÂ∫îËØ•Ë¢´Âà§‰∏∫ÂÅáÔºàÊé•Ëøë0Ôºâ\n",
    "    mismatch_labels = tf.zeros_like(wrong_logits)\n",
    "    \n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    loss = cross_entropy(mismatch_labels, wrong_logits)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"‚úì ÊñáÊú¨-ÂõæÂÉèÂØπÈΩêÊçüÂ§±ÂáΩÊï∞Â∑≤Ê∑ªÂä†:\")\n",
    "print(\"  - text_image_matching_loss: ÂØπÊØîÂ≠¶‰π†Á°Æ‰øùËØ≠‰πâÂåπÈÖç\")\n",
    "print(\"  - word_level_alignment_loss: Ê≥®ÊÑèÂäõËÅöÁÑ¶ÂÖ≥ÈîÆËØç\")\n",
    "print(\"  - mismatch_loss: Âà§Âà´Âô®ËØÜÂà´‰∏çÂåπÈÖçÂØπ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ ÊñáÊú¨-ÂõæÂÉèÂØπÈΩêÊîπËøõËØ¥Êòé\n",
    "\n",
    "### ÈóÆÈ¢òÂàÜÊûê\n",
    "ÁîüÊàêÁöÑËä±ÊúµË¥®Èáè‰∏çÈîô,‰ΩÜ‰∏éÊñáÊú¨ÊèèËø∞‰∏çÂ§üÁ¨¶Âêà,ÂØºËá¥ËØÑÂàÜ‰Ωé„ÄÇ‰∏ªË¶ÅÂéüÂõ†:\n",
    "1. **Áº∫‰πèÊòæÂºèÁöÑÊñáÊú¨-ÂõæÂÉèËØ≠‰πâÂåπÈÖçÁ∫¶Êùü**\n",
    "2. **Ê≥®ÊÑèÂäõÊú∫Âà∂Ê≤°ÊúâÂÖÖÂàÜÂà©Áî®ÊñáÊú¨ËØ≠‰πâ‰ø°ÊÅØ**\n",
    "3. **Âà§Âà´Âô®Êó†Ê≥ïÂå∫ÂàÜÊñáÊú¨-ÂõæÂÉè‰∏çÂåπÈÖçÁöÑÊÉÖÂÜµ**\n",
    "\n",
    "### Ê†∏ÂøÉÊîπËøõ\n",
    "\n",
    "#### 1. **ÊñáÊú¨-ÂõæÂÉèÂØπÊØîÊçüÂ§±** (`text_image_matching_loss`)\n",
    "- ÈááÁî®CLIPÈ£éÊ†ºÁöÑÂØπÊØîÂ≠¶‰π†\n",
    "- Á°Æ‰øùÁîüÊàêÂõæÂÉèÁöÑÁâπÂæÅ‰∏éÂØπÂ∫îÊñáÊú¨ÁâπÂæÅÁõ∏‰ºº\n",
    "- ‰∏ébatchÂÜÖÂÖ∂‰ªñ‰∏çÂåπÈÖçÁöÑÊñáÊú¨ÁâπÂæÅ‰∏çÁõ∏‰ºº\n",
    "- **ÊïàÊûú**: Âº∫Âà∂ËØ≠‰πâÂØπÈΩê,ÊèêÈ´òÊèèËø∞Á¨¶ÂêàÂ∫¶\n",
    "\n",
    "#### 2. **ËØçÁ∫ßÂØπÈΩêÊçüÂ§±** (`word_level_alignment_loss`)  \n",
    "- ÂºïÂØºÊ≥®ÊÑèÂäõÊùÉÈáçËÅöÁÑ¶Âà∞ÊñáÊú¨‰∏≠ÊúÄÈáçË¶ÅÁöÑËØç\n",
    "- Âü∫‰∫éËØçÁâπÂæÅ‰∏éÂè•Â≠êÁâπÂæÅÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÈáçË¶ÅÊÄß\n",
    "- ‰ΩøÁî®KLÊï£Â∫¶ËÆ©Ê≥®ÊÑèÂäõÂàÜÂ∏ÉÊé•ËøëËØçÈáçË¶ÅÊÄßÂàÜÂ∏É\n",
    "- **ÊïàÊûú**: ÁîüÊàêÂô®Êõ¥ÂÖ≥Ê≥®ÂÖ≥ÈîÆÊèèËø∞ËØç(Â¶ÇÈ¢úËâ≤„ÄÅÂΩ¢Áä∂)\n",
    "\n",
    "#### 3. **ÈîôÈÖçÊ£ÄÊµãÊçüÂ§±** (`mismatch_loss`)\n",
    "- ËÆ≠ÁªÉÂà§Âà´Âô®ËØÜÂà´\"ÁúüÂÆûÂõæÂÉè+ÈîôËØØÊñáÊú¨\"ÁöÑ‰∏çÂåπÈÖçÂØπ\n",
    "- ÊèêÈ´òÂà§Âà´Âô®ÁöÑÊñáÊú¨-ÂõæÂÉè‰∏ÄËá¥ÊÄßÊ£ÄÊü•ËÉΩÂäõ\n",
    "- **ÊïàÊûú**: Ëø´‰ΩøÁîüÊàêÂô®Êõ¥ÂáÜÁ°ÆÂåπÈÖçÊñáÊú¨ÊèèËø∞\n",
    "\n",
    "### ËÆ≠ÁªÉÁ≠ñÁï•\n",
    "- **Matching LossÊùÉÈáç**: 5.0 - ËæÉÈ´òÊùÉÈáçÁ°Æ‰øùËØ≠‰πâÂØπÈΩê‰ºòÂÖàÁ∫ß\n",
    "- **Word Align LossÊùÉÈáç**: 2.0 - ÂºïÂØºÊ≥®ÊÑèÂäõÊú∫Âà∂\n",
    "- **Mismatch LossÊùÉÈáç**: 0.5 - ËæÖÂä©Âà§Âà´Âô®Â≠¶‰π†\n",
    "\n",
    "### È¢ÑÊúüÊïàÊûú\n",
    "1. ‚úÖ ÁîüÊàêÁöÑËä±ÊúµÈ¢úËâ≤„ÄÅÂΩ¢Áä∂‰∏éÊñáÊú¨ÊèèËø∞Êõ¥ÂåπÈÖç\n",
    "2. ‚úÖ Ê≥®ÊÑèÂäõÊú∫Âà∂ËÅöÁÑ¶ÂÖ≥ÈîÆËØç(Â¶Ç\"red petals\", \"yellow center\")\n",
    "3. ‚úÖ ËØÑÂàÜÁ≥ªÁªüÁöÑÁõ∏‰ººÂ∫¶ÂàÜÊï∞ÊèêÂçá\n",
    "4. ‚úÖ ÈÅøÂÖçÁîüÊàê‰∏éÊèèËø∞Êó†ÂÖ≥ÁöÑÁâπÂæÅ\n",
    "\n",
    "### ‰ΩøÁî®Âª∫ËÆÆ\n",
    "- ‰ªé‰πãÂâçÊúÄ‰Ω≥checkpointÁªßÁª≠ËÆ≠ÁªÉ,Êñ∞ÊçüÂ§±‰ºöÈÄêÊ≠•ÊîπÂñÑÂØπÈΩêÂ∫¶\n",
    "- ËßÇÂØüËÆ≠ÁªÉÊó•Âøó‰∏≠ÁöÑ`Matching`Âíå`WordAlign`ÊçüÂ§±ÊòØÂê¶‰∏ãÈôç\n",
    "- ÁîüÊàêÊ†∑Êú¨ÂõæÂÉèÊó∂ÂØπÊØîÊñáÊú¨ÊèèËø∞,È™åËØÅÊîπËøõÊïàÊûú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ‰ºòÂåñÁöÑË∂ÖÂèÇÊï∞ÈÖçÁΩÆ =============\n",
    "hparas_improved = {\n",
    "    'MAX_SEQ_LENGTH': 20,\n",
    "    'EMBED_DIM': 300,                         # ËØçÂµåÂÖ•Áª¥Â∫¶\n",
    "    'VOCAB_SIZE': len(word2Id_dict),\n",
    "    'RNN_HIDDEN_SIZE': 256,                   # RNNÈöêËóèÂ±ÇÂ§ßÂ∞è\n",
    "    'Z_DIM': 128,                             # Âô™Â£∞Áª¥Â∫¶ÔºàÂ¢ûÂä†‰ª•ÊèêÈ´òÂ§öÊ†∑ÊÄßÔºâ\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'BATCH_SIZE': 64,                         # ÊâπÊ¨°Â§ßÂ∞è\n",
    "    \n",
    "    # Â≠¶‰π†ÁéáËÆæÁΩÆ\n",
    "    'LR_G': 1e-4,                             # ÁîüÊàêÂô®Â≠¶‰π†Áéá\n",
    "    'LR_D': 4e-4,                             # Âà§Âà´Âô®Â≠¶‰π†ÁéáÔºàÁ®çÈ´ò‰∫éÁîüÊàêÂô®Ôºâ\n",
    "    'BETA_1': 0.5,\n",
    "    'BETA_2': 0.999,\n",
    "    \n",
    "    # ËÆ≠ÁªÉËÆæÁΩÆ\n",
    "    'N_EPOCH': 300,                           # ËÆ≠ÁªÉËΩÆÊï∞\n",
    "    'N_SAMPLE': num_training_sample,\n",
    "    'D_STEPS': 1,                             # ÊØè‰∏™GÊ≠•È™§ÂâçDËÆ≠ÁªÉÁöÑÊ¨°Êï∞\n",
    "    'G_STEPS': 2,                             # ÊØè‰∏™DÊ≠•È™§ÂêéGËÆ≠ÁªÉÁöÑÊ¨°Êï∞\n",
    "    \n",
    "    # ÊçüÂ§±ÊùÉÈáç\n",
    "    'LAMBDA_FM': 10.0,                        # ÁâπÂæÅÂåπÈÖçÊçüÂ§±ÊùÉÈáç\n",
    "    'LAMBDA_COLOR': 0.5,                      # È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§±ÊùÉÈáç\n",
    "    'LAMBDA_DIV': 0.01,                       # Â§öÊ†∑ÊÄßÊçüÂ§±ÊùÉÈáç\n",
    "    'LAMBDA_MATCHING': 5.0,                   # ÊñáÊú¨-ÂõæÂÉèÂåπÈÖçÊçüÂ§±ÊùÉÈáçÔºàÊñ∞Â¢ûÔºâ\n",
    "    'LAMBDA_WORD_ALIGN': 2.0,                 # ËØçÁ∫ßÂØπÈΩêÊçüÂ§±ÊùÉÈáçÔºàÊñ∞Â¢ûÔºâ\n",
    "    'LAMBDA_MISMATCH': 0.5,                   # ÈîôÈÖçÊçüÂ§±ÊùÉÈáçÔºàÊñ∞Â¢ûÔºâ\n",
    "    \n",
    "    # Ë∑ØÂæÑËÆæÁΩÆ\n",
    "    'CHECKPOINTS_DIR': '/content/drive/MyDrive/Colab Notebooks/dl/comp3/checkpoints/improved_v2',\n",
    "    'PRINT_FREQ': 1,                          # ÊØèN‰∏™epochÊâìÂç∞‰∏ÄÊ¨°\n",
    "    'SAVE_FREQ': 10,                          # ÊØèN‰∏™epoch‰øùÂ≠ò‰∏ÄÊ¨°\n",
    "}\n",
    "\n",
    "# ÂàõÂª∫checkpointÁõÆÂΩï\n",
    "import os\n",
    "os.makedirs(hparas_improved['CHECKPOINTS_DIR'], exist_ok=True)\n",
    "os.makedirs('samples/improved_v2', exist_ok=True)\n",
    "\n",
    "print(\"ÊîπËøõÁâàË∂ÖÂèÇÊï∞ÈÖçÁΩÆ:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in hparas_improved.items():\n",
    "\n",
    "    if not key.endswith('_DIR'):print(\"=\"*60)\n",
    "        print(f\"{key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÂàùÂßãÂåñÊîπËøõÁöÑÊ®°Âûã =============\n",
    "print(\"ÂàùÂßãÂåñÊîπËøõÁâàÊ®°Âûã...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ‰ΩøÁî®ÊîπËøõÁöÑË∂ÖÂèÇÊï∞\n",
    "hparas = hparas_improved\n",
    "\n",
    "# ÂàùÂßãÂåñÊ®°Âûã\n",
    "text_encoder_improved = ImprovedTextEncoder(hparas)\n",
    "generator_improved = ImprovedGenerator(hparas)\n",
    "discriminator_improved = ImprovedDiscriminator(hparas)\n",
    "\n",
    "# ‰ºòÂåñÂô®\n",
    "generator_optimizer_improved = tf.keras.optimizers.Adam(\n",
    "    hparas['LR_G'], \n",
    "    beta_1=hparas['BETA_1'], \n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "discriminator_optimizer_improved = tf.keras.optimizers.Adam(\n",
    "    hparas['LR_D'], \n",
    "    beta_1=hparas['BETA_1'], \n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint_improved = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer_improved,\n",
    "    discriminator_optimizer=discriminator_optimizer_improved,\n",
    "    text_encoder=text_encoder_improved,\n",
    "    generator=generator_improved,\n",
    "    discriminator=discriminator_improved\n",
    ")\n",
    "\n",
    "print(\"‚úì Ê®°ÂûãÂàùÂßãÂåñÂÆåÊàê\")\n",
    "print(f\"‚úì ÊñáÊú¨ÁºñÁ†ÅÂô®: ImprovedTextEncoder (Â∏¶ËØçÁ∫ßLSTM)\")\n",
    "print(f\"‚úì ÁîüÊàêÂô®: ImprovedGenerator (Â∏¶Ê≥®ÊÑèÂäõ+ÊÆãÂ∑Æ+Ëá™Ê≥®ÊÑèÂäõ)\")\n",
    "print(f\"‚úì Âà§Âà´Âô®: ImprovedDiscriminator (Â∏¶Ëá™Ê≥®ÊÑèÂäõ)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊîπËøõÁöÑËÆ≠ÁªÉÊ≠•È™§ÔºàÂ¢ûÂº∫ÊñáÊú¨-ÂõæÂÉèÂØπÈΩêÔºâ=============\n",
    "@tf.function\n",
    "def improved_train_step(real_image, caption):\n",
    "    \"\"\"\n",
    "    Â¢ûÂº∫ÁöÑËÆ≠ÁªÉÊ≠•È™§Ôºö\n",
    "    1. ËØçÁ∫ßÊ≥®ÊÑèÂäõ\n",
    "    2. ÁâπÂæÅÂåπÈÖçÊçüÂ§±\n",
    "    3. È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§±\n",
    "    4. Â§öÊ†∑ÊÄßÊ≠£ÂàôÂåñ\n",
    "    5. ÊñáÊú¨-ÂõæÂÉèÂåπÈÖçÊçüÂ§± (Êñ∞Â¢û)\n",
    "    6. ËØçÁ∫ßÂØπÈΩêÊçüÂ§± (Êñ∞Â¢û)\n",
    "    7. ÈîôÈÖçÊçüÂ§± (Êñ∞Â¢û)\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal([hparas['BATCH_SIZE'], hparas['Z_DIM']])\n",
    "    batch_size = tf.shape(real_image)[0]\n",
    "    \n",
    "    # ‰∏∫ÈîôÈÖçÊçüÂ§±ÂáÜÂ§áÊâì‰π±ÁöÑÊñáÊú¨ÔºàÂ∞ÜÊñáÊú¨Á¥¢ÂºïÂêëÂêéÁßª1‰ΩçÔºåÊûÑÈÄ†‰∏çÂåπÈÖçÂØπÔºâ\n",
    "    indices = tf.range(batch_size)\n",
    "    shifted_indices = tf.concat([indices[1:], indices[:1]], axis=0)\n",
    "    \n",
    "    # ===== ËÆ≠ÁªÉÂà§Âà´Âô® =====\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # ÁºñÁ†ÅÊñáÊú¨ÔºàËé∑ÂèñËØçÁâπÂæÅÂíåÂè•Â≠êÁâπÂæÅÔºâ\n",
    "        word_features, sentence_feature = text_encoder_improved(caption, training=True)\n",
    "        \n",
    "        # ÁîüÊàêÂÅáÂõæÂÉè\n",
    "        fake_image, _ = generator_improved(word_features, sentence_feature, noise, training=True)\n",
    "        \n",
    "        # Âà§Âà´Âô®Âà§Êñ≠ÔºàËé∑ÂèñÁâπÂæÅÁî®‰∫éÁâπÂæÅÂåπÈÖçÔºâ\n",
    "        real_logits, _, real_features = discriminator_improved(real_image, sentence_feature, training=True)\n",
    "        fake_logits, _, fake_features = discriminator_improved(fake_image, sentence_feature, training=True)\n",
    "        \n",
    "        # Âà§Âà´Âô®ÊçüÂ§±ÔºàÂ∏¶ÁâπÂæÅÂåπÈÖçÔºâ\n",
    "        d_loss = improved_discriminator_loss(\n",
    "            real_logits, fake_logits, \n",
    "            real_features, fake_features,\n",
    "            use_feature_matching=True,\n",
    "            feature_weight=0.1\n",
    "        )\n",
    "        \n",
    "        # ÈîôÈÖçÊçüÂ§±ÔºöÁúüÂÆûÂõæÂÉè + ÈîôËØØÊñáÊú¨\n",
    "        wrong_sentence = tf.gather(sentence_feature, shifted_indices)\n",
    "        mismatch_d_loss = mismatch_loss(real_image, wrong_sentence, discriminator_improved)\n",
    "        \n",
    "        # ÊÄªÂà§Âà´Âô®ÊçüÂ§±\n",
    "        d_loss_total = d_loss + hparas['LAMBDA_MISMATCH'] * mismatch_d_loss\n",
    "    \n",
    "    # Êõ¥Êñ∞Âà§Âà´Âô®\n",
    "    disc_grads = disc_tape.gradient(d_loss_total, discriminator_improved.trainable_variables)\n",
    "    disc_grads, _ = tf.clip_by_global_norm(disc_grads, 5.0)\n",
    "    discriminator_optimizer_improved.apply_gradients(\n",
    "        zip(disc_grads, discriminator_improved.trainable_variables)\n",
    "    )\n",
    "    \n",
    "    # ===== ËÆ≠ÁªÉÁîüÊàêÂô® =====\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # ÈáçÊñ∞ÁºñÁ†ÅÊñáÊú¨\n",
    "        word_features, sentence_feature = text_encoder_improved(caption, training=True)\n",
    "        \n",
    "        # ÁîüÊàêÂÅáÂõæÂÉè\n",
    "        fake_image, attention_weights = generator_improved(\n",
    "            word_features, sentence_feature, noise, training=True\n",
    "        )\n",
    "        \n",
    "        # Âà§Âà´Âô®Âà§Êñ≠\n",
    "        fake_logits, _, fake_features = discriminator_improved(fake_image, sentence_feature, training=True)\n",
    "        real_logits, _, real_features = discriminator_improved(real_image, sentence_feature, training=True)\n",
    "        \n",
    "        # ÁîüÊàêÂô®ÊçüÂ§±ÔºàÂåÖÂê´ÁâπÂæÅÂåπÈÖçÔºâ\n",
    "        g_loss, adv_loss, fm_loss = improved_generator_loss(\n",
    "            fake_logits, fake_features, real_features,\n",
    "            use_feature_matching=True,\n",
    "            fm_weight=hparas['LAMBDA_FM']\n",
    "        )\n",
    "        \n",
    "        # È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§±\n",
    "        color_loss = color_consistency_loss(fake_image)\n",
    "        \n",
    "\n",
    "        # Â§öÊ†∑ÊÄßÊçüÂ§±    return d_loss_total, total_g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss, word_align_loss\n",
    "\n",
    "        div_loss = diversity_loss(fake_image, noise)    \n",
    "\n",
    "            generator_optimizer_improved.apply_gradients(zip(gen_grads, gen_vars))\n",
    "\n",
    "        # ÊñáÊú¨-ÂõæÂÉèÂåπÈÖçÊçüÂ§±ÔºàÂØπÊØîÂ≠¶‰π†Ôºâ    gen_grads, _ = tf.clip_by_global_norm(gen_grads, 5.0)\n",
    "\n",
    "        # ‰ΩøÁî®Âà§Âà´Âô®ÁöÑ‰∏≠Èó¥ÁâπÂæÅ‰Ωú‰∏∫ÂõæÂÉèË°®Á§∫    gen_grads = gen_tape.gradient(total_g_loss, gen_vars)\n",
    "\n",
    "        fake_img_flatten = tf.reduce_mean(fake_features, axis=[1, 2])  # (batch, channels)    gen_vars = generator_improved.trainable_variables + text_encoder_improved.trainable_variables\n",
    "\n",
    "        matching_loss = text_image_matching_loss(fake_img_flatten, sentence_feature)    # Êõ¥Êñ∞ÁîüÊàêÂô®ÂíåÊñáÊú¨ÁºñÁ†ÅÂô®\n",
    "\n",
    "            \n",
    "\n",
    "        # ËØçÁ∫ßÂØπÈΩêÊçüÂ§±                      hparas['LAMBDA_WORD_ALIGN'] * word_align_loss\n",
    "\n",
    "        word_align_loss = word_level_alignment_loss(                      hparas['LAMBDA_MATCHING'] * matching_loss + \\\n",
    "\n",
    "            attention_weights, word_features, sentence_feature                      hparas['LAMBDA_DIV'] * div_loss + \\\n",
    "\n",
    "        )                      hparas['LAMBDA_COLOR'] * color_loss + \\\n",
    "\n",
    "                total_g_loss = g_loss + \\\n",
    "        # ÊÄªÊçüÂ§±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊîπËøõÁöÑËÆ≠ÁªÉÂæ™ÁéØ =============\n",
    "def improved_train(dataset, hparas, start_epoch=0):\n",
    "    \"\"\"\n",
    "    ÊîπËøõÁöÑËÆ≠ÁªÉÂæ™ÁéØÔºö\n",
    "    1. Êõ¥Â•ΩÁöÑËøõÂ∫¶ÁõëÊéß\n",
    "    2. Ëá™Âä®‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã\n",
    "    3. ËØ¶ÁªÜÁöÑÊçüÂ§±ËÆ∞ÂΩï\n",
    "    \"\"\"\n",
    "    checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    \n",
    "    # ËÆ≠ÁªÉÂéÜÂè≤\n",
    "    history = {\n",
    "        'd_loss': [],\n",
    "        'g_loss': [],\n",
    "        'adv_loss': [],\n",
    "        'fm_loss': [],\n",
    "        'color_loss': [],\n",
    "        'div_loss': [],\n",
    "        'matching_loss': [],\n",
    "        'word_align_loss': []\n",
    "    }\n",
    "    \n",
    "    best_g_loss = float('inf')\n",
    "    \n",
    "    # ËÆ°ÁÆóÊØè‰∏™epochÁöÑÊâπÊ¨°Êï∞\n",
    "    steps_per_epoch = hparas['N_SAMPLE'] // hparas['BATCH_SIZE']\n",
    "    \n",
    "    print(\"ÂºÄÂßãÊîπËøõÁâàËÆ≠ÁªÉ...\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ÊØè‰∏™epochÁ∫¶ {steps_per_epoch} ‰∏™ÊâπÊ¨° (batch_size={hparas['BATCH_SIZE']})\")\n",
    "    print(\"‚ö†Ô∏è È¶ñÊ¨°ËøêË°åÈúÄË¶Å1-3ÂàÜÈíüÁºñËØëTensorFlowËÆ°ÁÆóÂõæÔºåËØ∑ËÄêÂøÉÁ≠âÂæÖ...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(start_epoch, hparas['N_EPOCH']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ÊØè‰∏™epochÁöÑÊçüÂ§±Á¥ØÁßØ\n",
    "        epoch_d_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_adv_loss = 0.0\n",
    "        epoch_fm_loss = 0.0\n",
    "        epoch_matching_loss = 0.0\n",
    "        epoch_word_align_loss = 0.0\n",
    "        \n",
    "        n_batches = 0\n",
    "        \n",
    "        for image_batch, caption_batch in dataset:\n",
    "            # ËÆ≠ÁªÉÂà§Âà´Âô® D_STEPS Ê¨°\n",
    "            for _ in range(hparas['D_STEPS']):\n",
    "                d_loss, g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss, word_align_loss = \\\n",
    "                    improved_train_step(image_batch, caption_batch)\n",
    "            \n",
    "            # ËÆ≠ÁªÉÁîüÊàêÂô® G_STEPS Ê¨°\n",
    "            for _ in range(hparas['G_STEPS'] - 1):\n",
    "                _, g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss, word_align_loss = \\\n",
    "                    improved_train_step(image_batch, caption_batch)\n",
    "            \n",
    "            epoch_d_loss += d_loss.numpy()\n",
    "            epoch_g_loss += g_loss.numpy()\n",
    "            epoch_adv_loss += adv_loss.numpy()\n",
    "            epoch_fm_loss += fm_loss.numpy()\n",
    "            epoch_color_loss += color_loss.numpy()\n",
    "            epoch_div_loss += div_loss.numpy()\n",
    "            epoch_matching_loss += matching_loss.numpy()\n",
    "            epoch_word_align_loss += word_align_loss.numpy()\n",
    "            \n",
    "            n_batches += 1\n",
    "            \n",
    "            # ÊòæÁ§∫ËÆ≠ÁªÉËøõÂ∫¶ÔºàÊØè10‰∏™ÊâπÊ¨°Ôºâ\n",
    "            if n_batches % 10 == 0:\n",
    "                avg_d = epoch_d_loss / n_batches\n",
    "                avg_g = epoch_g_loss / n_batches\n",
    "                print(f'  Batch [{n_batches:3d}/{steps_per_epoch}] - D: {avg_d:.4f}, G: {avg_g:.4f}', end='\\r')\n",
    "            \n",
    "            # ÈôêÂà∂ÊØè‰∏™epochÁöÑÊâπÊ¨°Êï∞ÔºåÈÅøÂÖçÂ§™ÊÖ¢\n",
    "            if n_batches >= steps_per_epoch:\n",
    "                break\n",
    "        \n",
    "        # ËÆ°ÁÆóÂπ≥ÂùáÊçüÂ§±\n",
    "        epoch_d_loss /= n_batches\n",
    "        epoch_g_loss /= n_batches\n",
    "        epoch_matching_loss /= n_batches\n",
    "        epoch_word_align_loss /= n_batches\n",
    "        \n",
    "        # ËÆ∞ÂΩïÂéÜÂè≤\n",
    "        history['d_loss'].append(epoch_d_loss)\n",
    "        history['g_loss'].append(epoch_g_loss)\n",
    "        history['adv_loss'].append(epoch_adv_loss)\n",
    "        history['fm_loss'].append(epoch_fm_loss)\n",
    "        history['color_loss'].append(epoch_color_loss)\n",
    "        history['div_loss'].append(epoch_div_loss)\n",
    "        history['matching_loss'].append(epoch_matching_loss)\n",
    "        history['word_align_loss'].append(epoch_word_align_loss)\n",
    "        \n",
    "        # ÊâìÂç∞ËøõÂ∫¶ÔºàÊØè‰∏™epochÈÉΩËæìÂá∫Ôºâ\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{hparas[\"N_EPOCH\"]} - {epoch_time:.2f}s - Batches: {n_batches}')\n",
    "        print(f'  D_loss: {epoch_d_loss:.4f}')\n",
    "        print(f'  G_loss: {epoch_g_loss:.4f} [Adv: {epoch_adv_loss:.4f}, FM: {epoch_fm_loss:.4f}, '\n",
    "              f'Color: {epoch_color_loss:.4f}, Div: {epoch_div_loss:.4f}]')\n",
    "        print(f'  Text-Image Alignment: [Matching: {epoch_matching_loss:.4f}, WordAlign: {epoch_word_align_loss:.4f}]')\n",
    "        \n",
    "        # ÊØè‰∏™epochÈÉΩÁîüÊàêÊ†∑Êú¨ÂõæÂÉè\n",
    "        print(f'  ‚Üí ÁîüÊàêÊ†∑Êú¨ÂõæÂÉè...')\n",
    "        generate_and_save_images_improved(\n",
    "            text_encoder_improved, \n",
    "            generator_improved,\n",
    "            epoch + 1,\n",
    "            sample_sentence,\n",
    "            sample_seed\n",
    "        )\n",
    "        # ÊØè10‰∏™epoch‰øùÂ≠òcheckpoint\n",
    "        # ÊØè10‰∏™epoch‰øùÂ≠òcheckpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_improved.save(file_prefix=checkpoint_prefix)\n",
    "            print(f'  ‚Üí Checkpoint saved at epoch {epoch+1}')\n",
    "        \n",
    "        # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã\n",
    "        if epoch_g_loss < best_g_loss:\n",
    "            best_g_loss = epoch_g_loss\n",
    "            checkpoint_improved.save(file_prefix=checkpoint_prefix + '_best')\n",
    "            print(f'  ‚òÖ Best model saved! (G_loss: {best_g_loss:.4f})')\n",
    "        \n",
    "        print('-'*60)\n",
    "\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "        return history\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"ËÆ≠ÁªÉÂÆåÊàêÔºÅ\")\n",
    "    return history    return history\n",
    "\n",
    "    print(\"ËÆ≠ÁªÉÂÆåÊàêÔºÅ\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊîπËøõÁöÑÂõæÂÉèÁîüÊàêÂáΩÊï∞ =============\n",
    "def generate_and_save_images_improved(text_encoder, generator, epoch, sample_sentences, sample_seed):\n",
    "    \"\"\"ÁîüÊàêÂπ∂‰øùÂ≠òÊ†∑Êú¨ÂõæÂÉè\"\"\"\n",
    "    \n",
    "    # ÂáÜÂ§áÊ†∑Êú¨\n",
    "    for caption_batch in sample_sentences:\n",
    "        # ÁºñÁ†ÅÊñáÊú¨\n",
    "        word_features, sentence_feature = text_encoder(caption_batch, training=False)\n",
    "        \n",
    "        # ÁîüÊàêÂõæÂÉè\n",
    "        noise = tf.constant(sample_seed, dtype=tf.float32)\n",
    "        fake_images, attention_weights = generator(\n",
    "            word_features, sentence_feature, noise, training=False\n",
    "        )\n",
    "        \n",
    "        # ‰øùÂ≠òÂõæÂÉè\n",
    "        save_path = f'samples/improved_v2/train_{epoch:04d}.png'\n",
    "        save_images(fake_images.numpy(), [ni, ni], save_path)\n",
    "        \n",
    "        print(f'    Ê†∑Êú¨Â∑≤‰øùÂ≠ò: {save_path}')\n",
    "        break  # Âè™ÁîüÊàê‰∏Ä‰∏™ÊâπÊ¨°\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def improved_test_step(caption, noise):\n",
    "    \"\"\"ÊîπËøõÁöÑÊµãËØïÊ≠•È™§\"\"\"\n",
    "    word_features, sentence_feature = text_encoder_improved(caption, training=False)\n",
    "    fake_image, _ = generator_improved(word_features, sentence_feature, noise, training=False)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÂºÄÂßãÊîπËøõÁâàËÆ≠ÁªÉ =============\n",
    "# Ê≥®ÊÑèÔºöÂ¶ÇÊûúË¶Å‰ªécheckpointÊÅ¢Â§çÔºåÂèñÊ∂à‰∏ãÈù¢ÁöÑÊ≥®Èáä\n",
    "# latest_checkpoint = tf.train.latest_checkpoint(hparas['CHECKPOINTS_DIR'])\n",
    "# if latest_checkpoint:\n",
    "#     checkpoint_improved.restore(latest_checkpoint)\n",
    "#     print(f\"‰ªécheckpointÊÅ¢Â§ç: {latest_checkpoint}\")\n",
    "\n",
    "# ÈáçÊñ∞ÂàõÂª∫Êï∞ÊçÆÈõÜÔºà‰ΩøÁî®‰πãÂâçÂÆö‰πâÁöÑdataset_generatorÔºâ\n",
    "print(\"ÂáÜÂ§áËÆ≠ÁªÉÊï∞ÊçÆÈõÜ...\")\n",
    "training_dataset_improved = dataset_generator(\n",
    "    data_path + '/text2ImgData.pkl', \n",
    "    hparas['BATCH_SIZE'], \n",
    "    training_data_generator\n",
    ")\n",
    "\n",
    "# ÂáÜÂ§áÊ†∑Êú¨Êï∞ÊçÆÔºàÁî®‰∫éÁîüÊàêÊ†∑Êú¨ÂõæÂÉèÔºâ\n",
    "print(\"ÂáÜÂ§áÊ†∑Êú¨Êï∞ÊçÆ...\")\n",
    "sample_captions = [\n",
    "    \"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "    \"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "    \"the petals on this flower are white with a yellow center\",\n",
    "    \"this flower has a lot of small round pink petals.\"\n",
    "]\n",
    "\n",
    "# ËΩ¨Êç¢‰∏∫IDÂπ∂Êâ©Â±ïÂà∞batch_size\n",
    "sample_caption_ids = []\n",
    "for caption in sample_captions:\n",
    "    ids = sent2IdList(caption)\n",
    "    # ËΩ¨Êç¢‰∏∫Êï¥Êï∞\n",
    "    ids = [int(x) for x in ids]\n",
    "    sample_caption_ids.append(ids)\n",
    "\n",
    "# Êâ©Â±ïÂà∞batch_size\n",
    "sample_caption_ids = sample_caption_ids * (hparas['BATCH_SIZE'] // len(sample_caption_ids))\n",
    "sample_sentence = [tf.constant(sample_caption_ids, dtype=tf.int32)]\n",
    "\n",
    "# Âõ∫ÂÆöÁöÑÈöèÊú∫ÁßçÂ≠êÔºàÁî®‰∫éÁîüÊàê‰∏ÄËá¥ÁöÑÊ†∑Êú¨Ôºâ\n",
    "sample_seed = tf.random.normal([hparas['BATCH_SIZE'], hparas['Z_DIM']])\n",
    "\n",
    "# ËÆ°ÁÆóniÔºàÁî®‰∫é‰øùÂ≠òÂõæÂÉèÁöÑÁΩëÊ†ºÂ§ßÂ∞èÔºâ\n",
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "\n",
    "print(\"\\nÂºÄÂßãËÆ≠ÁªÉÊîπËøõÁâàÊ®°ÂûãÔºÅ\")\n",
    "print(\"=\"*60)\n",
    "print(\"ÂÖ≥ÈîÆÊîπËøõÔºö\")\n",
    "print(\"  ‚úì ËØçÁ∫ßÊ≥®ÊÑèÂäõÊú∫Âà∂ - ÂÖ≥Ê≥®ÊñáÊú¨ÂÖ≥ÈîÆËØç\")\n",
    "print(\"  ‚úì ÊÆãÂ∑ÆËøûÊé• - Êõ¥Ê∑±ÁöÑÁΩëÁªúÁªìÊûÑ\")\n",
    "print(\"  ‚úì Ëá™Ê≥®ÊÑèÂäõÂ±Ç - ÂÖ≥Ê≥®ÂõæÂÉè‰∏çÂêåÂå∫Âüü\")\n",
    "print(\"  ‚úì ÁâπÂæÅÂåπÈÖçÊçüÂ§± - ÁîüÊàêÊõ¥ÁúüÂÆûÁöÑÁâπÂæÅ\")\n",
    "print(\"  ‚úì È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§± - Êõ¥Ëá™ÁÑ∂ÁöÑÈ¢úËâ≤\")\n",
    "print(\"  ‚úì Â§öÊ†∑ÊÄßÊ≠£ÂàôÂåñ - Èò≤Ê≠¢Ê®°ÂºèÂ¥©Ê∫É\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# ÂºÄÂßãËÆ≠ÁªÉ\n",
    "history_improved = improved_train(training_dataset_improved, hparas, start_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù ËÆ≠ÁªÉÂª∫ËÆÆ‰∏éÊ≥®ÊÑè‰∫ãÈ°π\n",
    "\n",
    "### ÁªßÁª≠ËÆ≠ÁªÉÁ≠ñÁï•\n",
    "Áî±‰∫éÂ∑≤ÁªèËÆ≠ÁªÉ‰∫Ü55‰∏™epoch‰∏îË¥®Èáè‰∏çÈîô,Âª∫ËÆÆ:\n",
    "1. **ËΩΩÂÖ•‰πãÂâçÁöÑÊúÄ‰Ω≥checkpoint** (Epoch 2ÁöÑÊ®°Âûã)\n",
    "2. **Èôç‰ΩéÂ≠¶‰π†ÁéáÁªßÁª≠ËÆ≠ÁªÉ** - Êñ∞ÊçüÂ§±ÈúÄË¶Åfine-tune\n",
    "3. **ËßÇÂØüÊñ∞ÊçüÂ§±È°πÁöÑÂèòÂåñË∂ãÂäø**\n",
    "\n",
    "```python\n",
    "# ‰ªéÊúÄ‰Ω≥checkpointÊÅ¢Â§ç\n",
    "best_ckpt = tf.train.latest_checkpoint(hparas['CHECKPOINTS_DIR'] + '_best')\n",
    "if best_ckpt:\n",
    "    checkpoint_improved.restore(best_ckpt)\n",
    "    print(f\"‰ªéÊúÄ‰Ω≥checkpointÊÅ¢Â§ç: {best_ckpt}\")\n",
    "\n",
    "# Èôç‰ΩéÂ≠¶‰π†Áéá (ÂéüÊù•ÊòØ1e-4Âíå4e-4)\n",
    "generator_optimizer_improved.learning_rate.assign(5e-5)\n",
    "discriminator_optimizer_improved.learning_rate.assign(2e-4)\n",
    "print(\"Â≠¶‰π†ÁéáÂ∑≤Èôç‰ΩéËá≥: G=5e-5, D=2e-4\")\n",
    "\n",
    "# ÁªßÁª≠ËÆ≠ÁªÉ\n",
    "history = improved_train(training_dataset, hparas, start_epoch=55)\n",
    "```\n",
    "\n",
    "### ÂÖ≥ÈîÆÁõëÊéßÊåáÊ†á\n",
    "ËÆ≠ÁªÉÊó∂ÈáçÁÇπËßÇÂØü:\n",
    "- **Matching Loss**: Â∫îËØ•ÈÄêÊ∏ê‰∏ãÈôç (ÁõÆÊ†á < 1.0)\n",
    "- **WordAlign Loss**: Â∫îËØ•ÈÄêÊ∏ê‰∏ãÈôç (ÁõÆÊ†á < 0.5)  \n",
    "- **Adv Loss**: ‰øùÊåÅÁ®≥ÂÆöÊàñÁï•ÂæÆ‰∏ãÈôç\n",
    "- **FM Loss**: ‰øùÊåÅÂú®0.004-0.006‰πãÈó¥\n",
    "\n",
    "### ËøáÊãüÂêàË≠¶Á§∫\n",
    "Â¶ÇÊûúÂá∫Áé∞‰ª•‰∏ãÊÉÖÂÜµ,ÂÅúÊ≠¢ËÆ≠ÁªÉ:\n",
    "- Matching LossÈôçÂà∞Êé•Ëøë0 (ËøáÂ∫¶ÊãüÂêà)\n",
    "- ÁîüÊàêÁöÑÂõæÂÉèÂºÄÂßãÂá∫Áé∞ÈáçÂ§çÊ®°Âºè\n",
    "- D_lossÊåÅÁª≠‰Ωé‰∫é0.5\n",
    "\n",
    "### ËØÑ‰º∞ÊîπËøõÊïàÊûú\n",
    "ÁîüÊàêÊµãËØïÂõæÂÉèÂêé:\n",
    "```python\n",
    "# ÂØπÊØîÊèèËø∞Êü•ÁúãÂØπÈΩêÂ∫¶\n",
    "test_captions = [\"red petals with yellow center\", \n",
    "                 \"purple flower with green leaves\"]\n",
    "# ÁîüÊàêÂπ∂‰∫∫Â∑•Ê£ÄÊü•ÊòØÂê¶Á¨¶ÂêàÊèèËø∞\n",
    "```\n",
    "\n",
    "ËÆ≠ÁªÉÂÆåÊàêÂêé‰ΩøÁî®`inception_score.py`ËØÑÂàÜ,È¢ÑÊúüÂàÜÊï∞ÊèêÂçá10-20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÂèØËßÜÂåñÊçüÂ§±ÊùÉÈáçÈÖçÁΩÆ =============\n",
    "print(\"üìä Êñ∞Â¢ûÊçüÂ§±È°πÊùÉÈáçÈÖçÁΩÆ:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'ÊçüÂ§±È°π':<30} {'ÊùÉÈáç':<10} {'‰ΩúÁî®'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'LAMBDA_FM (ÁâπÂæÅÂåπÈÖç)':<30} {hparas['LAMBDA_FM']:<10} ÁîüÊàêÁúüÂÆûÁâπÂæÅ\")\n",
    "print(f\"{'LAMBDA_COLOR (È¢úËâ≤‰∏ÄËá¥)':<30} {hparas['LAMBDA_COLOR']:<10} Ëá™ÁÑ∂Ëâ≤ÂΩ©\")\n",
    "print(f\"{'LAMBDA_DIV (Â§öÊ†∑ÊÄß)':<30} {hparas['LAMBDA_DIV']:<10} Èò≤Ê≠¢Â¥©Ê∫É\")\n",
    "print(f\"{'LAMBDA_MATCHING (ÊñáÊú¨ÂåπÈÖç) ‚òÖ':<30} {hparas['LAMBDA_MATCHING']:<10} ËØ≠‰πâÂØπÈΩê\")\n",
    "print(f\"{'LAMBDA_WORD_ALIGN (ËØçÂØπÈΩê) ‚òÖ':<30} {hparas['LAMBDA_WORD_ALIGN']:<10} ÂÖ≥Ê≥®ÂÖ≥ÈîÆËØç\")\n",
    "print(f\"{'LAMBDA_MISMATCH (ÈîôÈÖçÊ£ÄÊµã) ‚òÖ':<30} {hparas['LAMBDA_MISMATCH']:<10} ‰∏ÄËá¥ÊÄßÊ£ÄÊü•\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚òÖ Ê†áËÆ∞‰∏∫Êñ∞Â¢ûÁöÑÊñáÊú¨-ÂõæÂÉèÂØπÈΩêÊçüÂ§±\")\n",
    "print()\n",
    "print(\"üí° ÊèêÁ§∫:\")\n",
    "print(\"  - LAMBDA_MATCHINGËæÉÈ´ò(5.0)Á°Æ‰øùËØ≠‰πâÂØπÈΩê‰ºòÂÖàÁ∫ß\")\n",
    "print(\"  - LAMBDA_WORD_ALIGN(2.0)ÂºïÂØºÊ≥®ÊÑèÂäõÊú∫Âà∂\")\n",
    "print(\"  - LAMBDA_MISMATCH(0.5)ËæÖÂä©Âà§Âà´Âô®Â≠¶‰π†\")\n",
    "print()\n",
    "print(\"üéØ ÊîπËøõÁõÆÊ†á:\")\n",
    "print(\"  ‚úì ÁîüÊàêÁöÑËä±ÊúµÈ¢úËâ≤‰∏éÊèèËø∞ÂåπÈÖç\")\n",
    "print(\"  ‚úì Ëä±ÊúµÂΩ¢Áä∂ÁâπÂæÅÁ¨¶ÂêàÊñáÊú¨\")  \n",
    "print(\"  ‚úì Ê≥®ÊÑèÂäõËÅöÁÑ¶ÂÖ≥ÈîÆÊèèËø∞ËØç\")\n",
    "print(\"  ‚úì ËØÑÂàÜÁ≥ªÁªüÁõ∏‰ººÂ∫¶ÊèêÂçá\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÂèØËßÜÂåñËÆ≠ÁªÉÂéÜÂè≤ =============\n",
    "def plot_training_history(history):\n",
    "    \"\"\"ÁªòÂà∂ËÆ≠ÁªÉÂéÜÂè≤\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('ËÆ≠ÁªÉÂéÜÂè≤ - ÊîπËøõÁâàÊ®°Âûã', fontsize=16)\n",
    "    \n",
    "    # DÊçüÂ§±\n",
    "    axes[0, 0].plot(history['d_loss'])\n",
    "    axes[0, 0].set_title('Âà§Âà´Âô®ÊçüÂ§±')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # GÊÄªÊçüÂ§±\n",
    "    axes[0, 1].plot(history['g_loss'])\n",
    "    axes[0, 1].set_title('ÁîüÊàêÂô®ÊÄªÊçüÂ§±')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # ÂØπÊäóÊçüÂ§±\n",
    "    axes[0, 2].plot(history['adv_loss'])\n",
    "    axes[0, 2].set_title('ÂØπÊäóÊçüÂ§±')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Loss')\n",
    "    axes[0, 2].grid(True)\n",
    "    \n",
    "    # ÁâπÂæÅÂåπÈÖçÊçüÂ§±\n",
    "    axes[1, 0].plot(history['fm_loss'])\n",
    "    axes[1, 0].set_title('ÁâπÂæÅÂåπÈÖçÊçüÂ§±')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§±\n",
    "    axes[1, 1].plot(history['color_loss'])\n",
    "    axes[1, 1].set_title('È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§±')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # Â§öÊ†∑ÊÄßÊçüÂ§±\n",
    "    axes[1, 2].plot(history['div_loss'])\n",
    "    axes[1, 2].set_title('Â§öÊ†∑ÊÄßÊçüÂ§±')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Loss')\n",
    "    axes[1, 2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('samples/improved_v2/training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ËÆ≠ÁªÉÂéÜÂè≤Â∑≤‰øùÂ≠ò: samples/improved_v2/training_history.png\")\n",
    "\n",
    "# ÁªòÂà∂ËÆ≠ÁªÉÂéÜÂè≤ÔºàËÆ≠ÁªÉÂÆåÊàêÂêéË∞ÉÁî®Ôºâ\n",
    "# plot_training_history(history_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ÊµãËØïÈõÜÊï∞ÊçÆÈõÜ‰∏éÊé®ÁêÜ =============\n",
    "# Êåâ‰Ωú‰∏öËØ¥ÊòéÔºöËØªÂèñ dataset/testData.pklÔºåÁîüÊàê inference_{ID}.png\n",
    "\n",
    "\n",
    "def testing_data_generator(caption, index):\n",
    "    # ÊµãËØïÈõÜÊò†Â∞ÑÂáΩÊï∞ÔºåÁ°Æ‰øùÁ±ªÂûã‰∏ÄËá¥\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    return caption, index\n",
    "\n",
    "\n",
    "def testing_dataset_generator(data_path, batch_size, data_generator):\n",
    "    \"\"\"ÁîüÊàêÊµãËØïÈõÜdatasetÔºå‰øùÊåÅ‰∏éËÆ≠ÁªÉÂâçÂ§ÑÁêÜ‰∏ÄËá¥\"\"\"\n",
    "    data = pd.read_pickle(data_path)\n",
    "    # Captions ÂàóÂåÖÂê´Â∫èÂàóÔºàÂàóË°®/ndarrayÔºâÔºåÈúÄÁªü‰∏Ä‰∏∫ int32 Êï∞ÁªÑ\n",
    "    captions_raw = data['Captions'].values\n",
    "    captions = [np.asarray(c, dtype=np.int32) for c in captions_raw]\n",
    "    captions = np.stack(captions, axis=0)  # ÂΩ¢Áä∂: (N, seq_len)\n",
    "\n",
    "    # ID Áªü‰∏ÄËΩ¨Â≠óÁ¨¶‰∏≤ÔºåÊñπ‰æøÂêéÁª≠ÂëΩÂêç\n",
    "    ids = data['ID'].astype(str).values\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((captions, ids))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ÊûÑÂª∫ÊµãËØïÈõÜÔºà‰∏éËÆ≠ÁªÉÂâçÂ§ÑÁêÜ‰øùÊåÅ‰∏ÄËá¥Ôºâ\n",
    "testing_dataset = testing_dataset_generator(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/dataset/testData.pkl',\n",
    "    hparas['BATCH_SIZE'],\n",
    "    testing_data_generator\n",
    ")\n",
    "\n",
    "\n",
    "def improved_inference(testing_dataset, output_dir='inference/demo'):\n",
    "    \"\"\"\n",
    "    ‰ΩøÁî®ÊîπËøõÁâàÊ®°ÂûãÁîüÊàêÊµãËØïÈõÜÂõæÂÉè\n",
    "    ËæìÂá∫Êñá‰ª∂ÂêçÊ†ºÂºèÔºöinference_{ID}.pngÔºà‰∏•Ê†ºÊåâËØ¥ÊòéË¶ÅÊ±ÇÔºâ\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"ÂºÄÂßãÊîπËøõÁâàÊé®ÁêÜ...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    generated_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (caption_batch, image_name_batch) in enumerate(testing_dataset):\n",
    "        batch_size = tf.shape(caption_batch)[0]\n",
    "\n",
    "        # ÁîüÊàêÈöèÊú∫Âô™Â£∞\n",
    "        noise = tf.random.normal([batch_size, hparas['Z_DIM']])\n",
    "\n",
    "        # ÁºñÁ†ÅÊñáÊú¨\n",
    "        word_features, sentence_feature = text_encoder_improved(caption_batch, training=False)\n",
    "\n",
    "        # ÁîüÊàêÂõæÂÉè\n",
    "        fake_images, _ = generator_improved(\n",
    "            word_features, sentence_feature, noise, training=False\n",
    "        )\n",
    "\n",
    "        # ‰øùÂ≠òÊØèÂº†ÂõæÂÉè\n",
    "        for i in range(fake_images.shape[0]):\n",
    "            img = fake_images[i].numpy()\n",
    "            img = ((img + 1.0) * 127.5).astype(np.uint8)  # [-1,1] -> [0,255]\n",
    "\n",
    "            idx_val = image_name_batch[i].numpy()\n",
    "            if isinstance(idx_val, bytes):\n",
    "                idx_val = idx_val.decode('utf-8')\n",
    "            filename = f\"inference_{idx_val}.png\"\n",
    "            img_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            PIL.Image.fromarray(img).save(img_path)\n",
    "            generated_count += 1\n",
    "\n",
    "        if (step + 1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            speed = generated_count / max(elapsed, 1e-6)\n",
    "            print(f\"Â∑≤ÁîüÊàê {generated_count} Âº†ÂõæÂÉè... ({speed:.2f} Âº†/Áßí)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úì Êé®ÁêÜÂÆåÊàêÔºÅ\")\n",
    "    print(f\"  ÊÄªÂõæÂÉèÊï∞: {generated_count}\")\n",
    "    print(f\"  ÊÄªÊó∂Èó¥: {total_time:.2f} Áßí\")\n",
    "    print(f\"  Âπ≥ÂùáÈÄüÂ∫¶: {generated_count/total_time:.2f} Âº†/Áßí\")\n",
    "    print(f\"  ËæìÂá∫ÁõÆÂΩï: {output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ‰ΩøÁî®Á§∫‰æãÔºàÂÖàÂä†ËΩΩÊúÄ‰Ω≥ÊàñÊúÄÁªàcheckpointÂêéÂÜçËøêË°åÔºâ\n",
    "# best_ckpt = tf.train.latest_checkpoint(hparas['CHECKPOINTS_DIR'])\n",
    "# checkpoint_improved.restore(best_ckpt)\n",
    "# improved_inference(testing_dataset, output_dir='inference/demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã ‰ΩøÁî®ÊîπËøõÁâàÊ®°ÂûãÁöÑÂÆåÊï¥ÊµÅÁ®ã\n",
    "\n",
    "### Ê≠•È™§1: ËÆ≠ÁªÉÊ®°Âûã\n",
    "```python\n",
    "# Â∑≤ÁªèÂú®‰∏äÈù¢ÁöÑcell‰∏≠ÂÆö‰πâÔºåÁõ¥Êé•ËøêË°åËÆ≠ÁªÉ\n",
    "history_improved = improved_train(training_dataset_improved, hparas, start_epoch=0)\n",
    "```\n",
    "\n",
    "### Ê≠•È™§2: ÂèØËßÜÂåñËÆ≠ÁªÉÂéÜÂè≤\n",
    "```python\n",
    "plot_training_history(history_improved)\n",
    "```\n",
    "\n",
    "### Ê≠•È™§3: Âä†ËΩΩÊúÄ‰Ω≥Ê®°ÂûãÂπ∂Êé®ÁêÜ\n",
    "```python\n",
    "# Âä†ËΩΩÊúÄ‰Ω≥Ê®°Âûã\n",
    "best_checkpoint = hparas['CHECKPOINTS_DIR'] + '/ckpt_best-1'\n",
    "checkpoint_improved.restore(best_checkpoint)\n",
    "print(f\"Â∑≤Âä†ËΩΩÊúÄ‰Ω≥Ê®°Âûã: {best_checkpoint}\")\n",
    "\n",
    "# ÁîüÊàêÊµãËØïÈõÜÂõæÂÉè\n",
    "improved_inference(testing_dataset, output_dir='inference/improved_v2')\n",
    "```\n",
    "\n",
    "### Ê≠•È™§4: ËÆ°ÁÆóÂàÜÊï∞\n",
    "‰ΩøÁî®ÊØîËµõÊèê‰æõÁöÑËØÑÂàÜËÑöÊú¨ËØÑ‰º∞ÁîüÊàêÁöÑÂõæÂÉè„ÄÇ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ÂÖ≥ÈîÆÊîπËøõÁÇπÊÄªÁªì\n",
    "\n",
    "### 1. **Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºàAttention MechanismÔºâ**\n",
    "- **ËØçÁ∫ßÊ≥®ÊÑèÂäõ**: ËÆ©ÁîüÊàêÂô®ÂÖ≥Ê≥®ÊèèËø∞‰∏≠ÁöÑÂÖ≥ÈîÆËØçÔºàÂ¶ÇÈ¢úËâ≤„ÄÅÂΩ¢Áä∂Ôºâ\n",
    "- **Ëá™Ê≥®ÊÑèÂäõ**: ËÆ©ÁîüÊàêÂô®ÂÖ≥Ê≥®ÂõæÂÉèÁöÑ‰∏çÂêåÂå∫ÂüüÔºåÁîüÊàêÊõ¥ËøûË¥ØÁöÑÁªìÊûÑ\n",
    "\n",
    "### 2. **Êõ¥Ê∑±ÁöÑÁΩëÁªúÁªìÊûÑ**\n",
    "- **ÊÆãÂ∑ÆËøûÊé•**: Â∏ÆÂä©Ê¢ØÂ∫¶ÊµÅÂä®ÔºåÂÖÅËÆ∏ËÆ≠ÁªÉÊõ¥Ê∑±ÁöÑÁΩëÁªú\n",
    "- **ÊâπÂΩí‰∏ÄÂåñ**: Á®≥ÂÆöËÆ≠ÁªÉËøáÁ®ã\n",
    "\n",
    "### 3. **Â§öÈáçÊçüÂ§±ÂáΩÊï∞**\n",
    "- **ÂØπÊäóÊçüÂ§±**: Âü∫Êú¨ÁöÑGANÊçüÂ§±\n",
    "- **ÁâπÂæÅÂåπÈÖçÊçüÂ§±**: ËÆ©ÁîüÊàêÂõæÂÉèÁöÑÁâπÂæÅÂàÜÂ∏ÉÊé•ËøëÁúüÂÆûÂõæÂÉè\n",
    "- **È¢úËâ≤‰∏ÄËá¥ÊÄßÊçüÂ§±**: ÈºìÂä±ÁîüÊàêËá™ÁÑ∂ÁöÑÈ¢úËâ≤ËøáÊ∏°\n",
    "- **Â§öÊ†∑ÊÄßÊ≠£ÂàôÂåñ**: Èò≤Ê≠¢Ê®°ÂºèÂ¥©Ê∫ÉÔºåÂ¢ûÂä†ÁîüÊàêÂ§öÊ†∑ÊÄß\n",
    "\n",
    "### 4. **ËÆ≠ÁªÉÁ≠ñÁï•‰ºòÂåñ**\n",
    "- **‰∏çÂØπÁß∞Â≠¶‰π†Áéá**: Âà§Âà´Âô®Â≠¶‰π†ÁéáÁï•È´ò‰∫éÁîüÊàêÂô®\n",
    "- **Â§öÊ≠•ËÆ≠ÁªÉ**: ÁîüÊàêÂô®ËÆ≠ÁªÉ2Ê≠•ÔºåÂà§Âà´Âô®ËÆ≠ÁªÉ1Ê≠•\n",
    "- **Ê¢ØÂ∫¶Ë£ÅÂâ™**: Èò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏\n",
    "- **Êó©ÂÅúÊú∫Âà∂**: ÈÅøÂÖçËøáÊãüÂêà\n",
    "\n",
    "### 5. **Ê†áÁ≠æÂπ≥Êªë**\n",
    "- Èò≤Ê≠¢Âà§Âà´Âô®Ëøá‰∫éËá™‰ø°\n",
    "- ÁúüÂÆûÊ†áÁ≠æ‰ΩøÁî®[0.9, 1.0]ËåÉÂõ¥\n",
    "- ÂÅáÊ†áÁ≠æ‰ΩøÁî®[0.0, 0.1]ËåÉÂõ¥\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Ë∞É‰ºòÂª∫ËÆÆ\n",
    "\n",
    "Â¶ÇÊûúÁîüÊàêË¥®Èáè‰ªç‰∏çÁêÜÊÉ≥ÔºåÂèØ‰ª•Â∞ùËØïÔºö\n",
    "\n",
    "1. **Â¢ûÂä†ËÆ≠ÁªÉËΩÆÊï∞**: Â∞Ü`N_EPOCH`‰ªé300Â¢ûÂä†Âà∞500\n",
    "2. **Ë∞ÉÊï¥ÊçüÂ§±ÊùÉÈáç**: \n",
    "   - Â¢ûÂä†`LAMBDA_FM`ÔºàÁâπÂæÅÂåπÈÖçÊùÉÈáçÔºâÂà∞20.0\n",
    "   - Ë∞ÉÊï¥`LAMBDA_COLOR`Âà∞1.0\n",
    "3. **‰øÆÊîπÂ≠¶‰π†Áéá**:\n",
    "   - Èôç‰ΩéÁîüÊàêÂô®Â≠¶‰π†ÁéáÂà∞5e-5\n",
    "   - ‰øùÊåÅÂà§Âà´Âô®Â≠¶‰π†Áéá‰∏∫2e-4\n",
    "4. **Â¢ûÂä†ÊâπÊ¨°Â§ßÂ∞è**: Â¶ÇÊûúGPUÂÜÖÂ≠òÂÖÅËÆ∏ÔºåÂ¢ûÂä†Âà∞128\n",
    "5. **Êï∞ÊçÆÂ¢ûÂº∫**: Ëøõ‰∏ÄÊ≠•Âä†Âº∫Êï∞ÊçÆÂ¢ûÂº∫ÔºàÂú®dataset_generator‰∏≠Ôºâ\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Ê≥®ÊÑè‰∫ãÈ°π\n",
    "\n",
    "1. ËÆ≠ÁªÉÊó∂Èó¥‰ºöÊØîÂéüÂßãÊ®°ÂûãÈïøÔºåÂõ†‰∏∫Ê®°ÂûãÊõ¥Â§çÊùÇ\n",
    "2. Âª∫ËÆÆ‰ΩøÁî®GPUËÆ≠ÁªÉÔºàÂú®Colab‰∏ä‰ΩøÁî®T4ÊàñÊõ¥Â•ΩÁöÑGPUÔºâ\n",
    "3. ÂÆöÊúüÊ£ÄÊü•ÁîüÊàêÁöÑÊ†∑Êú¨ÂõæÂÉèÔºåÁ°Æ‰øùÊ®°ÂûãÊ≤°ÊúâÊ®°ÂºèÂ¥©Ê∫É\n",
    "4. Â¶ÇÊûúÂá∫Áé∞Ê¢ØÂ∫¶ÁàÜÁÇ∏ÔºåÂèØ‰ª•Èôç‰ΩéÂ≠¶‰π†Áéá\n",
    "5. ÊúÄ‰Ω≥Ê®°Âûã‰ºöËá™Âä®‰øùÂ≠ò‰∏∫`ckpt_best`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
