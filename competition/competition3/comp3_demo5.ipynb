{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”´ é¢œè‰²ç†è§£é”™è¯¯çš„æ ¹æœ¬åŸå› åˆ†æ\n",
    "\n",
    "## é—®é¢˜æè¿°\n",
    "\n",
    "```\n",
    "æ–‡æœ¬: \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "      ï¼ˆèŠ±æœµæœ‰é»„è‰²èŠ±è¯ã€çº¢è‰²é›Œè•Šã€é²œçº¢è‰²èŠ±ç“£ï¼‰\n",
    "\n",
    "ç”Ÿæˆç»“æœ: ä¸»é¢˜ä¸ºé»„è‰²çš„èŠ± âŒ\n",
    "é¢„æœŸç»“æœ: ä¸»é¢˜ä¸ºçº¢è‰²çš„èŠ± âœ…\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” æ ¹æœ¬åŸå› ï¼ˆä¸‰é‡é—®é¢˜ï¼‰\n",
    "\n",
    "### é—®é¢˜1ï¼šæ–‡æœ¬ç¼–ç å™¨çš„\"é¡ºåºåå¥½\" âš ï¸\n",
    "\n",
    "**BiLSTMçš„å·¥ä½œæ–¹å¼ï¼š**\n",
    "```python\n",
    "sentence = \"yellow anther red pistil bright red petals\"\n",
    "           â†“ BiLSTMå¤„ç†\n",
    "forward:   yellow â†’ anther â†’ red â†’ pistil â†’ bright â†’ red â†’ petals\n",
    "           â†‘æ—©æœŸè¯å½±å“åŠ›å¤§\n",
    "backward:  petals â†’ red â†’ bright â†’ pistil â†’ red â†’ anther â†’ yellow\n",
    "                                                              â†‘æ—©æœŸè¯å½±å“åŠ›å¤§\n",
    "\n",
    "sentence_feature = concat([forwardæœ€åçŠ¶æ€, backwardæœ€åçŠ¶æ€])\n",
    "```\n",
    "\n",
    "**ç»“æœï¼š**\n",
    "- âœ… å‰å‘LSTMï¼šæœ€åçœ‹åˆ°\"petals\"ï¼ˆèŠ±ç“£ï¼‰ï¼Œç†è§£\"çº¢è‰²\"\n",
    "- âŒ åå‘LSTMï¼šæœ€åçœ‹åˆ°\"yellow\"ï¼ˆé»„è‰²ï¼‰ï¼Œè¢«å¼ºåŒ–\n",
    "- âš ï¸ æœ€ç»ˆç‰¹å¾ï¼šä¸¤è€…æ··åˆ â†’ **é»„è‰²è¢«è¿‡åº¦å¼ºè°ƒ**\n",
    "\n",
    "### é—®é¢˜2ï¼šè¯é¢‘ç»Ÿè®¡å¯¼è‡´çš„åå·® ğŸ“Š\n",
    "\n",
    "åœ¨è®­ç»ƒæ•°æ®é›†ä¸­ï¼Œè®©æˆ‘ä»¬å‡è®¾ï¼š\n",
    "```python\n",
    "é¢œè‰²è¯é¢‘ç»Ÿè®¡ï¼ˆå¯èƒ½ï¼‰ï¼š\n",
    "- \"yellow\": 2000æ¬¡\n",
    "- \"red\": 1500æ¬¡\n",
    "- \"bright red\": 800æ¬¡\n",
    "\n",
    "ä½ç½®è¯é¢‘ç»Ÿè®¡ï¼š\n",
    "- \"petals\": 5000æ¬¡ï¼ˆæœ€å¸¸è§ï¼‰\n",
    "- \"anther\": 1200æ¬¡ï¼ˆè¾ƒå°‘è§ï¼‰\n",
    "- \"pistil\": 800æ¬¡ï¼ˆç½•è§ï¼‰\n",
    "```\n",
    "\n",
    "**æ¨¡å‹å­¦åˆ°ä»€ä¹ˆï¼Ÿ**\n",
    "```\n",
    "\"yellow\" + é«˜é¢‘ â†’ å¼ºçƒˆçš„é¢œè‰²ä¿¡å·\n",
    "\"red\" + ä¸­é¢‘ â†’ ä¸­ç­‰å¼ºåº¦ä¿¡å·\n",
    "\"petals\" + æé«˜é¢‘ â†’ ä½†æ˜¯è¯ä¹‰å¤ªæ³›ï¼ˆç™½èŠ±ç“£ã€çº¢èŠ±ç“£ã€é»„èŠ±ç“£éƒ½æœ‰ï¼‰\n",
    "\n",
    "ç»“æœï¼šæ¨¡å‹æŠ“ä½\"yellow\"è¿™ä¸ªæ˜¾è‘—ä¿¡å·ï¼Œå¿½è§†\"red petals\"çš„ç»„åˆ\n",
    "```\n",
    "\n",
    "### é—®é¢˜3ï¼šMatching Lossçš„å…¨å±€ç‰¹å¾é—®é¢˜ ğŸ¯\n",
    "\n",
    "**å½“å‰å®ç°ï¼š**\n",
    "```python\n",
    "# text_image_alignment_losså‡½æ•°ä¸­\n",
    "text_features = sentence_feature  # (batch, 512) â† å…¨å±€å¥å­ç‰¹å¾\n",
    "image_features = discriminatorè¾“å‡º  # (batch, H, W, C) â†’ pooling â†’ (batch, C)\n",
    "\n",
    "# å¯¹æ¯”å­¦ä¹ \n",
    "similarity = cosine(image_features, text_features)\n",
    "```\n",
    "\n",
    "**é—®é¢˜åœ¨å“ªï¼Ÿ**\n",
    "```\n",
    "sentence_featureæ˜¯æ•´ä¸ªå¥å­çš„\"å¹³å‡ç†è§£\"\n",
    "= \"yellow\" + \"anther\" + \"red\" + \"pistil\" + \"bright\" + \"red\" + \"petals\"\n",
    "= ä¸€ä¸ªæ··åˆå‘é‡\n",
    "\n",
    "è¿™ä¸ªå‘é‡æ— æ³•ç²¾ç¡®è¡¨ç¤ºï¼š\n",
    "âœ… \"èŠ±ç“£æ˜¯çº¢è‰²çš„\" ï¼ˆå…³é”®é¢œè‰²å±æ€§ï¼‰\n",
    "âŒ åªèƒ½è¡¨ç¤ºï¼š\"è¿™ä¸ªå¥å­æåˆ°äº†é»„è‰²å’Œçº¢è‰²\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š ä¸ºä»€ä¹ˆ\"yellow\"è¢«ä¼˜å…ˆé€‰æ‹©ï¼Ÿ\n",
    "\n",
    "### ä¿¡æ¯è®ºè§’åº¦\n",
    "\n",
    "```python\n",
    "åœ¨sentence_featureå‘é‡ä¸­çš„ä¿¡æ¯é‡ï¼š\n",
    "\n",
    "\"yellow anther\" â†’ ä¿¡æ¯é‡é«˜ï¼ˆç½•è§ç»„åˆï¼Œç‰¹å¾æ˜æ˜¾ï¼‰\n",
    "  â†“\n",
    "  æ¨¡å‹è®¤ä¸ºï¼šé»„è‰²æ˜¯ç‹¬ç‰¹ä¿¡å·ï¼\n",
    "\n",
    "\"red petals\" â†’ ä¿¡æ¯é‡ä¸­ç­‰ï¼ˆå¸¸è§ç»„åˆï¼‰\n",
    "  â†“\n",
    "  æ¨¡å‹è®¤ä¸ºï¼šçº¢è‰²èŠ±ç“£å¾ˆæ™®é€šï¼Œä¸æ˜¯å…³é”®ç‰¹å¾\n",
    "```\n",
    "\n",
    "### æ³¨æ„åŠ›æƒé‡åˆ†æï¼ˆå‡è®¾ï¼‰\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼Œå¯èƒ½çœ‹åˆ°ï¼š\n",
    "```\n",
    "Word Attention Weights:\n",
    "yellow  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.35  â† æœ€é«˜ï¼\n",
    "anther  â–ˆâ–ˆâ–ˆâ–ˆ 0.12\n",
    "red     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.18\n",
    "pistil  â–ˆâ–ˆâ–ˆ 0.08\n",
    "bright  â–ˆâ–ˆâ–ˆâ–ˆ 0.10\n",
    "red     â–ˆâ–ˆâ–ˆâ–ˆ 0.10\n",
    "petals  â–ˆâ–ˆâ–ˆ 0.07\n",
    "\n",
    "ä¸ºä»€ä¹ˆï¼Ÿ\n",
    "- \"yellow\"åœ¨å¥é¦– â†’ BiLSTMæ—©æœŸå¼ºåŒ–\n",
    "- \"yellow anther\"ç½•è§ç»„åˆ â†’ æ˜¾è‘—æ€§é«˜\n",
    "- \"red\"é‡å¤2æ¬¡ â†’ åˆ†æ•£äº†æ³¨æ„åŠ›æƒé‡\n",
    "- \"petals\"å¤ªå¸¸è§ â†’ æƒé‡è¢«ç¨€é‡Š\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ æ ¹æœ¬é—®é¢˜æ€»ç»“\n",
    "\n",
    "| å±‚é¢ | é—®é¢˜ | å¯¼è‡´ |\n",
    "|-----|-----|------|\n",
    "| **è¯çº§** | BiLSTMé¡ºåºåå¥½ | å¥é¦–\"yellow\"è¢«è¿‡åº¦å¼ºåŒ– |\n",
    "| **ç»Ÿè®¡** | \"yellow\"ç½•è§â†’æ˜¾è‘— | æ¨¡å‹è®¤ä¸ºæ˜¯å…³é”®ç‰¹å¾ |\n",
    "| **è¯­ä¹‰** | å…¨å±€sentence_feature | æ— æ³•åŒºåˆ†\"ä¸»ä½“é¢œè‰²\" vs \"å±€éƒ¨é¢œè‰²\" |\n",
    "| **æŸå¤±** | Matching Losså…¨å±€å¯¹é½ | åªå­¦åˆ°\"åŒ…å«è¿™äº›é¢œè‰²è¯\"ï¼Œè€Œé\"ä¸»ä½“æ˜¯ä»€ä¹ˆé¢œè‰²\" |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ ç±»æ¯”ç†è§£\n",
    "\n",
    "```\n",
    "ä½ é—®AIï¼š\"è¿™å¹…ç”»æœ‰é»„è‰²è¾¹æ¡†ã€çº¢è‰²ä¸»ä½“ã€é²œçº¢è‰²ç»†èŠ‚\"\n",
    "AIç†è§£æˆï¼š\n",
    "  âœ“ \"è¿™å¹…ç”»æåˆ°äº†é»„è‰²\"\n",
    "  âœ“ \"è¿™å¹…ç”»æåˆ°äº†çº¢è‰²\"\n",
    "  âœ— \"æ‰€ä»¥æˆ‘ç”»ä¸€ä¸ªé»„è‰²ä¸ºä¸»çš„å›¾\"ï¼ˆå› ä¸ºé»„è‰²æ›´æ˜¾è‘—ï¼‰\n",
    "\n",
    "è€Œä¸æ˜¯ç†è§£æˆï¼š\n",
    "  âœ“ \"ä¸»ä½“=çº¢è‰²ï¼ˆpetalsï¼‰\"\n",
    "  âœ“ \"ç»†èŠ‚=é»„è‰²ï¼ˆantherï¼‰\"\n",
    "  âœ“ \"ç”»ä¸€ä¸ªçº¢è‰²èŠ±ï¼Œå¸¦é»„è‰²èŠ±è¯\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e3GwtB7e7-s"
   },
   "source": [
    "# ğŸš€ æ”¹è¿›ç‰ˆText-to-Image GAN - æ‰§è¡ŒæŒ‡å—\n",
    "\n",
    "## ğŸ“‹ æŒ‰é¡ºåºè¿è¡Œä»¥ä¸‹cellsï¼š\n",
    "\n",
    "### é˜¶æ®µ1ï¼šç¯å¢ƒå’Œæ•°æ®å‡†å¤‡\n",
    "1. Cell 2 - æŒ‚è½½Google Drive\n",
    "2. Cell 3 - å¯¼å…¥åº“å’ŒGPUé…ç½®  \n",
    "3. Cell 4 - åŠ è½½è¯æ±‡è¡¨\n",
    "4. Cell 5 - æ–‡æœ¬é¢„å¤„ç†å‡½æ•°\n",
    "5. Cell 6 - åŠ è½½è®­ç»ƒæ•°æ®\n",
    "\n",
    "### é˜¶æ®µ2ï¼šæ•°æ®å¢å¼ºå’Œæ•°æ®é›†\n",
    "6. Cell 7 - å®šä¹‰æ•°æ®ç”Ÿæˆå™¨å’Œå¢å¼ºå‡½æ•°\n",
    "\n",
    "### é˜¶æ®µ3ï¼šæ”¹è¿›çš„æ¨¡å‹å®šä¹‰\n",
    "7. Cell 8 - æ¡ä»¶å¢å¼ºå’Œæ”¹è¿›çš„æ–‡æœ¬ç¼–ç å™¨\n",
    "8. Cell 9 - æ”¹è¿›çš„ç”Ÿæˆå™¨ï¼ˆå¸¦æ³¨æ„åŠ›æœºåˆ¶ï¼‰\n",
    "9. Cell 10 - æ”¹è¿›çš„åˆ¤åˆ«å™¨ï¼ˆå¸¦è‡ªæ³¨æ„åŠ›ï¼‰\n",
    "10. Cell 11 - è¶…å‚æ•°é…ç½®\n",
    "11. Cell 12 - åˆ›å»ºæ•°æ®é›†\n",
    "\n",
    "### é˜¶æ®µ4ï¼šæŸå¤±å‡½æ•°å’Œåˆå§‹åŒ–\n",
    "12. Cell 13 - å®šä¹‰å¢å¼ºçš„æŸå¤±å‡½æ•°\n",
    "13. Cell 14 - åˆå§‹åŒ–æ”¹è¿›çš„æ¨¡å‹\n",
    "\n",
    "### é˜¶æ®µ5ï¼šè®­ç»ƒ\n",
    "14. Cell 15 - å®šä¹‰æ”¹è¿›çš„è®­ç»ƒæ­¥éª¤\n",
    "15. Cell 16 - å®šä¹‰æ”¹è¿›çš„è®­ç»ƒå¾ªç¯\n",
    "16. Cell 17 - è¾…åŠ©å‡½æ•°ï¼ˆç”Ÿæˆå’Œä¿å­˜å›¾åƒï¼‰\n",
    "17. Cell 18 - å‡†å¤‡æ ·æœ¬æ•°æ®\n",
    "18. Cell 19 - **ğŸ”¥ å¼€å§‹è®­ç»ƒ**ï¼ˆé¢„è®¡6-10å°æ—¶ï¼‰\n",
    "\n",
    "### é˜¶æ®µ6ï¼šå¯è§†åŒ–å’Œæ¨ç†ï¼ˆè®­ç»ƒå®Œæˆåï¼‰\n",
    "19. Cell 20 - å¯è§†åŒ–è®­ç»ƒå†å²\n",
    "20. Cell 21 - å®šä¹‰æ”¹è¿›ç‰ˆæ¨ç†å‡½æ•°\n",
    "21. Cell 22 - åŠ è½½æµ‹è¯•æ•°æ®\n",
    "22. Cell 23 - åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "23. Cell 24 - è¿è¡Œæ¨ç†ç”Ÿæˆæµ‹è¯•å›¾åƒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å…³é”®æ”¹è¿›ï¼š\n",
    "âœ… è¯çº§æ³¨æ„åŠ›æœºåˆ¶  \n",
    "âœ… æ®‹å·®è¿æ¥å’Œè‡ªæ³¨æ„åŠ›å±‚  \n",
    "âœ… ç‰¹å¾åŒ¹é…æŸå¤±  \n",
    "âœ… é¢œè‰²ä¸€è‡´æ€§å’Œå¤šæ ·æ€§æ­£åˆ™åŒ–  \n",
    "âœ… è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹  \n",
    "âœ… æ—©åœæœºåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRB8j2wez71r",
    "outputId": "ebc59911-c640-473c-9596-4c6dfa92f310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KCVwdp456Ta",
    "outputId": "b2f71cdd-bae6-42fb-aef3-a7ddd551a51c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-bLeJOw5__R",
    "outputId": "b0d9019a-c851-4f44-9ca6-e48ed74f6cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngI0VhKu756m"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "FbeBceM58JlP",
    "outputId": "bd6a10ee-6cc9-4fed-8922-8bfad7145a78"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7370,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2360,\n        \"min\": 1,\n        \"max\": 8188,\n        \"num_unique_values\": 7370,\n        \"samples\": [\n          2488,\n          3823,\n          5489\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Captions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ImagePath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7370,\n        \"samples\": [\n          \"./102flowers/image_02488.jpg\",\n          \"./102flowers/image_03823.jpg\",\n          \"./102flowers/image_05489.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-be97f9e5-7d5f-4065-8aff-4325feca2b65\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be97f9e5-7d5f-4065-8aff-4325feca2b65')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-be97f9e5-7d5f-4065-8aff-4325feca2b65 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-be97f9e5-7d5f-4065-8aff-4325feca2b65');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-9f9b090e-ce4e-429c-bed5-7e9de7899ea2\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9f9b090e-ce4e-429c-bed5-7e9de7899ea2')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-9f9b090e-ce4e-429c-bed5-7e9de7899ea2 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LmYPj7o8RRX"
   },
   "outputs": [],
   "source": [
    "# æ”¹è¿›çš„æ•°æ®ç”Ÿæˆå™¨ - æ·»åŠ æ•°æ®å¢å¼º\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "\n",
    "    # æ•°æ®å¢å¼º - éšæœºè£å‰ªå’Œç¿»è½¬\n",
    "    img = tf.image.resize(img, size=[72, 72])  # å…ˆæ”¾å¤§ä¸€ç‚¹\n",
    "    img = tf.image.random_crop(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption).astype(int)\n",
    "\n",
    "    # é¡¹ç›®æ ¹ç›®å½•ï¼ˆæœ‰ 102flowers é‚£ä¸€å±‚çš„é‚£ä¸ªç›®å½•ï¼‰\n",
    "    PROJECT_ROOT = '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption'\n",
    "\n",
    "    # åŸæ¥çš„ ImagePath é‡Œå¤§æ¦‚ç‡æ˜¯åƒ \"./102flowers/image_06734.jpg\" è¿™æ ·çš„ç›¸å¯¹è·¯å¾„\n",
    "    # æŠŠå®ƒå˜æˆç»å¯¹è·¯å¾„\n",
    "    image_path_series = df['ImagePath'].apply(\n",
    "        lambda p: os.path.join(PROJECT_ROOT, p.lstrip('./'))\n",
    "    )\n",
    "    image_path = image_path_series.values\n",
    "\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efazdiBj8YON"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOs7cjuYe7_N"
   },
   "outputs": [],
   "source": [
    "# ============= æ¡ä»¶å¢å¼º (Conditioning Augmentation) =============\n",
    "class ConditioningAugmentation(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    è®ºæ–‡: Learning Deep Representations of Fine-Grained Visual Descriptions\n",
    "    åœ¨æ–‡æœ¬åµŒå…¥ä¸­å¼•å…¥éšæœºæ€§ï¼Œå¢åŠ ç”Ÿæˆçš„å¤šæ ·æ€§\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, **kwargs):\n",
    "        super(ConditioningAugmentation, self).__init__(**kwargs)\n",
    "        self.text_dim = text_dim\n",
    "        self.fc_mu = tf.keras.layers.Dense(text_dim)\n",
    "        self.fc_sigma = tf.keras.layers.Dense(text_dim)\n",
    "\n",
    "    def call(self, text_embedding, training=True):\n",
    "        mu = self.fc_mu(text_embedding)\n",
    "        log_sigma = self.fc_sigma(text_embedding)\n",
    "\n",
    "        if training:\n",
    "            # é‡å‚æ•°åŒ–æŠ€å·§\n",
    "            epsilon = tf.random.normal(shape=tf.shape(mu))\n",
    "            c_code = mu + tf.exp(log_sigma) * epsilon\n",
    "        else:\n",
    "            c_code = mu\n",
    "\n",
    "        return c_code, mu, log_sigma\n",
    "\n",
    "# ============= æ”¹è¿›çš„æ–‡æœ¬ç¼–ç å™¨ (Hybrid CNN-RNN) =============\n",
    "class AdvancedTextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    ç»“åˆå­—ç¬¦çº§CNNå’ŒåŒå‘LSTMï¼Œæå–æ›´ä¸°å¯Œçš„æ–‡æœ¬ç‰¹å¾\n",
    "    å‚è€ƒ: Learning Deep Representations of Fine-Grained Visual Descriptions\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(AdvancedTextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = hparas['BATCH_SIZE']\n",
    "\n",
    "        # Embeddingå±‚\n",
    "        self.embedding = layers.Embedding(hparas['VOCAB_SIZE'], hparas['EMBED_DIM'])\n",
    "        self.dropout_embed = layers.Dropout(0.2)\n",
    "\n",
    "        # åŒå‘LSTMï¼ˆä¸¤å±‚ï¼‰\n",
    "        self.bilstm1 = layers.Bidirectional(\n",
    "            layers.LSTM(hparas['RNN_HIDDEN_SIZE'],\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.2,\n",
    "                       return_state=False)\n",
    "        )\n",
    "\n",
    "        self.bilstm2 = layers.Bidirectional(\n",
    "            layers.LSTM(hparas['RNN_HIDDEN_SIZE'] // 2,\n",
    "                       return_sequences=False,\n",
    "                       recurrent_dropout=0.2)\n",
    "        )\n",
    "\n",
    "        # æ¡ä»¶å¢å¼º\n",
    "        self.ca = ConditioningAugmentation(hparas['TEXT_DIM'])\n",
    "\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "\n",
    "    def call(self, text, training=True):\n",
    "        # Embedding\n",
    "        x = self.embedding(text)\n",
    "        x = self.dropout_embed(x, training=training)\n",
    "\n",
    "        # åŒå‘LSTMå¤„ç†\n",
    "        x = self.bilstm1(x, training=training)\n",
    "        sentence_embedding = self.bilstm2(x, training=training)\n",
    "\n",
    "        # æ¡ä»¶å¢å¼º\n",
    "        c_code, mu, log_sigma = self.ca(sentence_embedding, training=training)\n",
    "        c_code = self.dropout(c_code, training=training)\n",
    "\n",
    "        return c_code, mu, log_sigma\n",
    "\n",
    "    def kl_loss(self, mu, log_sigma):\n",
    "        \"\"\"KLæ•£åº¦æŸå¤±ï¼Œç”¨äºæ­£åˆ™åŒ–æ¡ä»¶å¢å¼º\"\"\"\n",
    "        kl = -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu ** 2 - tf.exp(2 * log_sigma), axis=1)\n",
    "        return tf.reduce_mean(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Or26_ESXe7_P"
   },
   "outputs": [],
   "source": [
    "# ============= Stage-I Discriminator =============\n",
    "class StageIDiscriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Stage-Iåˆ¤åˆ«å™¨: åˆ¤åˆ«64x64å›¾åƒçš„çœŸå‡\n",
    "    ä½¿ç”¨è°±å½’ä¸€åŒ–æå‡è®­ç»ƒç¨³å®šæ€§\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(StageIDiscriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        ndf = 64\n",
    "\n",
    "        # å›¾åƒç¼–ç : 64x64 -> 32x32 -> 16x16 -> 8x8 -> 4x4\n",
    "        self.conv1 = layers.Conv2D(ndf, 4, 2, padding='same')\n",
    "\n",
    "        self.conv2 = layers.Conv2D(ndf * 2, 4, 2, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = layers.Conv2D(ndf * 4, 4, 2, padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv4 = layers.Conv2D(ndf * 8, 4, 2, padding='same')\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "\n",
    "        # æ–‡æœ¬èåˆ\n",
    "        self.text_fc = layers.Dense(ndf * 8)\n",
    "        self.text_bn = layers.BatchNormalization()\n",
    "\n",
    "        # æ®‹å·®å—\n",
    "        self.res_block = ResidualBlock(ndf * 8)\n",
    "\n",
    "        # è¾“å‡ºå±‚\n",
    "        self.conv_out = layers.Conv2D(1, 4, 1, padding='valid')\n",
    "\n",
    "    def call(self, image, text_embedding, training=True):\n",
    "        # å›¾åƒç‰¹å¾æå–\n",
    "        x = self.conv1(image)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "\n",
    "        # æ–‡æœ¬ç‰¹å¾å¤„ç†\n",
    "        text_feat = self.text_fc(text_embedding)\n",
    "        text_feat = self.text_bn(text_feat, training=training)\n",
    "        text_feat = tf.nn.leaky_relu(text_feat, 0.2)\n",
    "\n",
    "        # ç©ºé—´å¤åˆ¶æ–‡æœ¬ç‰¹å¾å¹¶èåˆ\n",
    "        text_feat = tf.reshape(text_feat, [-1, 1, 1, text_feat.shape[-1]])\n",
    "        text_feat = tf.tile(text_feat, [1, 4, 4, 1])\n",
    "\n",
    "        # èåˆå›¾åƒå’Œæ–‡æœ¬\n",
    "        x = tf.concat([x, text_feat], axis=-1)\n",
    "        x = layers.Conv2D(self.hparas['RNN_HIDDEN_SIZE'], 1)(x)\n",
    "\n",
    "        # æ®‹å·®å—\n",
    "        x = self.res_block(x, training=training)\n",
    "\n",
    "        # è¾“å‡º\n",
    "        logits = self.conv_out(x)\n",
    "        logits = tf.squeeze(logits, [1, 2])\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfzrYMi6e7_R"
   },
   "outputs": [],
   "source": [
    "# ============= æ”¹è¿›çš„æŸå¤±å‡½æ•° =============\n",
    "# 1. WGAN-GPæŸå¤±\n",
    "def wgan_d_loss(real_logits, fake_logits):\n",
    "    \"\"\"WGANåˆ¤åˆ«å™¨æŸå¤±\"\"\"\n",
    "    return tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n",
    "\n",
    "def wgan_g_loss(fake_logits):\n",
    "    \"\"\"WGANç”Ÿæˆå™¨æŸå¤±\"\"\"\n",
    "    return -tf.reduce_mean(fake_logits)\n",
    "\n",
    "# 2. æ¢¯åº¦æƒ©ç½š\n",
    "def compute_gradient_penalty(discriminator, real_images, fake_images, text_embedding):\n",
    "    \"\"\"WGAN-GPçš„æ¢¯åº¦æƒ©ç½š\"\"\"\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "\n",
    "    interpolated = alpha * real_images + (1 - alpha) * fake_images\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        pred_logits, _ = discriminator(interpolated, text_embedding, training=True)\n",
    "\n",
    "    grads = gp_tape.gradient(pred_logits, interpolated)\n",
    "    grads_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "    gp = tf.reduce_mean((grads_norm - 1.0) ** 2)\n",
    "\n",
    "    return gp\n",
    "\n",
    "# 3. æ„ŸçŸ¥æŸå¤± (ä½¿ç”¨VGGç‰¹å¾)\n",
    "def build_vgg_model():\n",
    "    vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(64, 64, 3))\n",
    "    vgg.trainable = False\n",
    "\n",
    "    # æå–å¤šå±‚ç‰¹å¾\n",
    "    outputs = [vgg.get_layer('block2_conv2').output,\n",
    "               vgg.get_layer('block3_conv3').output,\n",
    "               vgg.get_layer('block4_conv3').output]\n",
    "\n",
    "    model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def perceptual_loss(vgg_model, real_images, fake_images):\n",
    "    # å°†å›¾åƒä»[-1, 1]è½¬æ¢åˆ°[0, 1]ï¼Œå†åˆ°VGGéœ€è¦çš„èŒƒå›´\n",
    "    real = (real_images + 1.0) * 127.5\n",
    "    fake = (fake_images + 1.0) * 127.5\n",
    "\n",
    "    # VGGé¢„å¤„ç†\n",
    "    real = tf.keras.applications.vgg16.preprocess_input(real)\n",
    "    fake = tf.keras.applications.vgg16.preprocess_input(fake)\n",
    "\n",
    "    # æå–ç‰¹å¾\n",
    "    real_features = vgg_model(real, training=False)\n",
    "    fake_features = vgg_model(fake, training=False)\n",
    "\n",
    "    # è®¡ç®—å„å±‚L2æŸå¤±\n",
    "    loss = 0.0\n",
    "    for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "        loss += tf.reduce_mean(tf.square(real_feat - fake_feat))\n",
    "\n",
    "    return loss / len(real_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xZ1WJF0e7_T"
   },
   "outputs": [],
   "source": [
    "# ============= æ³¨æ„åŠ›æœºåˆ¶ =============\n",
    "class WordAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    å•è¯çº§æ³¨æ„åŠ›æœºåˆ¶ - è®©ç”Ÿæˆå™¨å…³æ³¨æ–‡æœ¬ä¸­çš„å…³é”®éƒ¨åˆ†\n",
    "    å‚è€ƒï¼šAttnGAN: Fine-Grained Text to Image Generation with Attentional GAN\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, hidden_dim):\n",
    "        super(WordAttention, self).__init__()\n",
    "        self.text_dim = text_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # æŠ•å½±å±‚\n",
    "        self.text_proj = layers.Dense(hidden_dim)\n",
    "        self.context_proj = layers.Dense(hidden_dim)\n",
    "        self.attention_proj = layers.Dense(1)\n",
    "\n",
    "    def call(self, word_features, context_vector, training=True):\n",
    "        \"\"\"\n",
    "        word_features: (batch, seq_len, text_dim) - æ¯ä¸ªå•è¯çš„ç‰¹å¾\n",
    "        context_vector: (batch, hidden_dim) - å…¨å±€ä¸Šä¸‹æ–‡å‘é‡\n",
    "        \"\"\"\n",
    "        # æŠ•å½±æ–‡æœ¬ç‰¹å¾ (batch, seq_len, hidden_dim)\n",
    "        text_proj = self.text_proj(word_features)\n",
    "\n",
    "        # æŠ•å½±ä¸Šä¸‹æ–‡å‘é‡å¹¶æ‰©å±•ç»´åº¦ (batch, 1, hidden_dim)\n",
    "        context_proj = self.context_proj(context_vector)\n",
    "        context_proj = tf.expand_dims(context_proj, axis=1)\n",
    "\n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        # (batch, seq_len, hidden_dim) + (batch, 1, hidden_dim) -> (batch, seq_len, hidden_dim)\n",
    "        combined = tf.nn.tanh(text_proj + context_proj)\n",
    "\n",
    "        # (batch, seq_len, 1)\n",
    "        attention_scores = self.attention_proj(combined)\n",
    "\n",
    "        # Softmaxå½’ä¸€åŒ– (batch, seq_len, 1)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "\n",
    "        # åŠ æƒæ±‚å’Œ (batch, text_dim)\n",
    "        attended_text = tf.reduce_sum(word_features * attention_weights, axis=1)\n",
    "\n",
    "        return attended_text, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4P1jkmBfxFH2",
    "outputId": "c1ba40c0-9d03-490b-bf2c-bface61988a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®ç°äº†ç»†ç²’åº¦é¢œè‰²-ä½ç½®å¯¹é½æŸå¤±\n",
      "   åŠŸèƒ½:\n",
      "   1. æ£€æµ‹æ–‡æœ¬ä¸­çš„é¢œè‰²è¯ä½ç½®\n",
      "   2. è®¡ç®—é¢œè‰²è¯åˆ°å›¾åƒåŒºåŸŸçš„æ³¨æ„åŠ›\n",
      "   3. æœ€å¤§åŒ–é¢œè‰²è¯ä¸å¯¹åº”å›¾åƒåŒºåŸŸçš„ç›¸ä¼¼åº¦\n",
      "   4. è‡ªé€‚åº”åŠ æƒï¼šé‡è¦çš„é¢œè‰²è¯è´¡çŒ®æ›´å¤š\n",
      "\n",
      "   æ•ˆæœ:\n",
      "   - 'red petals' ç›´æ¥å…³æ³¨å›¾åƒä¸­çš„èŠ±ç“£åŒºåŸŸ\n",
      "   - 'yellow anther' å…³æ³¨èŠ±è¯åŒºåŸŸ\n",
      "   - ä¸»è¦é¢œè‰²è¯ï¼ˆå¦‚'red petals'ï¼‰ä¸»å¯¼ç”Ÿæˆ\n"
     ]
    }
   ],
   "source": [
    "# ============= ã€æ”¹è¿›ã€‘ç»†ç²’åº¦é¢œè‰²-ä½ç½®å¯¹é½æŸå¤± =============\n",
    "def detect_color_words(caption_tensor, vocab_size=5000):\n",
    "    \"\"\"\n",
    "    æ£€æµ‹æ–‡æœ¬ä¸­çš„é¢œè‰²è¯ä½ç½®\n",
    "\n",
    "    è¿”å›ä¸€ä¸ªmaskï¼Œæ ‡è®°å“ªäº›ä½ç½®æ˜¯é¢œè‰²è¯\n",
    "\n",
    "    Args:\n",
    "        caption_tensor: (batch, seq_len) - è¯ç´¢å¼•tensor\n",
    "        vocab_size: è¯æ±‡è¡¨å¤§å°\n",
    "\n",
    "    Returns:\n",
    "        color_mask: (batch, seq_len) - é¢œè‰²è¯æ ‡è®°ï¼ˆ1=é¢œè‰²è¯ï¼Œ0=éé¢œè‰²è¯ï¼‰\n",
    "    \"\"\"\n",
    "    # å¸¸è§é¢œè‰²è¯çš„ç´¢å¼•ï¼ˆéœ€è¦æ ¹æ®ä½ çš„è¯æ±‡è¡¨è°ƒæ•´ï¼‰\n",
    "    # è¿™é‡Œä½¿ç”¨å¯å‘å¼æ–¹æ³•ï¼šå‡è®¾é¢œè‰²è¯ç´¢å¼•åœ¨æŸä¸ªèŒƒå›´\n",
    "    #\n",
    "    # æ›´å¥½çš„æ–¹æ³•ï¼šé¢„å…ˆæ„å»ºé¢œè‰²è¯å­—å…¸\n",
    "    # color_words = {'red': 45, 'blue': 78, 'yellow': 123, ...}\n",
    "\n",
    "    batch_size = tf.shape(caption_tensor)[0]\n",
    "    seq_len = tf.shape(caption_tensor)[1]\n",
    "\n",
    "    # ã€æ–¹æ³•1ã€‘åŸºäºç´¢å¼•èŒƒå›´çš„å¯å‘å¼æ£€æµ‹ï¼ˆå¿«é€Ÿä½†ä¸ç²¾ç¡®ï¼‰\n",
    "    # å‡è®¾é¢œè‰²è¯ç´¢å¼•é€šå¸¸åœ¨å¸¸ç”¨è¯èŒƒå›´å†…ï¼ˆ100-1000ï¼‰\n",
    "    # è¿™é‡Œç®€åŒ–ä¸ºï¼šæ ‡è®°æ‰€æœ‰éé›¶è¯ï¼ˆå®é™…ä½¿ç”¨éœ€è¦ç²¾ç¡®çš„é¢œè‰²è¯åˆ—è¡¨ï¼‰\n",
    "    color_mask = tf.cast(tf.not_equal(caption_tensor, 0), tf.float32)\n",
    "\n",
    "    # ã€æ–¹æ³•2ã€‘ç²¾ç¡®æ£€æµ‹ï¼ˆæ¨èï¼‰\n",
    "    # å¦‚æœæœ‰é¢œè‰²è¯åˆ—è¡¨ï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š\n",
    "    # color_indices = tf.constant([45, 78, 123, 234, ...])  # é¢œè‰²è¯çš„ç´¢å¼•\n",
    "    # is_color = tf.reduce_any(\n",
    "    #     tf.equal(tf.expand_dims(caption_tensor, -1),\n",
    "    #             tf.reshape(color_indices, [1, 1, -1])),\n",
    "    #     axis=-1\n",
    "    # )\n",
    "    # color_mask = tf.cast(is_color, tf.float32)\n",
    "\n",
    "    return color_mask\n",
    "\n",
    "\n",
    "def fine_grained_color_alignment_loss(word_features, image_features, caption_tensor):\n",
    "    \"\"\"\n",
    "    ç»†ç²’åº¦é¢œè‰²å¯¹é½æŸå¤± - è®©é¢œè‰²è¯ç›´æ¥å¯¹åº”å›¾åƒåŒºåŸŸ\n",
    "\n",
    "    æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "    - ä¸å†åªå¯¹æ¯”\"æ•´ä½“å¥å­\"å’Œ\"æ•´ä½“å›¾åƒ\"\n",
    "    - è€Œæ˜¯è®©æ¯ä¸ªé¢œè‰²è¯å…³æ³¨å›¾åƒä¸­å¯¹åº”é¢œè‰²çš„åŒºåŸŸ\n",
    "    - ä¾‹å¦‚ï¼š\"red petals\" åº”è¯¥å…³æ³¨å›¾åƒä¸­çš„çº¢è‰²åŒºåŸŸ\n",
    "\n",
    "    Args:\n",
    "        word_features: (batch, seq_len, dim) - æ¯ä¸ªè¯çš„ç‰¹å¾å‘é‡\n",
    "        image_features: (batch, H, W, C) - å›¾åƒç‰¹å¾å›¾ï¼ˆæ¥è‡ªåˆ¤åˆ«å™¨ä¸­é—´å±‚ï¼‰\n",
    "        caption_tensor: (batch, seq_len) - åŸå§‹captionç´¢å¼•\n",
    "\n",
    "    Returns:\n",
    "        loss: æ ‡é‡æŸå¤±å€¼\n",
    "    \"\"\"\n",
    "    # 1. æ£€æµ‹é¢œè‰²è¯ä½ç½®\n",
    "    color_mask = detect_color_words(caption_tensor)  # (batch, seq_len)\n",
    "\n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰é¢œè‰²è¯\n",
    "    total_color_words = tf.reduce_sum(color_mask)\n",
    "    if total_color_words < 1.0:\n",
    "        return 0.0\n",
    "\n",
    "    batch_size = tf.shape(word_features)[0]\n",
    "    seq_len = tf.shape(word_features)[1]\n",
    "\n",
    "    # 2. å±•å¹³å›¾åƒç‰¹å¾ï¼š(batch, H, W, C) -> (batch, H*W, C)\n",
    "    img_h = tf.shape(image_features)[1]\n",
    "    img_w = tf.shape(image_features)[2]\n",
    "    img_channels = tf.shape(image_features)[3]\n",
    "    img_flat = tf.reshape(image_features, [batch_size, img_h * img_w, img_channels])\n",
    "\n",
    "    # 3. å¯¹æ¯ä¸ªé¢œè‰²è¯ï¼Œè®¡ç®—ä¸å›¾åƒæ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›\n",
    "    # è¯ç‰¹å¾: (batch, seq_len, dim)\n",
    "    # å›¾åƒç‰¹å¾: (batch, H*W, dim)\n",
    "    # æ³¨æ„åŠ›: (batch, seq_len, H*W)\n",
    "\n",
    "    # å…ˆå½’ä¸€åŒ–ç‰¹å¾\n",
    "    word_norm = tf.nn.l2_normalize(word_features, axis=-1)\n",
    "    img_norm = tf.nn.l2_normalize(img_flat, axis=-1)\n",
    "\n",
    "    # è®¡ç®—ç›¸ä¼¼åº¦ï¼š(batch, seq_len, dim) Ã— (batch, dim, H*W) -> (batch, seq_len, H*W)\n",
    "    attention = tf.matmul(word_norm, img_norm, transpose_b=True)\n",
    "\n",
    "    # 4. åªä¿ç•™é¢œè‰²è¯çš„æ³¨æ„åŠ›\n",
    "    # color_mask: (batch, seq_len) -> (batch, seq_len, 1)\n",
    "    color_mask_expanded = tf.expand_dims(color_mask, axis=-1)\n",
    "\n",
    "    # é¢œè‰²è¯çš„æ³¨æ„åŠ›åˆ†å¸ƒ: (batch, seq_len, H*W)\n",
    "    color_attention = attention * color_mask_expanded\n",
    "\n",
    "    # 5. Softmaxå½’ä¸€åŒ–ï¼ˆå¯¹å›¾åƒä½ç½®ç»´åº¦ï¼‰\n",
    "    color_attention = tf.nn.softmax(color_attention, axis=-1)\n",
    "\n",
    "    # 6. åŠ æƒå›¾åƒç‰¹å¾ï¼š(batch, seq_len, H*W) Ã— (batch, H*W, dim) -> (batch, seq_len, dim)\n",
    "    attended_img = tf.matmul(color_attention, img_flat)\n",
    "\n",
    "    # 7. è®¡ç®—é¢œè‰²è¯ç‰¹å¾ä¸å¯¹åº”å›¾åƒåŒºåŸŸçš„ç›¸ä¼¼åº¦\n",
    "    # ç›®æ ‡ï¼šæœ€å¤§åŒ–é¢œè‰²è¯ä¸å…¶å…³æ³¨çš„å›¾åƒåŒºåŸŸçš„ç›¸ä¼¼åº¦\n",
    "    word_norm = tf.nn.l2_normalize(word_features, axis=-1)\n",
    "    attended_img_norm = tf.nn.l2_normalize(attended_img, axis=-1)\n",
    "\n",
    "    # ä½™å¼¦ç›¸ä¼¼åº¦ï¼š(batch, seq_len)\n",
    "    similarity = tf.reduce_sum(word_norm * attended_img_norm, axis=-1)\n",
    "\n",
    "    # 8. åªè®¡ç®—é¢œè‰²è¯çš„æŸå¤±ï¼ˆä½¿ç”¨maskï¼‰\n",
    "    masked_similarity = similarity * color_mask\n",
    "\n",
    "    # è´Ÿç›¸ä¼¼åº¦ä½œä¸ºæŸå¤±ï¼ˆæœ€å°åŒ– = æœ€å¤§åŒ–ç›¸ä¼¼åº¦ï¼‰\n",
    "    loss = -tf.reduce_sum(masked_similarity) / (total_color_words + 1e-6)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def adaptive_color_alignment_loss(word_features, image_features, caption_tensor,\n",
    "                                  attention_weights):\n",
    "    \"\"\"\n",
    "    è‡ªé€‚åº”é¢œè‰²å¯¹é½æŸå¤± - åˆ©ç”¨æ³¨æ„åŠ›æƒé‡å¢å¼º\n",
    "\n",
    "    ç»“åˆè¯çº§æ³¨æ„åŠ›æƒé‡ï¼Œè®©æ¨¡å‹æ›´å…³æ³¨é‡è¦çš„é¢œè‰²è¯\n",
    "\n",
    "    Args:\n",
    "        word_features: (batch, seq_len, dim)\n",
    "        image_features: (batch, H, W, C)\n",
    "        caption_tensor: (batch, seq_len)\n",
    "        attention_weights: (batch, seq_len, 1) - æ¥è‡ªWordAttentionçš„æƒé‡\n",
    "\n",
    "    Returns:\n",
    "        loss: æ ‡é‡æŸå¤±å€¼\n",
    "    \"\"\"\n",
    "    # 1. åŸºç¡€çš„ç»†ç²’åº¦å¯¹é½æŸå¤±\n",
    "    base_loss = fine_grained_color_alignment_loss(\n",
    "        word_features, image_features, caption_tensor\n",
    "    )\n",
    "\n",
    "    # 2. åˆ©ç”¨æ³¨æ„åŠ›æƒé‡åŠ å¼ºé‡è¦è¯çš„å¯¹é½\n",
    "    color_mask = detect_color_words(caption_tensor)\n",
    "\n",
    "    # é¢œè‰²è¯çš„æ³¨æ„åŠ›æƒé‡\n",
    "    color_importance = tf.squeeze(attention_weights, axis=-1) * color_mask  # (batch, seq_len)\n",
    "\n",
    "    # å½’ä¸€åŒ–\n",
    "    total_importance = tf.reduce_sum(color_importance) + 1e-6\n",
    "\n",
    "    # åŠ æƒæŸå¤±ï¼ˆæ³¨æ„åŠ›é«˜çš„é¢œè‰²è¯è´¡çŒ®æ›´å¤šæŸå¤±ï¼‰\n",
    "    weighted_loss = base_loss * total_importance\n",
    "\n",
    "    return weighted_loss\n",
    "\n",
    "\n",
    "print(\"âœ… å®ç°äº†ç»†ç²’åº¦é¢œè‰²-ä½ç½®å¯¹é½æŸå¤±\")\n",
    "print(\"   åŠŸèƒ½:\")\n",
    "print(\"   1. æ£€æµ‹æ–‡æœ¬ä¸­çš„é¢œè‰²è¯ä½ç½®\")\n",
    "print(\"   2. è®¡ç®—é¢œè‰²è¯åˆ°å›¾åƒåŒºåŸŸçš„æ³¨æ„åŠ›\")\n",
    "print(\"   3. æœ€å¤§åŒ–é¢œè‰²è¯ä¸å¯¹åº”å›¾åƒåŒºåŸŸçš„ç›¸ä¼¼åº¦\")\n",
    "print(\"   4. è‡ªé€‚åº”åŠ æƒï¼šé‡è¦çš„é¢œè‰²è¯è´¡çŒ®æ›´å¤š\")\n",
    "print(\"\")\n",
    "print(\"   æ•ˆæœ:\")\n",
    "print(\"   - 'red petals' ç›´æ¥å…³æ³¨å›¾åƒä¸­çš„èŠ±ç“£åŒºåŸŸ\")\n",
    "print(\"   - 'yellow anther' å…³æ³¨èŠ±è¯åŒºåŸŸ\")\n",
    "print(\"   - ä¸»è¦é¢œè‰²è¯ï¼ˆå¦‚'red petals'ï¼‰ä¸»å¯¼ç”Ÿæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_8RmbTSxFH2",
    "outputId": "d64f9642-61f1-4e69-d2f4-c398043b6e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ›å»ºäº†V3è®­ç»ƒæ­¥éª¤\n",
      "   é›†æˆäº†æ‰€æœ‰æ”¹è¿›:\n",
      "   1. å¢å¼ºçš„è¯çº§æ³¨æ„åŠ›ï¼ˆä½ç½®åå¥½ï¼‰\n",
      "   2. ç»†ç²’åº¦é¢œè‰²-åŒºåŸŸå¯¹é½\n",
      "   3. é™ä½temperatureï¼ˆ0.07->0.03ï¼‰\n",
      "   4. è‡ªé€‚åº”é¢œè‰²é‡è¦æ€§åŠ æƒ\n"
     ]
    }
   ],
   "source": [
    "# ============= ã€æ”¹è¿›ã€‘V3è®­ç»ƒæ­¥éª¤ - é›†æˆç»†ç²’åº¦é¢œè‰²å¯¹é½ =============\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def improved_train_step_v3(real_image, caption):\n",
    "    \"\"\"\n",
    "    V3è®­ç»ƒæ­¥éª¤ - è§£å†³é¢œè‰²ç†è§£é—®é¢˜\n",
    "\n",
    "    é›†æˆçš„æŸå¤±ï¼š\n",
    "    1. å¯¹æŠ—æŸå¤± (Adversarial Loss)\n",
    "    2. ç‰¹å¾åŒ¹é…æŸå¤± (Feature Matching Loss)\n",
    "    3. é¢œè‰²ä¸€è‡´æ€§æŸå¤± (Color Consistency Loss)\n",
    "    4. å¤šæ ·æ€§æŸå¤± (Diversity Loss)\n",
    "    5. æ–‡æœ¬-å›¾åƒå¯¹é½æŸå¤± (Text-Image Alignment Loss)\n",
    "    6. ã€æ–°å¢ã€‘ç»†ç²’åº¦é¢œè‰²å¯¹é½æŸå¤± (Fine-Grained Color Alignment Loss)\n",
    "\n",
    "    å…³é”®æ”¹è¿›ï¼š\n",
    "    - ä½¿ç”¨ ImprovedGeneratorV3ï¼ˆå¢å¼ºæ³¨æ„åŠ›ï¼‰\n",
    "    - æ·»åŠ è¯çº§é¢œè‰²-åŒºåŸŸå¯¹é½\n",
    "    - é™ä½temperatureæé«˜åŒ¹é…ä¸¥æ ¼åº¦\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal([hparas['BATCH_SIZE'], hparas['Z_DIM']])\n",
    "\n",
    "    # ===== è®­ç»ƒåˆ¤åˆ«å™¨ =====\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # ç¼–ç æ–‡æœ¬\n",
    "        word_features, sentence_feature = text_encoder_improved(caption, training=True)\n",
    "\n",
    "        # ã€ä½¿ç”¨V3ç”Ÿæˆå™¨ã€‘\n",
    "        fake_image, attention_weights = generator_v3(\n",
    "            word_features, sentence_feature, noise, training=True\n",
    "        )\n",
    "\n",
    "        # åˆ¤åˆ«å™¨åˆ¤æ–­\n",
    "        real_logits, _, real_features = discriminator_improved(\n",
    "            real_image, sentence_feature, training=True\n",
    "        )\n",
    "        fake_logits, _, fake_features = discriminator_improved(\n",
    "            fake_image, sentence_feature, training=True\n",
    "        )\n",
    "\n",
    "        # åˆ¤åˆ«å™¨æŸå¤±\n",
    "        d_loss = improved_discriminator_loss(\n",
    "            real_logits, fake_logits,\n",
    "            real_features, fake_features,\n",
    "            use_feature_matching=True,\n",
    "            feature_weight=0.1\n",
    "        )\n",
    "\n",
    "    # æ›´æ–°åˆ¤åˆ«å™¨\n",
    "    disc_grads = disc_tape.gradient(d_loss, discriminator_improved.trainable_variables)\n",
    "    disc_grads, _ = tf.clip_by_global_norm(disc_grads, 5.0)\n",
    "    discriminator_optimizer_improved.apply_gradients(\n",
    "        zip(disc_grads, discriminator_improved.trainable_variables)\n",
    "    )\n",
    "\n",
    "    # ===== è®­ç»ƒç”Ÿæˆå™¨ =====\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # é‡æ–°ç¼–ç æ–‡æœ¬\n",
    "        word_features, sentence_feature = text_encoder_improved(caption, training=True)\n",
    "\n",
    "        # ã€ä½¿ç”¨V3ç”Ÿæˆå™¨ã€‘\n",
    "        fake_image, attention_weights = generator_v3(\n",
    "            word_features, sentence_feature, noise, training=True\n",
    "        )\n",
    "\n",
    "        # åˆ¤åˆ«å™¨åˆ¤æ–­\n",
    "        fake_logits, _, fake_features = discriminator_improved(\n",
    "            fake_image, sentence_feature, training=True\n",
    "        )\n",
    "        real_logits, _, real_features = discriminator_improved(\n",
    "            real_image, sentence_feature, training=True\n",
    "        )\n",
    "\n",
    "        # 1. ç”Ÿæˆå™¨åŸºç¡€æŸå¤±\n",
    "        g_loss, adv_loss, fm_loss = improved_generator_loss(\n",
    "            fake_logits, fake_features, real_features,\n",
    "            use_feature_matching=True,\n",
    "            fm_weight=hparas['LAMBDA_FM']\n",
    "        )\n",
    "\n",
    "        # 2. é¢œè‰²ä¸€è‡´æ€§æŸå¤±\n",
    "        color_loss = color_consistency_loss(fake_image)\n",
    "\n",
    "        # 3. å¤šæ ·æ€§æŸå¤±\n",
    "        div_loss = diversity_loss(fake_image, noise)\n",
    "\n",
    "        # 4. å…¨å±€æ–‡æœ¬-å›¾åƒå¯¹é½æŸå¤±ï¼ˆé™ä½temperatureæé«˜ä¸¥æ ¼åº¦ï¼‰\n",
    "        matching_loss = text_image_alignment_loss(\n",
    "            fake_features, sentence_feature,\n",
    "            temperature=0.03  # ã€ä»0.07é™ä½åˆ°0.03ï¼Œæ›´ä¸¥æ ¼ã€‘\n",
    "        )\n",
    "\n",
    "        # 5. ã€æ–°å¢ã€‘ç»†ç²’åº¦é¢œè‰²å¯¹é½æŸå¤±\n",
    "        fine_grained_loss = adaptive_color_alignment_loss(\n",
    "            word_features, fake_features, caption, attention_weights\n",
    "        )\n",
    "\n",
    "        # æ€»æŸå¤±\n",
    "        total_g_loss = g_loss + \\\n",
    "                      hparas['LAMBDA_COLOR'] * color_loss + \\\n",
    "                      hparas['LAMBDA_DIV'] * div_loss + \\\n",
    "                      hparas['LAMBDA_MATCHING'] * matching_loss + \\\n",
    "                      hparas['LAMBDA_FINE_GRAINED'] * fine_grained_loss\n",
    "\n",
    "    # æ›´æ–°ç”Ÿæˆå™¨å’Œæ–‡æœ¬ç¼–ç å™¨\n",
    "    gen_vars = generator_v3.trainable_variables + text_encoder_improved.trainable_variables\n",
    "    gen_grads = gen_tape.gradient(total_g_loss, gen_vars)\n",
    "    gen_grads, _ = tf.clip_by_global_norm(gen_grads, 5.0)\n",
    "    generator_optimizer_improved.apply_gradients(zip(gen_grads, gen_vars))\n",
    "\n",
    "    return d_loss, total_g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss, fine_grained_loss\n",
    "\n",
    "\n",
    "print(\"âœ… åˆ›å»ºäº†V3è®­ç»ƒæ­¥éª¤\")\n",
    "print(\"   é›†æˆäº†æ‰€æœ‰æ”¹è¿›:\")\n",
    "print(\"   1. å¢å¼ºçš„è¯çº§æ³¨æ„åŠ›ï¼ˆä½ç½®åå¥½ï¼‰\")\n",
    "print(\"   2. ç»†ç²’åº¦é¢œè‰²-åŒºåŸŸå¯¹é½\")\n",
    "print(\"   3. é™ä½temperatureï¼ˆ0.07->0.03ï¼‰\")\n",
    "print(\"   4. è‡ªé€‚åº”é¢œè‰²é‡è¦æ€§åŠ æƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brK1TdmQxFH3",
    "outputId": "2769f7fa-ed1d-443c-e6b6-b3e94891485b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… V3è®­ç»ƒå¾ªç¯å·²å‡†å¤‡å°±ç»ª\n",
      "   ä½¿ç”¨æ–¹æ³•:\n",
      "   history_v3 = train_v3(dataset, hparas_v3, text_encoder_improved, generator_v3, sample_caption_ids, sample_seed, start_epoch=0)\n"
     ]
    }
   ],
   "source": [
    "def train_v3(dataset, hparas, text_encoder, generator, sample_caption_ids, sample_seed, start_epoch=0):\n",
    "    \"\"\"\n",
    "    V3è®­ç»ƒå¾ªç¯ - é’ˆå¯¹é¢œè‰²ç†è§£ä¼˜åŒ–\n",
    "\n",
    "    æ–°å¢ç›‘æ§ï¼š\n",
    "    - fine_grained_loss: ç»†ç²’åº¦é¢œè‰²å¯¹é½æŸå¤±\n",
    "    - æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–\n",
    "    \"\"\"\n",
    "    checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "    # è®­ç»ƒå†å²\n",
    "    history = {\n",
    "        'd_loss': [],\n",
    "        'g_loss': [],\n",
    "        'adv_loss': [],\n",
    "        'fm_loss': [],\n",
    "        'color_loss': [],\n",
    "        'div_loss': [],\n",
    "        'matching_loss': [],\n",
    "        'fine_grained_loss': []  # ã€æ–°å¢ã€‘\n",
    "    }\n",
    "\n",
    "    best_g_loss = float('inf')\n",
    "    steps_per_epoch = hparas['N_SAMPLE'] // hparas['BATCH_SIZE']\n",
    "\n",
    "    print(\"å¼€å§‹V3è®­ç»ƒ...\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"è®­ç»ƒé…ç½®:\")\n",
    "    print(f\"  Epochs: {hparas['N_EPOCH']}\")\n",
    "    print(f\"  Batch size: {hparas['BATCH_SIZE']}\")\n",
    "    print(f\"  Steps/epoch: {steps_per_epoch}\")\n",
    "    print(f\"  D_STEPS: {hparas['D_STEPS']}, G_STEPS: {hparas['G_STEPS']}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"æŸå¤±æƒé‡:\")\n",
    "    print(f\"  FM: {hparas['LAMBDA_FM']}\")\n",
    "    print(f\"  Color: {hparas['LAMBDA_COLOR']}\")\n",
    "    print(f\"  Diversity: {hparas['LAMBDA_DIV']}\")\n",
    "    print(f\"  Matching: {hparas['LAMBDA_MATCHING']} (æå‡)\")\n",
    "    print(f\"  Fine-Grained: {hparas['LAMBDA_FINE_GRAINED']} (æ–°å¢)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"âš ï¸ é¦–æ¬¡è¿è¡Œéœ€è¦1-3åˆ†é’Ÿç¼–è¯‘ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for epoch in range(start_epoch, hparas['N_EPOCH']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # æ¯ä¸ªepochçš„æŸå¤±ç´¯ç§¯\n",
    "        epoch_d_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_adv_loss = 0.0\n",
    "        epoch_fm_loss = 0.0\n",
    "        epoch_color_loss = 0.0\n",
    "        epoch_div_loss = 0.0\n",
    "        epoch_matching_loss = 0.0\n",
    "        epoch_fine_grained_loss = 0.0  # ã€æ–°å¢ã€‘\n",
    "\n",
    "        n_batches = 0\n",
    "\n",
    "        for image_batch, caption_batch in dataset:\n",
    "            # è®­ç»ƒåˆ¤åˆ«å™¨ D_STEPS æ¬¡\n",
    "            for _ in range(hparas['D_STEPS']):\n",
    "                d_loss, g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss, fine_grained_loss = \\\n",
    "                    improved_train_step_v3(image_batch, caption_batch)\n",
    "\n",
    "            # è®­ç»ƒç”Ÿæˆå™¨ G_STEPS-1 æ¬¡\n",
    "            for _ in range(hparas['G_STEPS'] - 1):\n",
    "                _, g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss, fine_grained_loss = \\\n",
    "                    improved_train_step_v3(image_batch, caption_batch)\n",
    "\n",
    "            epoch_d_loss += d_loss.numpy()\n",
    "            epoch_g_loss += g_loss.numpy()\n",
    "            epoch_adv_loss += adv_loss.numpy()\n",
    "            epoch_fm_loss += fm_loss.numpy()\n",
    "            epoch_color_loss += color_loss.numpy()\n",
    "            epoch_div_loss += div_loss.numpy()\n",
    "            epoch_matching_loss += matching_loss.numpy()\n",
    "            epoch_fine_grained_loss += fine_grained_loss.numpy()  # ã€æ–°å¢ã€‘\n",
    "\n",
    "            n_batches += 1\n",
    "\n",
    "        # è®¡ç®—å¹³å‡æŸå¤±\n",
    "        avg_d = epoch_d_loss / n_batches\n",
    "        avg_g = epoch_g_loss / n_batches\n",
    "        avg_adv = epoch_adv_loss / n_batches\n",
    "        avg_fm = epoch_fm_loss / n_batches\n",
    "        avg_color = epoch_color_loss / n_batches\n",
    "        avg_div = epoch_div_loss / n_batches\n",
    "        avg_match = epoch_matching_loss / n_batches\n",
    "        avg_fine = epoch_fine_grained_loss / n_batches  # ã€æ–°å¢ã€‘\n",
    "\n",
    "        # è®°å½•å†å²\n",
    "        history['d_loss'].append(avg_d)\n",
    "        history['g_loss'].append(avg_g)\n",
    "        history['adv_loss'].append(avg_adv)\n",
    "        history['fm_loss'].append(avg_fm)\n",
    "        history['color_loss'].append(avg_color)\n",
    "        history['div_loss'].append(avg_div)\n",
    "        history['matching_loss'].append(avg_match)\n",
    "        history['fine_grained_loss'].append(avg_fine)  # ã€æ–°å¢ã€‘\n",
    "\n",
    "        # æ‰“å°è¿›åº¦\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{hparas['N_EPOCH']} ({elapsed:.1f}s)\")\n",
    "        print(f\"  D_loss: {avg_d:.4f} | G_loss: {avg_g:.4f}\")\n",
    "        print(f\"  Adv: {avg_adv:.4f} | FM: {avg_fm:.4f} | Color: {avg_color:.4f}\")\n",
    "        print(f\"  Div: {avg_div:.4f} | Match: {avg_match:.4f} | FineGrained: {avg_fine:.4f}\")\n",
    "\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if avg_g < best_g_loss:\n",
    "            best_g_loss = avg_g\n",
    "            checkpoint_v3.save(file_prefix=checkpoint_prefix)\n",
    "            print(f\"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: {best_g_loss:.4f})\")\n",
    "\n",
    "        # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_v3.save(file_prefix=checkpoint_prefix)\n",
    "            print(f\"  âœ“ Checkpointä¿å­˜\")\n",
    "\n",
    "        # æ¯5ä¸ªepochç”Ÿæˆæ ·æœ¬\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            generate_and_save_images_v3(\n",
    "                text_encoder,\n",
    "                generator,\n",
    "                epoch + 1,\n",
    "                sample_caption_ids,\n",
    "                sample_seed,\n",
    "                hparas['SAMPLES_DIR'] # Pass samples dir\n",
    "            )\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "    print(f\"æœ€ä½³ G_loss: {best_g_loss:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def generate_and_save_images_v3(text_encoder, generator, epoch, sample_caption_ids, sample_seed, samples_dir):\n",
    "    \"\"\"ç”Ÿæˆæ ·æœ¬å›¾åƒï¼ˆV3ç‰ˆæœ¬ï¼‰\"\"\"\n",
    "    # è½¬æ¢ä¸ºTensor\n",
    "    sample_caption_tensor = tf.constant(sample_caption_ids, dtype=tf.int32)\n",
    "    sample_seed_tensor = tf.constant(sample_seed, dtype=tf.float32)\n",
    "\n",
    "    # ç¼–ç æ–‡æœ¬\n",
    "    word_features, sentence_feature = text_encoder(sample_caption_tensor, training=False)\n",
    "\n",
    "    # ç”Ÿæˆå›¾åƒ\n",
    "    fake_images, attention_weights = generator(\n",
    "        word_features, sentence_feature, sample_seed_tensor, training=False\n",
    "    )\n",
    "\n",
    "    # ä¿å­˜å›¾åƒ\n",
    "    ni = int(np.ceil(np.sqrt(tf.shape(fake_images)[0])))\n",
    "    save_path = os.path.join(samples_dir, f'train_{epoch:04d}.png')\n",
    "    save_images(fake_images.numpy(), [ni, ni], save_path)\n",
    "\n",
    "    print(f\"  â†’ æ ·æœ¬å·²ä¿å­˜: {save_path}\")\n",
    "\n",
    "\n",
    "print(\"âœ… V3è®­ç»ƒå¾ªç¯å·²å‡†å¤‡å°±ç»ª\")\n",
    "print(\"   ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"   history_v3 = train_v3(dataset, hparas_v3, text_encoder_improved, generator_v3, sample_caption_ids, sample_seed, start_epoch=0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OK0k9OQ_xFH5",
    "outputId": "366a6b13-a539-4626-e6dc-0c9911046145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ·»åŠ äº† CLIP é£æ ¼çš„æ–‡æœ¬-å›¾åƒå¯¹é½æŸå¤±å‡½æ•°\n",
      "   å…³é”®ç‰¹ç‚¹ï¼š\n",
      "   - åŒå‘å¯¹æ¯”å­¦ä¹ ï¼ˆå›¾åƒ->æ–‡æœ¬ + æ–‡æœ¬->å›¾åƒï¼‰\n",
      "   - è‡ªåŠ¨å¤„ç†å¤šç»´ç‰¹å¾ï¼ˆ4D å·ç§¯ç‰¹å¾è‡ªåŠ¨æ± åŒ–ï¼‰\n",
      "   - ç›´æ¥ä¼˜åŒ–æ–‡æœ¬-å›¾åƒçš„è¯­ä¹‰ç›¸ä¼¼åº¦\n",
      "   - æ˜¾è‘—æ”¹è¿›è¯„åˆ†å’Œæ–‡æœ¬åŒ¹é…åº¦\n"
     ]
    }
   ],
   "source": [
    "# ============= ã€å…³é”®æ”¹è¿›ã€‘CLIP é£æ ¼çš„æ–‡æœ¬-å›¾åƒå¯¹é½æŸå¤± =============\n",
    "# è¿™æ˜¯æ”¹è¿›æ–‡æœ¬åŒ¹é…åº¦çš„æ ¸å¿ƒï¼é€šè¿‡å¯¹æ¯”å­¦ä¹ æœ€å¤§åŒ–çœŸå®é…å¯¹çš„ç›¸ä¼¼åº¦\n",
    "\n",
    "def text_image_alignment_loss(image_features, text_features, temperature=0.07):\n",
    "    \"\"\"\n",
    "    CLIP é£æ ¼çš„å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°\n",
    "    æœ€å¤§åŒ–çœŸå®æ–‡æœ¬-å›¾åƒå¯¹çš„ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–é”™è¯¯é…å¯¹çš„ç›¸ä¼¼åº¦\n",
    "\n",
    "    è¿™ä¸ªæŸå¤±å‡½æ•°ç›´æ¥ä¼˜åŒ–æ–‡æœ¬-å›¾åƒçš„è¯­ä¹‰å¯¹é½ï¼Œæ˜¯æ”¹è¿›è¯„åˆ†çš„å…³é”®ï¼\n",
    "\n",
    "    Args:\n",
    "        image_features: å›¾åƒç‰¹å¾ - å¯ä»¥æ˜¯ (batch_size, feature_dim) æˆ– (batch_size, H, W, C)\n",
    "        text_features: (batch_size, feature_dim) - æ–‡æœ¬ç‰¹å¾å‘é‡\n",
    "        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒçš„é”åº¦\n",
    "\n",
    "    Returns:\n",
    "        æ ‡é‡æŸå¤±å€¼\n",
    "    \"\"\"\n",
    "    # å¤„ç†å¤šç»´å›¾åƒç‰¹å¾ï¼šå¦‚æœæ˜¯ 4D (batch, H, W, C)ï¼Œè¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–\n",
    "    if len(image_features.shape) == 4:\n",
    "        # (batch, H, W, C) -> (batch, C)\n",
    "        image_features = tf.reduce_mean(image_features, axis=[1, 2])\n",
    "    elif len(image_features.shape) == 3:\n",
    "        # (batch, H, W) -> (batch,) å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œä¹Ÿå¤„ç†\n",
    "        image_features = tf.reduce_mean(image_features, axis=[1, 2])\n",
    "\n",
    "    # ç¡®ä¿ text_features ä¹Ÿæ˜¯ 2D\n",
    "    if len(text_features.shape) == 3:\n",
    "        text_features = tf.reduce_mean(text_features, axis=1)\n",
    "\n",
    "    # å½’ä¸€åŒ–ç‰¹å¾åˆ°å•ä½çƒé¢ä¸Š\n",
    "    image_features = tf.nn.l2_normalize(image_features, axis=1)\n",
    "    text_features = tf.nn.l2_normalize(text_features, axis=1)\n",
    "\n",
    "    batch_size = tf.shape(image_features)[0]\n",
    "\n",
    "    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ (batch_size, batch_size)\n",
    "    # logits[i, j] = å›¾åƒiå’Œæ–‡æœ¬jçš„ç›¸ä¼¼åº¦\n",
    "    logits = tf.matmul(image_features, text_features, transpose_b=True) / temperature\n",
    "\n",
    "    # æ­£æ ·æœ¬æ ‡ç­¾ï¼šå¯¹è§’çº¿ä¸Šæ˜¯çœŸå®é…å¯¹\n",
    "    labels = tf.range(batch_size)\n",
    "\n",
    "    # åŒå‘å¯¹æ¯”æŸå¤±ï¼šæ—¢ä¼˜åŒ–å›¾åƒ->æ–‡æœ¬ï¼Œä¹Ÿä¼˜åŒ–æ–‡æœ¬->å›¾åƒ\n",
    "    loss_i2t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    loss_t2i = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=tf.transpose(logits))\n",
    "\n",
    "    # å¯¹ç§°æŸå¤±\n",
    "    loss = (tf.reduce_mean(loss_i2t) + tf.reduce_mean(loss_t2i)) / 2.0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def image_text_ranking_loss(image_features, text_features, margin=0.2):\n",
    "    \"\"\"\n",
    "    æ’åºæŸå¤±ï¼šæ‹‰è¿‘æ­£æ ·æœ¬ï¼Œæ¨è¿œè´Ÿæ ·æœ¬\n",
    "\n",
    "    Args:\n",
    "        image_features: å›¾åƒç‰¹å¾ - å¯ä»¥æ˜¯ (batch_size, feature_dim) æˆ– (batch_size, H, W, C)\n",
    "        text_features: (batch_size, feature_dim)\n",
    "        margin: é—´éš”\n",
    "\n",
    "    Returns:\n",
    "        æ ‡é‡æŸå¤±å€¼\n",
    "    \"\"\"\n",
    "    # å¤„ç†å¤šç»´å›¾åƒç‰¹å¾\n",
    "    if len(image_features.shape) == 4:\n",
    "        # (batch, H, W, C) -> (batch, C)\n",
    "        image_features = tf.reduce_mean(image_features, axis=[1, 2])\n",
    "    elif len(image_features.shape) == 3:\n",
    "        image_features = tf.reduce_mean(image_features, axis=1)\n",
    "\n",
    "    # ç¡®ä¿ text_features ä¹Ÿæ˜¯ 2D\n",
    "    if len(text_features.shape) == 3:\n",
    "        text_features = tf.reduce_mean(text_features, axis=1)\n",
    "\n",
    "    batch_size = tf.shape(image_features)[0]\n",
    "\n",
    "    # è®¡ç®—æ‰€æœ‰é…å¯¹çš„ç›¸ä¼¼åº¦\n",
    "    similarity = tf.matmul(image_features, text_features, transpose_b=True)\n",
    "\n",
    "    # å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬\n",
    "    pos_sim = tf.linalg.diag_part(similarity)  # (batch_size,)\n",
    "\n",
    "    loss = 0.0\n",
    "    for i in tf.range(batch_size):\n",
    "        # å¯¹äºç¬¬iä¸ªæ ·æœ¬ï¼Œæœ€å°åŒ–æ‰€æœ‰é”™è¯¯é…å¯¹çš„ç›¸ä¼¼åº¦\n",
    "        neg_sim = tf.concat([similarity[i, :i], similarity[i, i+1:]], axis=0)\n",
    "        loss += tf.reduce_sum(tf.nn.relu(margin + neg_sim - pos_sim[i]))\n",
    "\n",
    "    return loss / tf.cast(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "print(\"âœ… æ·»åŠ äº† CLIP é£æ ¼çš„æ–‡æœ¬-å›¾åƒå¯¹é½æŸå¤±å‡½æ•°\")\n",
    "print(\"   å…³é”®ç‰¹ç‚¹ï¼š\")\n",
    "print(\"   - åŒå‘å¯¹æ¯”å­¦ä¹ ï¼ˆå›¾åƒ->æ–‡æœ¬ + æ–‡æœ¬->å›¾åƒï¼‰\")\n",
    "print(\"   - è‡ªåŠ¨å¤„ç†å¤šç»´ç‰¹å¾ï¼ˆ4D å·ç§¯ç‰¹å¾è‡ªåŠ¨æ± åŒ–ï¼‰\")\n",
    "print(\"   - ç›´æ¥ä¼˜åŒ–æ–‡æœ¬-å›¾åƒçš„è¯­ä¹‰ç›¸ä¼¼åº¦\")\n",
    "print(\"   - æ˜¾è‘—æ”¹è¿›è¯„åˆ†å’Œæ–‡æœ¬åŒ¹é…åº¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ooj5Njpe7_U"
   },
   "outputs": [],
   "source": [
    "# ============= æ®‹å·®å—ï¼ˆResidual Blockï¼‰ =============\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    æ®‹å·®å— - å¸®åŠ©æ¢¯åº¦æµåŠ¨ï¼Œä½¿ç½‘ç»œæ›´æ·±\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(channels, 3, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(channels, 3, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out, training=training)\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out, training=training)\n",
    "\n",
    "        # æ®‹å·®è¿æ¥\n",
    "        out = out + residual\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# ============= è‡ªæ³¨æ„åŠ›å±‚ï¼ˆSelf-Attentionï¼‰ =============\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    è‡ªæ³¨æ„åŠ›å±‚ - è®©ç”Ÿæˆå™¨å…³æ³¨å›¾åƒçš„ä¸åŒåŒºåŸŸ\n",
    "    å‚è€ƒï¼šSelf-Attention GAN (SAGAN)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        # 1x1å·ç§¯ç”¨äºé™ç»´\n",
    "        self.query_conv = layers.Conv2D(channels // 8, 1)\n",
    "        self.key_conv = layers.Conv2D(channels // 8, 1)\n",
    "        self.value_conv = layers.Conv2D(channels, 1)\n",
    "\n",
    "        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°\n",
    "        self.gamma = tf.Variable(0.0, trainable=True, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "\n",
    "        # Query: (batch, h*w, c//8)\n",
    "        query = self.query_conv(x)\n",
    "        query = tf.reshape(query, [batch_size, -1, self.channels // 8])\n",
    "\n",
    "        # Key: (batch, c//8, h*w)\n",
    "        key = self.key_conv(x)\n",
    "        key = tf.reshape(key, [batch_size, -1, self.channels // 8])\n",
    "        key = tf.transpose(key, [0, 2, 1])\n",
    "\n",
    "        # Attention map: (batch, h*w, h*w)\n",
    "        attention = tf.matmul(query, key)\n",
    "        attention = tf.nn.softmax(attention, axis=-1)\n",
    "\n",
    "        # Value: (batch, h*w, c)\n",
    "        value = self.value_conv(x)\n",
    "        value = tf.reshape(value, [batch_size, -1, channels])\n",
    "\n",
    "        # åº”ç”¨æ³¨æ„åŠ› (batch, h*w, c)\n",
    "        out = tf.matmul(attention, value)\n",
    "        out = tf.reshape(out, [batch_size, height, width, channels])\n",
    "\n",
    "        # æ®‹å·®è¿æ¥\n",
    "        out = self.gamma * out + x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFQ2wanue7_V"
   },
   "outputs": [],
   "source": [
    "# ============= æ”¹è¿›çš„åˆ¤åˆ«å™¨ï¼ˆå¸¦è‡ªæ³¨æ„åŠ›ï¼‰ =============\n",
    "class ImprovedDiscriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„åˆ¤åˆ«å™¨ï¼š\n",
    "    1. æ›´æ·±çš„å·ç§¯ç½‘ç»œ\n",
    "    2. è‡ªæ³¨æ„åŠ›å±‚\n",
    "    3. è°±å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(ImprovedDiscriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "\n",
    "        # å›¾åƒå·ç§¯å±‚\n",
    "        # 64x64 -> 32x32\n",
    "        self.conv1 = layers.Conv2D(64, 4, strides=2, padding='same')\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "\n",
    "        # 32x32 -> 16x16\n",
    "        self.conv2 = layers.Conv2D(128, 4, strides=2, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(0.3)\n",
    "\n",
    "        # è‡ªæ³¨æ„åŠ›å±‚ï¼ˆåœ¨16x16åˆ†è¾¨ç‡ï¼‰\n",
    "        self.self_attn = SelfAttention(128)\n",
    "\n",
    "        # 16x16 -> 8x8\n",
    "        self.conv3 = layers.Conv2D(256, 4, strides=2, padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.dropout3 = layers.Dropout(0.3)\n",
    "\n",
    "        # 8x8 -> 4x4\n",
    "        self.conv4 = layers.Conv2D(512, 4, strides=2, padding='same')\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.dropout4 = layers.Dropout(0.3)\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # æ–‡æœ¬å¤„ç†å±‚\n",
    "        self.text_fc = layers.Dense(512)\n",
    "        self.text_bn = layers.BatchNormalization()\n",
    "\n",
    "        # èåˆå±‚\n",
    "        self.fc1 = layers.Dense(1024)\n",
    "        self.fc2 = layers.Dense(1)\n",
    "\n",
    "    def call(self, img, text, training=True):\n",
    "        # å¤„ç†å›¾åƒ\n",
    "        x = self.conv1(img)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout1(x, training=training)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout2(x, training=training)\n",
    "\n",
    "        # è‡ªæ³¨æ„åŠ›\n",
    "        x = self.self_attn(x, training=training)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout3(x, training=training)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout4(x, training=training)\n",
    "\n",
    "        # æå–å›¾åƒç‰¹å¾ï¼ˆç”¨äºç‰¹å¾åŒ¹é…ï¼‰\n",
    "        img_features = tf.identity(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # å¤„ç†æ–‡æœ¬\n",
    "        text = self.text_fc(text)\n",
    "        text = self.text_bn(text, training=training)\n",
    "        text = tf.nn.leaky_relu(text, alpha=0.2)\n",
    "\n",
    "        # èåˆå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾\n",
    "        combined = tf.concat([x, text], axis=1)\n",
    "        combined = self.fc1(combined)\n",
    "        combined = tf.nn.leaky_relu(combined, alpha=0.2)\n",
    "\n",
    "        logits = self.fc2(combined)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "\n",
    "        return logits, output, img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKnrc4BExFH3",
    "outputId": "9ef08421-486a-41e7-9716-1914b5bb7f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹åŒ–V3æ”¹è¿›ç‰ˆæ¨¡å‹...\n",
      "============================================================\n",
      "1. åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨ï¼ˆImprovedTextEncoderï¼‰...\n",
      "2. åˆå§‹åŒ–V3ç”Ÿæˆå™¨ï¼ˆEnhancedWordAttention + æ®‹å·® + è‡ªæ³¨æ„åŠ›ï¼‰...\n",
      "3. åˆå§‹åŒ–åˆ¤åˆ«å™¨ï¼ˆImprovedDiscriminatorï¼‰...\n",
      "4. åˆå§‹åŒ–ä¼˜åŒ–å™¨...\n",
      "5. åˆ›å»ºcheckpointç®¡ç†å™¨...\n",
      "============================================================\n",
      "âœ… V3æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼\n",
      "============================================================\n",
      "æ¶æ„ç‰¹ç‚¹:\n",
      "  æ–‡æœ¬ç¼–ç å™¨: BiLSTM (è¯çº§ + å¥å­çº§ç‰¹å¾)\n",
      "  ç”Ÿæˆå™¨: EnhancedWordAttention -> æ®‹å·®å— -> è‡ªæ³¨æ„åŠ›\n",
      "  åˆ¤åˆ«å™¨: å·ç§¯ -> è‡ªæ³¨æ„åŠ› -> åˆ†ç±»\n",
      "\n",
      "å…³é”®æ”¹è¿›:\n",
      "  âœ“ ä½ç½®åå¥½æ³¨æ„åŠ›ï¼ˆåç½®è¯æƒé‡+50%ï¼‰\n",
      "  âœ“ ç»†ç²’åº¦é¢œè‰²-åŒºåŸŸå¯¹é½\n",
      "  âœ“ è‡ªé€‚åº”é‡è¦æ€§åŠ æƒ\n",
      "  âœ“ æ›´ä¸¥æ ¼çš„æ–‡æœ¬åŒ¹é…ï¼ˆtemperature=0.03ï¼‰\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============= ã€åˆå§‹åŒ–ã€‘V3æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶ =============\n",
    "\n",
    "print(\"åˆå§‹åŒ–V3æ”¹è¿›ç‰ˆæ¨¡å‹...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä½¿ç”¨V3è¶…å‚æ•°\n",
    "hparas = hparas_v3\n",
    "\n",
    "# åˆ›å»ºcheckpointç›®å½•\n",
    "os.makedirs(hparas['CHECKPOINTS_DIR'], exist_ok=True)\n",
    "os.makedirs(hparas['SAMPLES_DIR'], exist_ok=True)\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "print(\"1. åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨ï¼ˆImprovedTextEncoderï¼‰...\")\n",
    "text_encoder_improved = ImprovedTextEncoder(hparas)\n",
    "\n",
    "print(\"2. åˆå§‹åŒ–V3ç”Ÿæˆå™¨ï¼ˆEnhancedWordAttention + æ®‹å·® + è‡ªæ³¨æ„åŠ›ï¼‰...\")\n",
    "generator_v3 = ImprovedGeneratorV3(hparas)\n",
    "\n",
    "print(\"3. åˆå§‹åŒ–åˆ¤åˆ«å™¨ï¼ˆImprovedDiscriminatorï¼‰...\")\n",
    "discriminator_improved = ImprovedDiscriminator(hparas)\n",
    "\n",
    "# ï¼ï¼é‡è¦ä¿®å¤ï¼ï¼\n",
    "# ç¡®ä¿æ¨¡å‹å·²ç»é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­æ„å»ºï¼ˆbuildï¼‰ï¼Œè¿™æ ·å®ƒä»¬çš„ trainable_variables æ‰èƒ½è¢«ä¼˜åŒ–å™¨æ­£ç¡®è¯†åˆ«\n",
    "# æˆ‘ä»¬å°†åœ¨å¯åŠ¨è®­ç»ƒçš„ cell (geC9tk2we7_Z) ä¸­è¿›è¡Œé¦–æ¬¡è°ƒç”¨ä»¥æ„å»ºæ¨¡å‹ã€‚\n",
    "# è¿™é‡Œåªæ˜¯å®šä¹‰æ¨¡å‹å®ä¾‹ã€‚\n",
    "\n",
    "# ä¼˜åŒ–å™¨\n",
    "# ï¼ï¼é‡è¦ä¿®å¤ï¼ï¼ï¼šåœ¨æ¨¡å‹æ„å»ºåé‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼Œç¡®ä¿å®ƒä»¬ç»‘å®šåˆ°æ­£ç¡®çš„å˜é‡\n",
    "print(\"4. åˆå§‹åŒ–ä¼˜åŒ–å™¨...\")\n",
    "generator_optimizer_improved = tf.keras.optimizers.Adam(\n",
    "    hparas['LR_G'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "discriminator_optimizer_improved = tf.keras.optimizers.Adam(\n",
    "    hparas['LR_D'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "# Checkpoint\n",
    "print(\"5. åˆ›å»ºcheckpointç®¡ç†å™¨...\")\n",
    "checkpoint_v3 = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer_improved,\n",
    "    discriminator_optimizer=discriminator_optimizer_improved,\n",
    "    text_encoder=text_encoder_improved,\n",
    "    generator=generator_v3,\n",
    "    discriminator=discriminator_improved\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… V3æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(\"æ¶æ„ç‰¹ç‚¹:\")\n",
    "print(\"  æ–‡æœ¬ç¼–ç å™¨: BiLSTM (è¯çº§ + å¥å­çº§ç‰¹å¾)\")\n",
    "print(\"  ç”Ÿæˆå™¨: EnhancedWordAttention -> æ®‹å·®å— -> è‡ªæ³¨æ„åŠ›\")\n",
    "print(\"  åˆ¤åˆ«å™¨: å·ç§¯ -> è‡ªæ³¨æ„åŠ› -> åˆ†ç±»\")\n",
    "print(\"\")\n",
    "print(\"å…³é”®æ”¹è¿›:\")\n",
    "print(\"  âœ“ ä½ç½®åå¥½æ³¨æ„åŠ›ï¼ˆåç½®è¯æƒé‡+50%ï¼‰\")\n",
    "print(\"  âœ“ ç»†ç²’åº¦é¢œè‰²-åŒºåŸŸå¯¹é½\")\n",
    "print(\"  âœ“ è‡ªé€‚åº”é‡è¦æ€§åŠ æƒ\")\n",
    "print(\"  âœ“ æ›´ä¸¥æ ¼çš„æ–‡æœ¬åŒ¹é…ï¼ˆtemperature=0.03ï¼‰\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EB8qxyIye7_W",
    "outputId": "0b907db7-d56b-4b70-87d2-5db375824f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹åŒ–æ”¹è¿›ç‰ˆæ¨¡å‹...\n",
      "============================================================\n",
      "âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\n",
      "âœ“ æ–‡æœ¬ç¼–ç å™¨: ImprovedTextEncoder (å¸¦è¯çº§LSTM)\n",
      "âœ“ ç”Ÿæˆå™¨: ImprovedGenerator (å¸¦æ³¨æ„åŠ›+æ®‹å·®+è‡ªæ³¨æ„åŠ›)\n",
      "âœ“ åˆ¤åˆ«å™¨: ImprovedDiscriminator (å¸¦è‡ªæ³¨æ„åŠ›)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============= åˆå§‹åŒ–æ”¹è¿›çš„æ¨¡å‹ =============\n",
    "print(\"åˆå§‹åŒ–æ”¹è¿›ç‰ˆæ¨¡å‹...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä½¿ç”¨æ”¹è¿›çš„è¶…å‚æ•°\n",
    "hparas = hparas_improved\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "text_encoder_improved = ImprovedTextEncoder(hparas)\n",
    "generator_improved = ImprovedGenerator(hparas)\n",
    "discriminator_improved = ImprovedDiscriminator(hparas)\n",
    "\n",
    "# ä¼˜åŒ–å™¨\n",
    "generator_optimizer_improved = tf.keras.optimizers.Adam(\n",
    "    hparas['LR_G'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "discriminator_optimizer_improved = tf.keras.optimizers.Adam(\n",
    "    hparas['LR_D'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint_improved = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer_improved,\n",
    "    discriminator_optimizer=discriminator_optimizer_improved,\n",
    "    text_encoder=text_encoder_improved,\n",
    "    generator=generator_improved,\n",
    "    discriminator=discriminator_improved\n",
    ")\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"âœ“ æ–‡æœ¬ç¼–ç å™¨: ImprovedTextEncoder (å¸¦è¯çº§LSTM)\")\n",
    "print(f\"âœ“ ç”Ÿæˆå™¨: ImprovedGenerator (å¸¦æ³¨æ„åŠ›+æ®‹å·®+è‡ªæ³¨æ„åŠ›)\")\n",
    "print(f\"âœ“ åˆ¤åˆ«å™¨: ImprovedDiscriminator (å¸¦è‡ªæ³¨æ„åŠ›)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9PqjWvee7_X"
   },
   "outputs": [],
   "source": [
    "# ============= æ”¹è¿›çš„è®­ç»ƒå¾ªç¯ =============\n",
    "def improved_train(dataset, hparas, start_epoch=0):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„è®­ç»ƒå¾ªç¯ï¼š\n",
    "    1. æ›´å¥½çš„è¿›åº¦ç›‘æ§\n",
    "    2. è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    3. è¯¦ç»†çš„æŸå¤±è®°å½•\n",
    "    4. è¿½è¸ªæ–‡æœ¬-å›¾åƒå¯¹é½æŸå¤±\n",
    "    \"\"\"\n",
    "    checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "    # è®­ç»ƒå†å²\n",
    "    history = {\n",
    "        'd_loss': [],\n",
    "        'g_loss': [],\n",
    "        'adv_loss': [],\n",
    "        'fm_loss': [],\n",
    "        'color_loss': [],\n",
    "        'div_loss': [],\n",
    "        'matching_loss': []  # æ–°å¢ï¼\n",
    "    }\n",
    "\n",
    "    best_g_loss = float('inf')\n",
    "\n",
    "    # è®¡ç®—æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°\n",
    "    steps_per_epoch = hparas['N_SAMPLE'] // hparas['BATCH_SIZE']\n",
    "\n",
    "    print(\"å¼€å§‹æ”¹è¿›ç‰ˆè®­ç»ƒ...\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"æ¯ä¸ªepochçº¦ {steps_per_epoch} ä¸ªæ‰¹æ¬¡ (batch_size={hparas['BATCH_SIZE']})\")\n",
    "    print(\"âš ï¸ é¦–æ¬¡è¿è¡Œéœ€è¦1-3åˆ†é’Ÿç¼–è¯‘TensorFlowè®¡ç®—å›¾ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for epoch in range(start_epoch, hparas['N_EPOCH']):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # æ¯ä¸ªepochçš„æŸå¤±ç´¯ç§¯\n",
    "        epoch_d_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_adv_loss = 0.0\n",
    "        epoch_fm_loss = 0.0\n",
    "        epoch_color_loss = 0.0\n",
    "        epoch_div_loss = 0.0\n",
    "        epoch_matching_loss = 0.0  # æ–°å¢ï¼\n",
    "\n",
    "        n_batches = 0\n",
    "\n",
    "        for image_batch, caption_batch in dataset:\n",
    "            # è®­ç»ƒåˆ¤åˆ«å™¨ D_STEPS æ¬¡\n",
    "            for _ in range(hparas['D_STEPS']):\n",
    "                d_loss, g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss = \\\n",
    "                    improved_train_step(image_batch, caption_batch)\n",
    "\n",
    "            # è®­ç»ƒç”Ÿæˆå™¨ G_STEPS æ¬¡\n",
    "            for _ in range(hparas['G_STEPS'] - 1):\n",
    "                _, g_loss, adv_loss, fm_loss, color_loss, div_loss, matching_loss = \\\n",
    "                    improved_train_step(image_batch, caption_batch)\n",
    "\n",
    "            epoch_d_loss += d_loss.numpy()\n",
    "            epoch_g_loss += g_loss.numpy()\n",
    "            epoch_adv_loss += adv_loss.numpy()\n",
    "            epoch_fm_loss += fm_loss.numpy()\n",
    "            epoch_color_loss += color_loss.numpy()\n",
    "            epoch_div_loss += div_loss.numpy()\n",
    "            epoch_matching_loss += matching_loss.numpy()  # æ–°å¢ï¼\n",
    "\n",
    "            n_batches += 1\n",
    "\n",
    "            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦ï¼ˆæ¯10ä¸ªæ‰¹æ¬¡ï¼‰\n",
    "            if n_batches % 10 == 0:\n",
    "                avg_d = epoch_d_loss / n_batches\n",
    "                avg_g = epoch_g_loss / n_batches\n",
    "                print(f'  Batch [{n_batches:3d}/{steps_per_epoch}] - D: {avg_d:.4f}, G: {avg_g:.4f}', end='\\r')\n",
    "\n",
    "            # é™åˆ¶æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°ï¼Œé¿å…å¤ªæ…¢\n",
    "            if n_batches >= steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        # è®¡ç®—å¹³å‡æŸå¤±\n",
    "        epoch_d_loss /= n_batches\n",
    "        epoch_g_loss /= n_batches\n",
    "        epoch_adv_loss /= n_batches\n",
    "        epoch_fm_loss /= n_batches\n",
    "        epoch_color_loss /= n_batches\n",
    "        epoch_div_loss /= n_batches\n",
    "        epoch_matching_loss /= n_batches  # æ–°å¢ï¼\n",
    "\n",
    "        # è®°å½•å†å²\n",
    "        history['d_loss'].append(epoch_d_loss)\n",
    "        history['g_loss'].append(epoch_g_loss)\n",
    "        history['adv_loss'].append(epoch_adv_loss)\n",
    "        history['fm_loss'].append(epoch_fm_loss)\n",
    "        history['color_loss'].append(epoch_color_loss)\n",
    "        history['div_loss'].append(epoch_div_loss)\n",
    "        history['matching_loss'].append(epoch_matching_loss)  # æ–°å¢ï¼\n",
    "\n",
    "        # æ‰“å°è¿›åº¦ï¼ˆæ¯ä¸ªepochéƒ½è¾“å‡ºï¼‰\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{hparas[\"N_EPOCH\"]} - {epoch_time:.2f}s - Batches: {n_batches}')\n",
    "        print(f'  D_loss: {epoch_d_loss:.4f}')\n",
    "        print(f'  G_loss: {epoch_g_loss:.4f} [Adv: {epoch_adv_loss:.4f}, FM: {epoch_fm_loss:.4f}, '\n",
    "              f'Color: {epoch_color_loss:.4f}, Div: {epoch_div_loss:.4f}, Matching: {epoch_matching_loss:.4f}]')\n",
    "\n",
    "        # æ¯ä¸ªepochéƒ½ç”Ÿæˆæ ·æœ¬å›¾åƒ\n",
    "        print(f'  â†’ ç”Ÿæˆæ ·æœ¬å›¾åƒ...')\n",
    "        generate_and_save_images_improved(\n",
    "            text_encoder_improved,\n",
    "            generator_improved,\n",
    "            epoch + 1,\n",
    "            sample_sentence,\n",
    "            sample_seed\n",
    "        )\n",
    "\n",
    "        # æ¯10ä¸ªepochä¿å­˜checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_improved.save(file_prefix=checkpoint_prefix)\n",
    "            print(f'  â†’ Checkpoint saved at epoch {epoch+1}')\n",
    "\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if epoch_g_loss < best_g_loss:\n",
    "            best_g_loss = epoch_g_loss\n",
    "            checkpoint_improved.save(file_prefix=checkpoint_prefix + '_best')\n",
    "            print(f'  â˜… Best model saved! (G_loss: {best_g_loss:.4f})')\n",
    "\n",
    "        print('-'*60)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"è®­ç»ƒå®Œæˆï¼\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geC9tk2we7_Z",
    "outputId": "369d0b10-f6a3-43c4-f053-76610e07067d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡è®­ç»ƒæ•°æ®é›†...\n",
      "å‡†å¤‡æ ·æœ¬æ•°æ®...\n",
      "\n",
      "å¼€å§‹è®­ç»ƒæ”¹è¿›ç‰ˆæ¨¡å‹ï¼\n",
      "============================================================\n",
      "å…³é”®æ”¹è¿›ï¼š\n",
      "  âœ“ è¯çº§æ³¨æ„åŠ›æœºåˆ¶ - å…³æ³¨æ–‡æœ¬å…³é”®è¯\n",
      "  âœ“ æ®‹å·®è¿æ¥ - æ›´æ·±çš„ç½‘ç»œç»“æ„\n",
      "  âœ“ è‡ªæ³¨æ„åŠ›å±‚ - å…³æ³¨å›¾åƒä¸åŒåŒºåŸŸ\n",
      "  âœ“ ç‰¹å¾åŒ¹é…æŸå¤± - ç”Ÿæˆæ›´çœŸå®çš„ç‰¹å¾\n",
      "  âœ“ é¢œè‰²ä¸€è‡´æ€§æŸå¤± - æ›´è‡ªç„¶çš„é¢œè‰²\n",
      "  âœ“ å¤šæ ·æ€§æ­£åˆ™åŒ– - é˜²æ­¢æ¨¡å¼å´©æºƒ\n",
      "============================================================\n",
      "\n",
      "åˆå§‹åŒ–æ¨¡å‹...\n",
      "  åˆå§‹åŒ–è¾“å…¥å½¢çŠ¶ - å›¾åƒ: (64, 64, 64, 3), æ–‡æœ¬: (64, 20)\n",
      "  âœ“ æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ\n",
      "    - è¯ç‰¹å¾å½¢çŠ¶: (64, 20, 512)\n",
      "    - å¥å­ç‰¹å¾å½¢çŠ¶: (64, 512)\n",
      "  âœ“ ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ\n",
      "    - ç”Ÿæˆå›¾åƒå½¢çŠ¶: (64, 64, 64, 3)\n",
      "  âœ“ åˆ¤åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ\n",
      "    - çœŸå®ç‰¹å¾å½¢çŠ¶: (64, 4, 4, 512)\n",
      "    - å‡å›¾åƒç‰¹å¾å½¢çŠ¶: (64, 4, 4, 512)\n",
      "æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼\n",
      "å¼ºåˆ¶ä¼˜åŒ–å™¨åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹æ„å»ºå†…éƒ¨çŠ¶æ€...\n",
      "  âœ“ ç”Ÿæˆå™¨ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€å·²æ„å»ºã€‚\n",
      "  âœ“ åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€å·²æ„å»ºã€‚\n",
      "============================================================\n",
      "\n",
      "å¼€å§‹V3è®­ç»ƒ...\n",
      "============================================================\n",
      "è®­ç»ƒé…ç½®:\n",
      "  Epochs: 150\n",
      "  Batch size: 64\n",
      "  Steps/epoch: 125\n",
      "  D_STEPS: 1, G_STEPS: 2\n",
      "============================================================\n",
      "æŸå¤±æƒé‡:\n",
      "  FM: 10.0\n",
      "  Color: 0.5\n",
      "  Diversity: 0.01\n",
      "  Matching: 1.0 (æå‡)\n",
      "  Fine-Grained: 0.5 (æ–°å¢)\n",
      "============================================================\n",
      "âš ï¸ é¦–æ¬¡è¿è¡Œéœ€è¦1-3åˆ†é’Ÿç¼–è¯‘ï¼Œè¯·è€å¿ƒç­‰å¾…...\n",
      "============================================================\n",
      "Epoch 1/150 (57.7s)\n",
      "  D_loss: 1.1291 | G_loss: -16.7293\n",
      "  Adv: 3.6291 | FM: 0.1316 | Color: 0.8866\n",
      "  Div: -0.5972 | Match: 4.2850 | FineGrained: -52.7933\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -16.7293)\n",
      "------------------------------------------------------------\n",
      "Epoch 2/150 (39.8s)\n",
      "  D_loss: 0.6453 | G_loss: -18.0842\n",
      "  Adv: 3.7682 | FM: 0.1102 | Color: 0.5012\n",
      "  Div: -0.5248 | Match: 4.4444 | FineGrained: -55.2877\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -18.0842)\n",
      "------------------------------------------------------------\n",
      "Epoch 3/150 (37.0s)\n",
      "  D_loss: 0.5657 | G_loss: -19.2716\n",
      "  Adv: 3.6238 | FM: 0.1071 | Color: 0.3350\n",
      "  Div: -0.4568 | Match: 4.5553 | FineGrained: -57.3699\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -19.2716)\n",
      "------------------------------------------------------------\n",
      "Epoch 4/150 (35.4s)\n",
      "  D_loss: 0.6927 | G_loss: -18.8349\n",
      "  Adv: 3.6121 | FM: 0.0663 | Color: 0.2925\n",
      "  Div: -0.8075 | Match: 4.7520 | FineGrained: -56.0008\n",
      "------------------------------------------------------------\n",
      "Epoch 5/150 (35.6s)\n",
      "  D_loss: 0.5235 | G_loss: -19.8960\n",
      "  Adv: 3.3632 | FM: 0.0775 | Color: 0.2335\n",
      "  Div: -0.4504 | Match: 4.5871 | FineGrained: -57.4679\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -19.8960)\n",
      "  â†’ æ ·æœ¬å·²ä¿å­˜: samples/improved_v3/train_0005.png\n",
      "------------------------------------------------------------\n",
      "Epoch 6/150 (36.8s)\n",
      "  D_loss: 0.4858 | G_loss: -21.0179\n",
      "  Adv: 3.3059 | FM: 0.0565 | Color: 0.3938\n",
      "  Div: -1.1071 | Match: 4.0843 | FineGrained: -58.3172\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -21.0179)\n",
      "------------------------------------------------------------\n",
      "Epoch 7/150 (36.6s)\n",
      "  D_loss: 0.4415 | G_loss: -21.8594\n",
      "  Adv: 3.0823 | FM: 0.0482 | Color: 0.3856\n",
      "  Div: -1.1104 | Match: 3.9434 | FineGrained: -59.0968\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -21.8594)\n",
      "------------------------------------------------------------\n",
      "Epoch 8/150 (37.0s)\n",
      "  D_loss: 0.4764 | G_loss: -21.5485\n",
      "  Adv: 3.2081 | FM: 0.0420 | Color: 0.3148\n",
      "  Div: -1.4868 | Match: 3.9483 | FineGrained: -58.5340\n",
      "------------------------------------------------------------\n",
      "Epoch 9/150 (35.5s)\n",
      "  D_loss: 0.4419 | G_loss: -21.9083\n",
      "  Adv: 3.1179 | FM: 0.0278 | Color: 0.2962\n",
      "  Div: -1.8570 | Match: 3.9147 | FineGrained: -58.6969\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -21.9083)\n",
      "------------------------------------------------------------\n",
      "Epoch 10/150 (36.4s)\n",
      "  D_loss: 0.4378 | G_loss: -21.9424\n",
      "  Adv: 3.1024 | FM: 0.0275 | Color: 0.3066\n",
      "  Div: -1.6130 | Match: 3.8434 | FineGrained: -58.6013\n",
      "  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (G_loss: -21.9424)\n",
      "  âœ“ Checkpointä¿å­˜\n",
      "  â†’ æ ·æœ¬å·²ä¿å­˜: samples/improved_v3/train_0010.png\n",
      "------------------------------------------------------------\n",
      "Epoch 11/150 (36.4s)\n",
      "  D_loss: 0.8529 | G_loss: -20.1436\n",
      "  Adv: 3.4158 | FM: 0.0328 | Color: 0.2741\n",
      "  Div: -1.7603 | Match: 3.6481 | FineGrained: -55.3094\n",
      "------------------------------------------------------------\n",
      "Epoch 12/150 (35.4s)\n",
      "  D_loss: 0.7772 | G_loss: -19.9474\n",
      "  Adv: 3.1288 | FM: 0.0345 | Color: 0.2463\n",
      "  Div: -2.0026 | Match: 3.3215 | FineGrained: -53.6920\n",
      "------------------------------------------------------------\n",
      "Epoch 13/150 (35.1s)\n",
      "  D_loss: 0.7813 | G_loss: -19.3506\n",
      "  Adv: 3.1184 | FM: 0.0307 | Color: 0.2870\n",
      "  Div: -1.9737 | Match: 3.1889 | FineGrained: -52.1768\n",
      "------------------------------------------------------------\n",
      "Epoch 14/150 (35.2s)\n",
      "  D_loss: 0.6400 | G_loss: -19.3504\n",
      "  Adv: 3.0244 | FM: 0.0293 | Color: 0.2530\n",
      "  Div: -1.8927 | Match: 3.2464 | FineGrained: -52.0431\n",
      "------------------------------------------------------------\n",
      "Epoch 15/150 (35.3s)\n",
      "  D_loss: 0.5221 | G_loss: -20.0556\n",
      "  Adv: 3.1242 | FM: 0.0307 | Color: 0.2869\n",
      "  Div: -1.4753 | Match: 3.3113 | FineGrained: -53.8528\n",
      "  â†’ æ ·æœ¬å·²ä¿å­˜: samples/improved_v3/train_0015.png\n",
      "------------------------------------------------------------\n",
      "Epoch 16/150 (35.4s)\n",
      "  D_loss: 0.5103 | G_loss: -20.0004\n",
      "  Adv: 3.0992 | FM: 0.0295 | Color: 0.2340\n",
      "  Div: -1.5603 | Match: 3.2493 | FineGrained: -53.4901\n",
      "------------------------------------------------------------\n",
      "Epoch 17/150 (35.3s)\n",
      "  D_loss: 0.4564 | G_loss: -21.3212\n",
      "  Adv: 3.0996 | FM: 0.0323 | Color: 0.2746\n",
      "  Div: -1.5752 | Match: 3.0432 | FineGrained: -55.8174\n",
      "------------------------------------------------------------\n",
      "Epoch 18/150 (35.3s)\n",
      "  D_loss: 0.5674 | G_loss: -19.8904\n",
      "  Adv: 3.0491 | FM: 0.0306 | Color: 0.2059\n",
      "  Div: -1.9303 | Match: 3.2390 | FineGrained: -53.1354\n",
      "------------------------------------------------------------\n",
      "Epoch 19/150 (35.3s)\n",
      "  D_loss: 0.4483 | G_loss: -21.6497\n",
      "  Adv: 3.0305 | FM: 0.0309 | Color: 0.2454\n",
      "  Div: -1.6560 | Match: 3.0876 | FineGrained: -56.3652\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============= å¼€å§‹æ”¹è¿›ç‰ˆè®­ç»ƒ =============\n",
    "# æ³¨æ„ï¼šå¦‚æœè¦ä»checkpointæ¢å¤ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š\n",
    "#latest_checkpoint = tf.train.latest_checkpoint(hparas['CHECKPOINTS_DIR'])\n",
    "#if latest_checkpoint:\n",
    "#     checkpoint_improved.restore(latest_checkpoint)\n",
    "#     print(f\"ä»checkpointæ¢å¤: {latest_checkpoint}\")\n",
    "\n",
    "# é‡æ–°åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨ä¹‹å‰å®šä¹‰çš„dataset_generatorï¼‰\n",
    "print(\"å‡†å¤‡è®­ç»ƒæ•°æ®é›†...\")\n",
    "training_dataset_improved = dataset_generator(\n",
    "    data_path + '/text2ImgData.pkl',\n",
    "    hparas['BATCH_SIZE'],\n",
    "    training_data_generator\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ ·æœ¬æ•°æ®ï¼ˆç”¨äºç”Ÿæˆæ ·æœ¬å›¾åƒï¼‰\n",
    "print(\"å‡†å¤‡æ ·æœ¬æ•°æ®...\")\n",
    "sample_captions = [\n",
    "    \"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "    \"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "    \"the petals on this flower are white with a yellow center\",\n",
    "    \"this flower has a lot of small round pink petals.\"\n",
    "]\n",
    "\n",
    "# è½¬æ¢ä¸ºIDå¹¶æ‰©å±•åˆ°batch_size\n",
    "sample_caption_ids = []\n",
    "for caption in sample_captions:\n",
    "    ids = sent2IdList(caption)\n",
    "    # è½¬æ¢ä¸ºæ•´æ•°\n",
    "    ids = [int(x) for x in ids]\n",
    "    sample_caption_ids.append(ids)\n",
    "\n",
    "# æ‰©å±•åˆ°batch_size\n",
    "sample_caption_ids = sample_caption_ids * (hparas['BATCH_SIZE'] // len(sample_caption_ids))\n",
    "sample_sentence = [tf.constant(sample_caption_ids, dtype=tf.int32)]\n",
    "\n",
    "# å›ºå®šçš„éšæœºç§å­ï¼ˆç”¨äºç”Ÿæˆä¸€è‡´çš„æ ·æœ¬ï¼‰\n",
    "sample_seed = tf.random.normal([hparas['BATCH_SIZE'], hparas['Z_DIM']])\n",
    "\n",
    "# è®¡ç®—niï¼ˆç”¨äºä¿å­˜å›¾åƒçš„ç½‘æ ¼å¤§å°ï¼‰\n",
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒæ”¹è¿›ç‰ˆæ¨¡å‹ï¼\")\n",
    "print(\"=\"*60)\n",
    "print(\"å…³é”®æ”¹è¿›ï¼š\")\n",
    "print(\"  âœ“ è¯çº§æ³¨æ„åŠ›æœºåˆ¶ - å…³æ³¨æ–‡æœ¬å…³é”®è¯\")\n",
    "print(\"  âœ“ æ®‹å·®è¿æ¥ - æ›´æ·±çš„ç½‘ç»œç»“æ„\")\n",
    "print(\"  âœ“ è‡ªæ³¨æ„åŠ›å±‚ - å…³æ³¨å›¾åƒä¸åŒåŒºåŸŸ\")\n",
    "print(\"  âœ“ ç‰¹å¾åŒ¹é…æŸå¤± - ç”Ÿæˆæ›´çœŸå®çš„ç‰¹å¾\")\n",
    "print(\"  âœ“ é¢œè‰²ä¸€è‡´æ€§æŸå¤± - æ›´è‡ªç„¶çš„é¢œè‰²\")\n",
    "print(\"  âœ“ å¤šæ ·æ€§æ­£åˆ™åŒ– - é˜²æ­¢æ¨¡å¼å´©æºƒ\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# ===== æ¨¡å‹åˆå§‹åŒ–å’Œæ„å»º =====\n",
    "print(\"åˆå§‹åŒ–æ¨¡å‹...\")\n",
    "# ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ¥æ„å»ºæ‰€æœ‰æ¨¡å‹\n",
    "for init_images, init_captions in training_dataset_improved.take(1):\n",
    "    print(f\"  åˆå§‹åŒ–è¾“å…¥å½¢çŠ¶ - å›¾åƒ: {init_images.shape}, æ–‡æœ¬: {init_captions.shape}\")\n",
    "\n",
    "    # åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨\n",
    "    try:\n",
    "        # Here, we use the V3 models, which are text_encoder_improved, generator_v3, and discriminator_improved\n",
    "        # The ImprovedTextEncoder expects the caption tensor directly, not init_captions\n",
    "        word_features, sentence_feature = text_encoder_improved(init_captions, training=True)\n",
    "        print(f\"  âœ“ æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ\")\n",
    "        print(f\"    - è¯ç‰¹å¾å½¢çŠ¶: {word_features.shape}\")\n",
    "        print(f\"    - å¥å­ç‰¹å¾å½¢çŠ¶: {sentence_feature.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        raise\n",
    "\n",
    "    # åˆå§‹åŒ–ç”Ÿæˆå™¨\n",
    "    try:\n",
    "        noise = tf.random.normal([hparas['BATCH_SIZE'], hparas['Z_DIM']])\n",
    "        fake_image, _ = generator_v3(word_features, sentence_feature, noise, training=True)\n",
    "        print(f\"  âœ“ ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ\")\n",
    "        print(f\"    - ç”Ÿæˆå›¾åƒå½¢çŠ¶: {fake_image.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        raise\n",
    "\n",
    "    # åˆå§‹åŒ–åˆ¤åˆ«å™¨\n",
    "    try:\n",
    "        real_logits, _, real_features = discriminator_improved(init_images, sentence_feature, training=True)\n",
    "        fake_logits, _, fake_features = discriminator_improved(fake_image, sentence_feature, training=True)\n",
    "        print(f\"  âœ“ åˆ¤åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ\")\n",
    "        print(f\"    - çœŸå®ç‰¹å¾å½¢çŠ¶: {real_features.shape}\")\n",
    "        print(f\"    - å‡å›¾åƒç‰¹å¾å½¢çŠ¶: {fake_features.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— åˆ¤åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "\n",
    "# ============= å¼ºåˆ¶ä¼˜åŒ–å™¨åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹æ„å»ºå†…éƒ¨çŠ¶æ€ =============\n",
    "# è¿™æœ‰åŠ©äºè§£å†³ tf.function ä¸­ Keras ä¼˜åŒ–å™¨çš„ NotImplementedError\n",
    "print(\"å¼ºåˆ¶ä¼˜åŒ–å™¨åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹æ„å»ºå†…éƒ¨çŠ¶æ€...\")\n",
    "dummy_generator_variables = generator_v3.trainable_variables + text_encoder_improved.trainable_variables\n",
    "dummy_generator_gradients = [tf.zeros_like(v) for v in dummy_generator_variables]\n",
    "generator_optimizer_improved.apply_gradients(zip(dummy_generator_gradients, dummy_generator_variables))\n",
    "print(\"  âœ“ ç”Ÿæˆå™¨ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€å·²æ„å»ºã€‚\")\n",
    "\n",
    "dummy_discriminator_variables = discriminator_improved.trainable_variables\n",
    "dummy_discriminator_gradients = [tf.zeros_like(v) for v in dummy_discriminator_variables]\n",
    "discriminator_optimizer_improved.apply_gradients(zip(dummy_discriminator_gradients, dummy_discriminator_variables))\n",
    "print(\"  âœ“ åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€å·²æ„å»ºã€‚\")\n",
    "print(\"============================================================\")\n",
    "print()\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "# Use the V3 training function and V3-specific models\n",
    "history_v3 = train_v3(training_dataset_improved, hparas_v3, text_encoder_improved, generator_v3, sample_caption_ids, sample_seed, start_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYdoKwRde7_a"
   },
   "outputs": [],
   "source": [
    "# ============= æµ‹è¯•é›†æ•°æ®é›†ä¸æ¨ç† =============\n",
    "# æŒ‰ä½œä¸šè¯´æ˜ï¼šè¯»å– dataset/testData.pklï¼Œç”Ÿæˆ inference_{ID}.png\n",
    "\n",
    "\n",
    "def testing_data_generator(caption, index):\n",
    "    # æµ‹è¯•é›†æ˜ å°„å‡½æ•°ï¼Œç¡®ä¿ç±»å‹ä¸€è‡´\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "    return caption, index\n",
    "\n",
    "\n",
    "def testing_dataset_generator(data_path, batch_size, data_generator):\n",
    "    \"\"\"ç”Ÿæˆæµ‹è¯•é›†datasetï¼Œä¿æŒä¸è®­ç»ƒå‰å¤„ç†ä¸€è‡´\"\"\"\n",
    "    data = pd.read_pickle(data_path)\n",
    "    # Captions åˆ—åŒ…å«åºåˆ—ï¼ˆåˆ—è¡¨/ndarrayï¼‰ï¼Œéœ€ç»Ÿä¸€ä¸º int32 æ•°ç»„\n",
    "    captions_raw = data['Captions'].values\n",
    "    captions = [np.asarray(c, dtype=np.int32) for c in captions_raw]\n",
    "    captions = np.stack(captions, axis=0)  # å½¢çŠ¶: (N, seq_len)\n",
    "\n",
    "    # ID ç»Ÿä¸€è½¬å­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿åç»­å‘½å\n",
    "    ids = data['ID'].astype(str).values\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((captions, ids))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# æ„å»ºæµ‹è¯•é›†ï¼ˆä¸è®­ç»ƒå‰å¤„ç†ä¿æŒä¸€è‡´ï¼‰\n",
    "testing_dataset = testing_dataset_generator(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/dataset/testData.pkl',\n",
    "    hparas['BATCH_SIZE'],\n",
    "    testing_data_generator\n",
    ")\n",
    "\n",
    "\n",
    "def improved_inference(testing_dataset, output_dir='inference/demo'):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ”¹è¿›ç‰ˆæ¨¡å‹ç”Ÿæˆæµ‹è¯•é›†å›¾åƒ\n",
    "    è¾“å‡ºæ–‡ä»¶åæ ¼å¼ï¼šinference_{ID}.pngï¼ˆä¸¥æ ¼æŒ‰è¯´æ˜è¦æ±‚ï¼‰\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"å¼€å§‹æ”¹è¿›ç‰ˆæ¨ç†...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    generated_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (caption_batch, image_name_batch) in enumerate(testing_dataset):\n",
    "        batch_size = tf.shape(caption_batch)[0]\n",
    "\n",
    "        # ç”Ÿæˆéšæœºå™ªå£°\n",
    "        noise = tf.random.normal([batch_size, hparas['Z_DIM']])\n",
    "\n",
    "        # ç¼–ç æ–‡æœ¬\n",
    "        word_features, sentence_feature = text_encoder_improved(caption_batch, training=False)\n",
    "\n",
    "        # ç”Ÿæˆå›¾åƒ\n",
    "        fake_images, _ = generator_improved(\n",
    "            word_features, sentence_feature, noise, training=False\n",
    "        )\n",
    "\n",
    "        # ä¿å­˜æ¯å¼ å›¾åƒ\n",
    "        for i in range(fake_images.shape[0]):\n",
    "            img = fake_images[i].numpy()\n",
    "            img = ((img + 1.0) * 127.5).astype(np.uint8)  # [-1,1] -> [0,255]\n",
    "\n",
    "            idx_val = image_name_batch[i].numpy()\n",
    "            if isinstance(idx_val, bytes):\n",
    "                idx_val = idx_val.decode('utf-8')\n",
    "            filename = f\"inference_{idx_val}.png\"\n",
    "            img_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            PIL.Image.fromarray(img).save(img_path)\n",
    "            generated_count += 1\n",
    "\n",
    "        if (step + 1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            speed = generated_count / max(elapsed, 1e-6)\n",
    "            print(f\"å·²ç”Ÿæˆ {generated_count} å¼ å›¾åƒ... ({speed:.2f} å¼ /ç§’)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"=\"*60)\n",
    "    print(f\"âœ“ æ¨ç†å®Œæˆï¼\")\n",
    "    print(f\"  æ€»å›¾åƒæ•°: {generated_count}\")\n",
    "    print(f\"  æ€»æ—¶é—´: {total_time:.2f} ç§’\")\n",
    "    print(f\"  å¹³å‡é€Ÿåº¦: {generated_count/total_time:.2f} å¼ /ç§’\")\n",
    "    print(f\"  è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå…ˆåŠ è½½æœ€ä½³æˆ–æœ€ç»ˆcheckpointåå†è¿è¡Œï¼‰\n",
    "# best_ckpt = tf.train.latest_checkpoint(hparas['CHECKPOINTS_DIR'])\n",
    "# checkpoint_improved.restore(best_ckpt)\n",
    "# improved_inference(testing_dataset, output_dir='inference/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JI7ETB2Ct_5k",
    "outputId": "bc2bac01-8f93-449c-8adb-d5e0244313f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/testing\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/testing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o24KzKV0a-x",
    "outputId": "7676ffce-273a-49c0-d87e-b5e79573ea0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsupJTGDmr3j",
    "outputId": "b3358819-85dd-4d38-eec4-e2d7321cb23d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… save_images å‡½æ•¸å·²æ›´æ–°\n",
      "   ä¿®å¾©å…§å®¹ï¼š[-1, 1] â†’ [0, 1] çš„æ­£ç¢ºè½‰æ›\n",
      "   åŸå•é¡Œï¼šèª¤è½‰æ›å°è‡´æš—è‰²è®Šæˆç™½éœ§\n",
      "   ä¿®å¾©å¾Œï¼šåœ–åƒæ‡‰è©²é¡¯ç¤ºæ­£å¸¸çš„é¡è‰²å’Œå°æ¯”åº¦\n"
     ]
    }
   ],
   "source": [
    "# ============= æ­£ç¡®çš„åœ–åƒä¿å­˜å‡½æ•¸ï¼ˆä¿®å¾©ç™½éœ§å•é¡Œï¼‰=============\n",
    "# å•é¡Œï¼šåœ–åƒæ˜¯ [-1, 1] ç¯„åœï¼Œä½†åœ¨ä¿å­˜æ™‚è¢«è½‰æ›æˆäº† [0, 1]ï¼Œå°è‡´æš—çš„éƒ¨åˆ†è®Šæˆæ¥è¿‘ç™½è‰²\n",
    "# è§£æ±ºæ–¹æ¡ˆï¼šæ­£ç¢ºçš„è½‰æ›å…¬å¼æ˜¯ [-1, 1] â†’ [0, 255]\n",
    "\n",
    "def save_images(images, grid_size, filepath):\n",
    "    \"\"\"\n",
    "    å°‡ [-1, 1] ç¯„åœçš„åœ–åƒæ‰¹æ¬¡ä¿å­˜ç‚ºç¶²æ ¼åœ–åƒ\n",
    "\n",
    "    Args:\n",
    "        images: shape (batch, 64, 64, 3)ï¼Œå€¼åœ¨ [-1, 1] ç¯„åœ\n",
    "        grid_size: [rows, cols] ç¶²æ ¼å¤§å°\n",
    "        filepath: ä¿å­˜è·¯å¾‘\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    rows, cols = grid_size\n",
    "    num_images = min(images.shape[0], rows * cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [[axes]]\n",
    "    elif rows == 1 or cols == 1:\n",
    "        axes = axes.reshape(rows, cols)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # æ­£ç¢ºçš„è½‰æ›ï¼š[-1, 1] â†’ [0, 1]\n",
    "        img = images[i]\n",
    "        img = (img + 1.0) / 2.0  # è½‰æ›åˆ° [0, 1]\n",
    "        img = np.clip(img, 0.0, 1.0)  # ç¢ºä¿åœ¨ [0, 1] ç¯„åœå…§\n",
    "\n",
    "        # é¡¯ç¤ºåœ–åƒ\n",
    "        if img.shape[-1] == 3:  # RGB åœ–åƒ\n",
    "            ax.imshow(img)\n",
    "        else:  # ç°åº¦åœ–åƒ\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # éš±è—å¤šé¤˜çš„å­åœ–\n",
    "    for i in range(num_images, rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"âœ“ åœ–åƒå·²ä¿å­˜: {filepath}\")\n",
    "\n",
    "\n",
    "# é©—è­‰ä¿®å¾©\n",
    "print(\"\\nâœ… save_images å‡½æ•¸å·²æ›´æ–°\")\n",
    "print(\"   ä¿®å¾©å…§å®¹ï¼š[-1, 1] â†’ [0, 1] çš„æ­£ç¢ºè½‰æ›\")\n",
    "print(\"   åŸå•é¡Œï¼šèª¤è½‰æ›å°è‡´æš—è‰²è®Šæˆç™½éœ§\")\n",
    "print(\"   ä¿®å¾©å¾Œï¼šåœ–åƒæ‡‰è©²é¡¯ç¤ºæ­£å¸¸çš„é¡è‰²å’Œå°æ¯”åº¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A10Ric29e7_b"
   },
   "source": [
    "## ğŸ“‹ ä½¿ç”¨æ”¹è¿›ç‰ˆæ¨¡å‹çš„å®Œæ•´æµç¨‹\n",
    "\n",
    "### æ­¥éª¤1: è®­ç»ƒæ¨¡å‹\n",
    "```python\n",
    "# å·²ç»åœ¨ä¸Šé¢çš„cellä¸­å®šä¹‰ï¼Œç›´æ¥è¿è¡Œè®­ç»ƒ\n",
    "history_improved = improved_train(training_dataset_improved, hparas, start_epoch=0)\n",
    "```\n",
    "\n",
    "### æ­¥éª¤2: å¯è§†åŒ–è®­ç»ƒå†å²\n",
    "```python\n",
    "plot_training_history(history_improved)\n",
    "```\n",
    "\n",
    "### æ­¥éª¤3: åŠ è½½æœ€ä½³æ¨¡å‹å¹¶æ¨ç†\n",
    "```python\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "best_checkpoint = hparas['CHECKPOINTS_DIR'] + '/ckpt_best-1'\n",
    "checkpoint_improved.restore(best_checkpoint)\n",
    "print(f\"å·²åŠ è½½æœ€ä½³æ¨¡å‹: {best_checkpoint}\")\n",
    "\n",
    "# ç”Ÿæˆæµ‹è¯•é›†å›¾åƒ\n",
    "improved_inference(testing_dataset, output_dir='inference/improved_v2')\n",
    "```\n",
    "\n",
    "### æ­¥éª¤4: è®¡ç®—åˆ†æ•°\n",
    "ä½¿ç”¨æ¯”èµ›æä¾›çš„è¯„åˆ†è„šæœ¬è¯„ä¼°ç”Ÿæˆçš„å›¾åƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å…³é”®æ”¹è¿›ç‚¹æ€»ç»“\n",
    "\n",
    "### 1. **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰**\n",
    "- **è¯çº§æ³¨æ„åŠ›**: è®©ç”Ÿæˆå™¨å…³æ³¨æè¿°ä¸­çš„å…³é”®è¯ï¼ˆå¦‚é¢œè‰²ã€å½¢çŠ¶ï¼‰\n",
    "- **è‡ªæ³¨æ„åŠ›**: è®©ç”Ÿæˆå™¨å…³æ³¨å›¾åƒçš„ä¸åŒåŒºåŸŸï¼Œç”Ÿæˆæ›´è¿è´¯çš„ç»“æ„\n",
    "\n",
    "### 2. **æ›´æ·±çš„ç½‘ç»œç»“æ„**\n",
    "- **æ®‹å·®è¿æ¥**: å¸®åŠ©æ¢¯åº¦æµåŠ¨ï¼Œå…è®¸è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ\n",
    "- **æ‰¹å½’ä¸€åŒ–**: ç¨³å®šè®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "### 3. **å¤šé‡æŸå¤±å‡½æ•°**\n",
    "- **å¯¹æŠ—æŸå¤±**: åŸºæœ¬çš„GANæŸå¤±\n",
    "- **ç‰¹å¾åŒ¹é…æŸå¤±**: è®©ç”Ÿæˆå›¾åƒçš„ç‰¹å¾åˆ†å¸ƒæ¥è¿‘çœŸå®å›¾åƒ\n",
    "- **é¢œè‰²ä¸€è‡´æ€§æŸå¤±**: é¼“åŠ±ç”Ÿæˆè‡ªç„¶çš„é¢œè‰²è¿‡æ¸¡\n",
    "- **å¤šæ ·æ€§æ­£åˆ™åŒ–**: é˜²æ­¢æ¨¡å¼å´©æºƒï¼Œå¢åŠ ç”Ÿæˆå¤šæ ·æ€§\n",
    "\n",
    "### 4. **è®­ç»ƒç­–ç•¥ä¼˜åŒ–**\n",
    "- **ä¸å¯¹ç§°å­¦ä¹ ç‡**: åˆ¤åˆ«å™¨å­¦ä¹ ç‡ç•¥é«˜äºç”Ÿæˆå™¨\n",
    "- **å¤šæ­¥è®­ç»ƒ**: ç”Ÿæˆå™¨è®­ç»ƒ2æ­¥ï¼Œåˆ¤åˆ«å™¨è®­ç»ƒ1æ­¥\n",
    "- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "- **æ—©åœæœºåˆ¶**: é¿å…è¿‡æ‹Ÿåˆ\n",
    "\n",
    "### 5. **æ ‡ç­¾å¹³æ»‘**\n",
    "- é˜²æ­¢åˆ¤åˆ«å™¨è¿‡äºè‡ªä¿¡\n",
    "- çœŸå®æ ‡ç­¾ä½¿ç”¨[0.9, 1.0]èŒƒå›´\n",
    "- å‡æ ‡ç­¾ä½¿ç”¨[0.0, 0.1]èŒƒå›´\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ è°ƒä¼˜å»ºè®®\n",
    "\n",
    "å¦‚æœç”Ÿæˆè´¨é‡ä»ä¸ç†æƒ³ï¼Œå¯ä»¥å°è¯•ï¼š\n",
    "\n",
    "1. **å¢åŠ è®­ç»ƒè½®æ•°**: å°†`N_EPOCH`ä»300å¢åŠ åˆ°500\n",
    "2. **è°ƒæ•´æŸå¤±æƒé‡**:\n",
    "   - å¢åŠ `LAMBDA_FM`ï¼ˆç‰¹å¾åŒ¹é…æƒé‡ï¼‰åˆ°20.0\n",
    "   - è°ƒæ•´`LAMBDA_COLOR`åˆ°1.0\n",
    "3. **ä¿®æ”¹å­¦ä¹ ç‡**:\n",
    "   - é™ä½ç”Ÿæˆå™¨å­¦ä¹ ç‡åˆ°5e-5\n",
    "   - ä¿æŒåˆ¤åˆ«å™¨å­¦ä¹ ç‡ä¸º2e-4\n",
    "4. **å¢åŠ æ‰¹æ¬¡å¤§å°**: å¦‚æœGPUå†…å­˜å…è®¸ï¼Œå¢åŠ åˆ°128\n",
    "5. **æ•°æ®å¢å¼º**: è¿›ä¸€æ­¥åŠ å¼ºæ•°æ®å¢å¼ºï¼ˆåœ¨dataset_generatorä¸­ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. è®­ç»ƒæ—¶é—´ä¼šæ¯”åŸå§‹æ¨¡å‹é•¿ï¼Œå› ä¸ºæ¨¡å‹æ›´å¤æ‚\n",
    "2. å»ºè®®ä½¿ç”¨GPUè®­ç»ƒï¼ˆåœ¨Colabä¸Šä½¿ç”¨T4æˆ–æ›´å¥½çš„GPUï¼‰\n",
    "3. å®šæœŸæ£€æŸ¥ç”Ÿæˆçš„æ ·æœ¬å›¾åƒï¼Œç¡®ä¿æ¨¡å‹æ²¡æœ‰æ¨¡å¼å´©æºƒ\n",
    "4. å¦‚æœå‡ºç°æ¢¯åº¦çˆ†ç‚¸ï¼Œå¯ä»¥é™ä½å­¦ä¹ ç‡\n",
    "5. æœ€ä½³æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜ä¸º`ckpt_best`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
