{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-sY702fwXxY"
   },
   "source": [
    "# 改进的文本到图像生成模型 (Text-to-Image GAN)\n",
    "\n",
    "## 主要改进点\n",
    "\n",
    "本notebook基于比赛PDF中的hints进行了以下改进：\n",
    "\n",
    "### 1. 数据增强 (Data Augmentation)\n",
    "- 随机裁剪和翻转\n",
    "- 亮度和对比度调整\n",
    "- 颜色抖动\n",
    "- 提高模型泛化能力，防止过拟合\n",
    "\n",
    "### 2. 改进的模型架构\n",
    "- **Text Encoder**: 使用双向GRU + 多层RNN结构，更好地捕捉文本语义\n",
    "- **Generator**: 采用DCGAN架构，使用转置卷积逐步生成64x64图像\n",
    "- **Discriminator**: CNN架构 + Dropout + Batch Normalization，提高判别能力\n",
    "\n",
    "### 3. 更复杂的损失函数\n",
    "- 标签平滑 (Label Smoothing) - 防止判别器过于自信\n",
    "- WGAN-GP选项 - 更稳定的训练（可选）\n",
    "- 梯度惩罚 (Gradient Penalty) - 改善训练稳定性\n",
    "\n",
    "### 4. 训练技巧\n",
    "- 梯度裁剪 - 防止梯度爆炸\n",
    "- 学习率调度 - 动态调整学习率\n",
    "- 最佳模型保存 - 保存表现最好的模型\n",
    "- 训练历史可视化 - 监控训练过程\n",
    "\n",
    "### 5. 可选的高级技术\n",
    "- 预训练词嵌入 (Word2Vec/GloVe)\n",
    "- 多样性检查 - 防止模式崩溃\n",
    "- 更强的数据增强\n",
    "\n",
    "## 使用说明\n",
    "\n",
    "1. 按顺序运行所有cell\n",
    "2. 训练过程中会定期保存checkpoint和生成样本图像\n",
    "3. 可以调整 `hparas` 中的超参数进行实验\n",
    "4. 如果训练不稳定，可以尝试启用WGAN-GP (`USE_WGAN_GP = True`)\n",
    "\n",
    "## 参考论文\n",
    "- Generative Adversarial Text to Image Synthesis\n",
    "- DCGAN: Unsupervised Representation Learning with Deep Convolutional GANs\n",
    "- Improved Training of Wasserstein GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "islve_lE51yC"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ1GwGjS6AfM"
   },
   "source": [
    "Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuHfeMac70N2",
    "outputId": "b4309d60-5c9f-4690-b5b5-eda728017aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "[np.str_('9'), np.str_('1'), np.str_('82'), np.str_('5'), np.str_('11'), np.str_('70'), np.str_('20'), np.str_('31'), np.str_('3'), np.str_('29'), np.str_('20'), np.str_('2'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427'), np.str_('5427')]\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "\n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "\n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rbp0zk-e8ABG",
    "outputId": "38befaa6-a257-49bc-b2f4-cc7428b110fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = r'C:\\Users\\11958\\Desktop\\vscode-c\\c\\deep_learning\\competition\\competition3\\2025-datalab-cup3-reverse-image-caption\\dataset'\n",
    "df = pd.read_pickle(os.path.join(data_path, 'text2ImgData.pkl'))\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vq39KJ58Pke"
   },
   "source": [
    "Create Dataset by Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_z912VArJJrC",
    "outputId": "a87e72aa-4738-4909-8fbb-d0b6732c794b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\n",
      "6734    ./102flowers/image_06734.jpg\n",
      "6736    ./102flowers/image_06736.jpg\n",
      "6737    ./102flowers/image_06737.jpg\n",
      "6738    ./102flowers/image_06738.jpg\n",
      "6739    ./102flowers/image_06739.jpg\n",
      "Name: ImagePath, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['ImagePath'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0sS1Ukt9IVZ"
   },
   "source": [
    "Conditional GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NJOv23u9H19"
   },
   "outputs": [],
   "source": [
    "# 改进的Text Encoder - 使用双向GRU和更深的结构\n",
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    改进的文本编码器\n",
    "    - 使用双向GRU\n",
    "    - 添加Dropout防止过拟合\n",
    "    - 多层RNN结构\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "\n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        \n",
    "        # 使用双向GRU获取更好的文本表示\n",
    "        self.bi_gru = layers.Bidirectional(\n",
    "            layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                      return_sequences=True,\n",
    "                      return_state=False,\n",
    "                      recurrent_initializer='glorot_uniform',\n",
    "                      recurrent_dropout=0.2)\n",
    "        )\n",
    "        \n",
    "        # 第二层GRU用于进一步提取特征\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'] * 2,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform',\n",
    "                              recurrent_dropout=0.2)\n",
    "\n",
    "    def call(self, text, hidden, training=True):\n",
    "        text = self.embedding(text)\n",
    "        text = self.dropout(text, training=training)\n",
    "        \n",
    "        # 双向GRU处理\n",
    "        text = self.bi_gru(text, training=training)\n",
    "        \n",
    "        # 第二层GRU\n",
    "        gru_output = self.gru(text, initial_state=hidden)\n",
    "\n",
    "        if isinstance(gru_output, (list, tuple)):\n",
    "            output = gru_output[0]\n",
    "            state = gru_output[1]\n",
    "        else:\n",
    "            output = gru_output\n",
    "            state = output[:, -1, :]\n",
    "\n",
    "        # 返回最后时间步的输出和最终状态\n",
    "        return output[:, -1, :], state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE'] * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4OYULDC9nb3"
   },
   "outputs": [],
   "source": [
    "# 改进的Generator - 使用DCGAN架构\n",
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    改进的生成器 - 使用转置卷积(反卷积)架构\n",
    "    采用DCGAN的设计思路\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # 文本处理层\n",
    "        self.text_fc = tf.keras.layers.Dense(256)\n",
    "        self.text_bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 初始全连接层：将噪声和文本映射到特征图\n",
    "        self.fc1 = tf.keras.layers.Dense(4 * 4 * 512, use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 反卷积层 - 逐步放大特征图\n",
    "        # 4x4 -> 8x8\n",
    "        self.deconv1 = tf.keras.layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), \n",
    "                                                        padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 8x8 -> 16x16\n",
    "        self.deconv2 = tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), \n",
    "                                                        padding='same', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 16x16 -> 32x32\n",
    "        self.deconv3 = tf.keras.layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), \n",
    "                                                        padding='same', use_bias=False)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 32x32 -> 64x64\n",
    "        self.deconv4 = tf.keras.layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), \n",
    "                                                        padding='same', use_bias=False)\n",
    "\n",
    "    def call(self, text, noise_z, training=True):\n",
    "        # 处理文本特征\n",
    "        text = self.text_fc(text)\n",
    "        text = self.text_bn(text, training=training)\n",
    "        text = tf.nn.relu(text)\n",
    "        \n",
    "        # 连接噪声和文本\n",
    "        x = tf.concat([noise_z, text], axis=1)\n",
    "        \n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 重塑为特征图 (batch, 4, 4, 512)\n",
    "        x = tf.reshape(x, [-1, 4, 4, 512])\n",
    "        \n",
    "        # 反卷积层 1: 4x4 -> 8x8\n",
    "        x = self.deconv1(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 反卷积层 2: 8x8 -> 16x16\n",
    "        x = self.deconv2(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 反卷积层 3: 16x16 -> 32x32\n",
    "        x = self.deconv3(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 反卷积层 4: 32x32 -> 64x64\n",
    "        x = self.deconv4(x)\n",
    "        output = tf.nn.tanh(x)\n",
    "        \n",
    "        return x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-Fyhk-n-U9O"
   },
   "outputs": [],
   "source": [
    "# 改进的Discriminator - 使用DCGAN架构\n",
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    改进的判别器 - 使用卷积神经网络\n",
    "    采用DCGAN的设计思路\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # 图像卷积层\n",
    "        # 64x64 -> 32x32\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same')\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        # 32x32 -> 16x16\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        # 16x16 -> 8x8\n",
    "        self.conv3 = tf.keras.layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same')\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout3 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        # 8x8 -> 4x4\n",
    "        self.conv4 = tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same')\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout4 = tf.keras.layers.Dropout(0.3)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        # 文本处理层\n",
    "        self.text_fc = tf.keras.layers.Dense(256)\n",
    "        self.text_bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 融合层\n",
    "        self.fc1 = tf.keras.layers.Dense(512)\n",
    "        self.fc2 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, img, text, training=True):\n",
    "        # 处理图像\n",
    "        x = self.conv1(img)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout3(x, training=training)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        x = self.dropout4(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # 处理文本\n",
    "        text = self.text_fc(text)\n",
    "        text = self.text_bn(text, training=training)\n",
    "        text = tf.nn.leaky_relu(text, alpha=0.2)\n",
    "        \n",
    "        # 融合图像和文本特征\n",
    "        combined = tf.concat([x, text], axis=1)\n",
    "        combined = self.fc1(combined)\n",
    "        combined = tf.nn.leaky_relu(combined, alpha=0.2)\n",
    "        \n",
    "        logits = self.fc2(combined)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xjkhkRwk-Z2g"
   },
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iwRTOVAm-kua"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJF0beuy-qUk"
   },
   "outputs": [],
   "source": [
    "# 改进的优化器 - 使用不同的学习率\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR_G'], beta_1=hparas['BETA_1'], beta_2=hparas['BETA_2'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR_D'], beta_1=hparas['BETA_1'], beta_2=hparas['BETA_2'])\n",
    "\n",
    "# 学习率调度器\n",
    "lr_schedule_g = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=hparas['LR_G'],\n",
    "    decay_steps=1000,\n",
    "    decay_rate=hparas['LR_DECAY'],\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "lr_schedule_d = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=hparas['LR_D'],\n",
    "    decay_steps=1000,\n",
    "    decay_rate=hparas['LR_DECAY'],\n",
    "    staircase=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmpagdaY-uB6"
   },
   "outputs": [],
   "source": [
    "# 改进的训练步骤 - 添加Gradient Penalty和更稳定的训练策略\n",
    "@tf.function\n",
    "def train_step(real_image, caption, hidden, use_wgan_gp=False):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # 编码文本\n",
    "        text_embed, hidden = text_encoder(caption, hidden, training=True)\n",
    "        \n",
    "        # 生成假图像\n",
    "        _, fake_image = generator(text_embed, noise, training=True)\n",
    "        \n",
    "        # 判别器判断真假\n",
    "        real_logits, real_output = discriminator(real_image, text_embed, training=True)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed, training=True)\n",
    "\n",
    "        # 计算损失\n",
    "        if use_wgan_gp:\n",
    "            # WGAN-GP损失\n",
    "            g_loss = generator_loss(fake_logits, use_wgan=True)\n",
    "            d_loss = discriminator_loss(real_logits, fake_logits, use_wgan=True)\n",
    "            \n",
    "            # 添加梯度惩罚\n",
    "            gp = gradient_penalty(discriminator, real_image, fake_image, text_embed)\n",
    "            d_loss = d_loss + hparas['LAMBDA_GP'] * gp\n",
    "        else:\n",
    "            # 标准GAN损失（带标签平滑）\n",
    "            g_loss = generator_loss(fake_logits, use_wgan=False)\n",
    "            d_loss = discriminator_loss(real_logits, fake_logits, use_wgan=False)\n",
    "\n",
    "    # 计算梯度\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    # 梯度裁剪 - 防止梯度爆炸\n",
    "    grad_g, _ = tf.clip_by_global_norm(grad_g, 5.0)\n",
    "    grad_d, _ = tf.clip_by_global_norm(grad_d, 5.0)\n",
    "\n",
    "    # 应用梯度\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "\n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv6K3z7C-xa_"
   },
   "source": [
    "Visualiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Li-iepjT-0Aa"
   },
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXt2mGpA-5Td"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高级技巧和额外的改进"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像质量评估函数\n",
    "def calculate_inception_score(images, n_split=10, eps=1e-16):\n",
    "    \"\"\"\n",
    "    计算Inception Score来评估生成图像的质量\n",
    "    注意：这需要预训练的Inception模型\n",
    "    \"\"\"\n",
    "    # 这里简化实现，实际应用中需要使用Inception v3\n",
    "    pass\n",
    "\n",
    "def calculate_fid_score(real_images, fake_images):\n",
    "    \"\"\"\n",
    "    计算Frechet Inception Distance (FID)\n",
    "    FID越低表示生成图像质量越好\n",
    "    \"\"\"\n",
    "    # 简化版本，实际需要使用Inception特征\n",
    "    pass\n",
    "\n",
    "# 多样性检查 - 防止模式崩溃\n",
    "def check_diversity(generated_images, threshold=0.9):\n",
    "    \"\"\"\n",
    "    检查生成图像的多样性，防止mode collapse\n",
    "    \"\"\"\n",
    "    # 计算图像之间的相似度\n",
    "    n = len(generated_images)\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(min(n, 10)):\n",
    "        for j in range(i+1, min(n, 10)):\n",
    "            img1 = generated_images[i].numpy().flatten()\n",
    "            img2 = generated_images[j].numpy().flatten()\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            similarity = np.dot(img1, img2) / (np.linalg.norm(img1) * np.linalg.norm(img2))\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    avg_similarity = np.mean(similarities)\n",
    "    print(f\"Average similarity between generated images: {avg_similarity:.4f}\")\n",
    "    \n",
    "    if avg_similarity > threshold:\n",
    "        print(\"Warning: High similarity detected! Possible mode collapse.\")\n",
    "    else:\n",
    "        print(\"Good diversity in generated images.\")\n",
    "    \n",
    "    return avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZjiy90tIZTg"
   },
   "outputs": [],
   "source": [
    "# 改进的训练函数 - 添加更多训练技巧\n",
    "def train(dataset, epochs, use_wgan_gp=False, start_epoch=0, best_loss=None):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    # 用于追踪最佳模型\n",
    "    best_g_loss = best_loss if best_loss is not None else float('inf')\n",
    "    \n",
    "    # 训练历史记录\n",
    "    history = {\n",
    "        'g_loss': [],\n",
    "        'd_loss': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(start_epoch, hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        step = 0\n",
    "        for image, caption in dataset:\n",
    "            # 训练判别器多次（WGAN的常见做法）\n",
    "            for _ in range(1 if not use_wgan_gp else 5):\n",
    "                g_loss, d_loss = train_step(image, caption, hidden, use_wgan_gp)\n",
    "            \n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "            step += 1\n",
    "            \n",
    "            if step >= steps_per_epoch:\n",
    "                break\n",
    "        \n",
    "        # 计算平均损失\n",
    "        avg_g_loss = g_total_loss / steps_per_epoch\n",
    "        avg_d_loss = d_total_loss / steps_per_epoch\n",
    "        \n",
    "        history['g_loss'].append(float(avg_g_loss))\n",
    "        history['d_loss'].append(float(avg_d_loss))\n",
    "\n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "\n",
    "        print(\"Epoch {}/{}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(\n",
    "            epoch+1, hparas['N_EPOCH'],\n",
    "            avg_g_loss,\n",
    "            avg_d_loss))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if avg_g_loss < best_g_loss:\n",
    "            best_g_loss = avg_g_loss\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix + '_best')\n",
    "            print(f'  -> Best model saved with G loss: {best_g_loss:.4f}')\n",
    "\n",
    "        # 定期保存模型\n",
    "        if (epoch + 1) % hparas['SAVE_FREQ'] == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            print(f'  -> Checkpoint saved at epoch {epoch+1}')\n",
    "\n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], 'samples/demo/train_{:02d}.jpg'.format(epoch))\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['g_loss'])\n",
    "    plt.title('Generator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['d_loss'])\n",
    "    plt.title('Discriminator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('samples/demo/training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQsD-cNAI5IJ"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Nd_9M9JI-4U"
   },
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    test_data_path = os.path.join(data_path, 'testData.pkl')\n",
    "    data = pd.read_pickle(test_data_path)\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(int)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTFKx9h8JDp3"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(os.path.join(data_path, 'text2ImgData.pkl'))\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "S2St5cglJGq2"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./inference/demo'):\n",
    "    os.makedirs('./inference/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIyEAoAZJKWv",
    "outputId": "2bdccd72-42d1-4742-d862-a79631f69183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7adc3d8fb4a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载最佳模型进行推理\n",
    "# 选项1: 加载最佳模型\n",
    "best_checkpoint = checkpoint_dir + '/ckpt_best-1'\n",
    "\n",
    "# 选项2: 加载最新的checkpoint\n",
    "# latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# 选项3: 加载特定epoch的checkpoint\n",
    "# specific_checkpoint = checkpoint_dir + '/ckpt-10'\n",
    "\n",
    "print(\"Available checkpoints:\")\n",
    "for ckpt in tf.train.get_checkpoint_state(checkpoint_dir).all_model_checkpoint_paths:\n",
    "    print(f\"  - {ckpt}\")\n",
    "\n",
    "# 尝试加载最佳模型\n",
    "try:\n",
    "    status = checkpoint.restore(best_checkpoint)\n",
    "    print(f\"\\nSuccessfully loaded best model: {best_checkpoint}\")\n",
    "except:\n",
    "    print(f\"\\nBest model not found, loading latest checkpoint...\")\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest:\n",
    "        checkpoint.restore(latest)\n",
    "        print(f\"Loaded: {latest}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "610e68d3"
   },
   "outputs": [],
   "source": [
    "#檢查是否缺失文件\n",
    "\n",
    "import os\n",
    "\n",
    "base_image_directory = '/content/drive/MyDrive/Colab Notebooks/dl/comp3/2025-datalab-cup-3-reverse-image-caption/102flowers'\n",
    "\n",
    "missing_files = []\n",
    "\n",
    "for i in range(1, 8190):  # From 00001 to 08189\n",
    "    filename = f\"image_{i:05d}.jpg\"\n",
    "    file_path = os.path.join(base_image_directory, filename)\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(filename)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"The following {len(missing_files)} files are missing from {base_image_directory}:\")\n",
    "    for missing_file in missing_files:\n",
    "        print(missing_file)\n",
    "else:\n",
    "    print(f\"All image files from image_00001.jpg to image_08189.jpg are present in {base_image_directory}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化生成结果对比\n",
    "def visualize_results(num_examples=5):\n",
    "    \"\"\"\n",
    "    可视化不同描述生成的图像\n",
    "    \"\"\"\n",
    "    test_captions = [\n",
    "        \"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "        \"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "        \"the petals on this flower are white with a yellow center\",\n",
    "        \"this flower has a lot of small round pink petals.\",\n",
    "        \"this flower is orange in color, and has petals that are ruffled and rounded.\"\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(20, 4))\n",
    "    \n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    \n",
    "    for i, caption_text in enumerate(test_captions[:num_examples]):\n",
    "        # 将文本转换为ID列表\n",
    "        caption_ids = sent2IdList(caption_text)\n",
    "        caption_batch = np.array([caption_ids] * hparas['BATCH_SIZE'])\n",
    "        caption_batch = tf.constant(caption_batch, dtype=tf.int32)\n",
    "        \n",
    "        # 生成随机噪声\n",
    "        noise = tf.random.normal([hparas['BATCH_SIZE'], hparas['Z_DIM']])\n",
    "        \n",
    "        # 生成图像\n",
    "        fake_image = test_step(caption_batch, noise, hidden)\n",
    "        \n",
    "        # 显示第一张生成的图像\n",
    "        axes[i].imshow(fake_image[0].numpy() * 0.5 + 0.5)\n",
    "        axes[i].axis('off')\n",
    "        # 截取前30个字符作为标题\n",
    "        axes[i].set_title(caption_text[:30] + '...', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('samples/demo/comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualization saved to samples/demo/comparison.png\")\n",
    "\n",
    "# 运行可视化（在训练后）\n",
    "# visualize_results()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
