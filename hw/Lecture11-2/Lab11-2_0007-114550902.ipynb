{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec803a8",
   "metadata": {},
   "source": [
    "## PART 1 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf846aff",
   "metadata": {},
   "source": [
    "Import and configure modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe06c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as display\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa3bbf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f2d5e",
   "metadata": {},
   "source": [
    "Visualize the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f7b51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    # in order to use CNN, add one additional dimension \n",
    "    # to the original image\n",
    "    # img shape: [height, width, channel] -> [batch_size, height, width, channel]\n",
    "    img = img[tf.newaxis, :]\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937a416",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13668107",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "content_path = './dataset/content_nthu.jpg'\n",
    "content_image = load_img(content_path)\n",
    "print('Image shape:', content_image.shape)\n",
    "imshow(content_image, 'Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33203d7b",
   "metadata": {},
   "source": [
    "Load a pretrained network (VGG19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be339b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "\n",
    "# load pretrained network(VGG19)\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "prediction_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eed161",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396208d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876b6d9",
   "metadata": {},
   "source": [
    "Visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb21e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# summarize filter shapes\n",
    "for layer in vgg.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd558692",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = vgg.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 64, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(14, 14, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead43b2",
   "metadata": {},
   "source": [
    "Visualize feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3c1e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "model = tf.keras.Model(inputs=[vgg.input], outputs=vgg.layers[1].output)\n",
    "model.summary()\n",
    "\n",
    "# preprocess input\n",
    "content_image = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "content_image = tf.image.resize(content_image, (224, 224))\n",
    "\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(content_image)\n",
    "\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455dd27a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get feature maps for last convolutional layer in each block\n",
    "ixs = [2, 5, 10, 15, 20]\n",
    "outputs = [vgg.layers[i].output for i in ixs]\n",
    "model = tf.keras.Model(inputs=[vgg.input], outputs=outputs)\n",
    "feature_maps = model.predict(content_image)\n",
    "\n",
    "# plot the output from each block\n",
    "square = 8\n",
    "for i, fmap in enumerate(feature_maps):\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    ix = 1\n",
    "    print(outputs[i])\n",
    "    plt.figure(figsize=(16,16))\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            pyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
    "            ix += 1\n",
    "            \n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e592af",
   "metadata": {},
   "source": [
    "Visualize gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645287a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b8ade",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GradientModel(tf.keras.models.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(GradientModel, self).__init__()\n",
    "        self.vgg =  vgg_layers(layers)\n",
    "        self.num_style_layers = len(layers)\n",
    "        self.vgg.trainable = False\n",
    "        \n",
    "    # return the feature map of required layer\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e70f4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def visualize_gradient(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feature = extractor(image)\n",
    "        # grad = d_feature/d_image\n",
    "        grad = tape.gradient(tf.reduce_max(feature, axis=3), image)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fde725",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "content_image = load_img(content_path)\n",
    "\n",
    "# activation layer\n",
    "layers = ['block4_conv2']\n",
    "image = tf.Variable(content_image)\n",
    "\n",
    "extractor = GradientModel(layers)\n",
    "grad = visualize_gradient(image)\n",
    "\n",
    "# look at the range of gradients\n",
    "print(\"shape: \", grad.numpy().shape)\n",
    "print(\"min: \", grad.numpy().min())\n",
    "print(\"max: \", grad.numpy().max())\n",
    "print(\"mean: \", grad.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32295b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# normalize filter values to 0-1 so we can visualize them\n",
    "g_min, g_max = grad.numpy().min(), grad.numpy().max()\n",
    "filters = (grad - g_min) / (g_max - g_min)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(image.read_value()[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(filters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6197a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def visualize_gradient_single_layer(image, layer_i):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feature = extractor(image)\n",
    "        grad = tape.gradient(tf.reduce_mean(feature[:, :, :, layer_i]), image)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0fca8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "grad = visualize_gradient_single_layer(image, 77)\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "g_min, g_max = grad.numpy().min(), grad.numpy().max()\n",
    "filters = (grad - g_min) / (g_max - g_min)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(image.read_value()[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(filters[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90311e19",
   "metadata": {},
   "source": [
    "Guided-Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b3765",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from guided_backprop import GuidedBackprop\n",
    "\n",
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "\n",
    "# backprop_vgg = GuidedBackprop(model=vgg, layerName='predictions') # original\n",
    "backprop_vgg = GuidedBackprop(model=vgg, layerName='block5_conv4') # use this layer instead, b/c we need to extract from vgg19.\n",
    "grad = backprop_vgg.guided_backprop(x)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6ab77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# plot the original image and the three saliency map variants\n",
    "plt.figure(figsize=(16, 16), facecolor='w')\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Input')\n",
    "plt.imshow(tf.image.resize(content_image, (224, 224))[0])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Abs. saliency')\n",
    "plt.imshow(np.abs(grad).max(axis=-1), cmap='gray')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Pos. saliency')\n",
    "plt.imshow((np.maximum(0, grad) / grad.max()))\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Neg. saliency')\n",
    "plt.imshow((np.maximum(0, -grad) / -grad.min()))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
