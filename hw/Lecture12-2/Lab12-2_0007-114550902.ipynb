{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SXb5eSPkILW2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXb5eSPkILW2",
        "outputId": "bafab10a-90b2-46ca-bdd9-395059b8108f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "McDEDzAecTw8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McDEDzAecTw8",
        "outputId": "962d2d4c-9807-4cc5-f589-63dc34008a78"
      },
      "outputs": [],
      "source": [
        "#解压 补全缺失文件\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Define zip_path and extract_path here or ensure they are defined before this cell\n",
        "\n",
        "#zip_path = '/content/drive/MyDrive/Colab Notebooks/dl/lecture12-2/words_captcha.zip'\n",
        "#extract_path = '/content/drive/MyDrive/Colab Notebooks/dl/lecture12-2/'\n",
        "\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Get the list of missing files from the previous cell's output\n",
        "# Assuming 'missing_files' variable is available from the previous cell execution\n",
        "if 'missing_files' not in locals() or not missing_files:\n",
        "    print(\"No missing files found or missing_files list is not available. Skipping extraction.\")\n",
        "else:\n",
        "    print(f\"Attempting to extract {len(missing_files)} missing files.\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            # Create a set of missing files for efficient lookup\n",
        "            missing_files_set = set(missing_files)\n",
        "\n",
        "            # Get the list of files in the zip archive\n",
        "            zip_file_list = zip_ref.namelist()\n",
        "\n",
        "            # Filter the zip file list to include only missing files\n",
        "            files_to_extract = [file for file in zip_file_list if os.path.basename(file) in missing_files_set]\n",
        "\n",
        "            for file in tqdm(files_to_extract, desc=\"Extracting missing files\"):\n",
        "                # Construct the full path where the file should be extracted\n",
        "                # Ensure the path within the zip file is handled correctly relative to extract_path\n",
        "                # If the zip contains files directly at the root, just join extract_path and the filename\n",
        "                # If the zip contains a folder structure, adjust accordingly\n",
        "                file_dest_path = os.path.join(extract_path, os.path.basename(file))\n",
        "\n",
        "                # Check again just in case to avoid redundant extraction if a file appeared between checks\n",
        "                if not os.path.exists(file_dest_path):\n",
        "                    # Extract the specific file by its name within the zip\n",
        "                    # zip_ref.extract expects the path within the zip and an optional destination dir\n",
        "                    # To extract a specific file to a specific location, we need to handle the path.\n",
        "                    # A simpler approach for flat structures is to extract all and overwrite/skip existing,\n",
        "                    # but the user asked to only extract missing.\n",
        "                    # For this structure (a*.png directly in zip), we can extract to extract_path\n",
        "                    zip_ref.extract(file, extract_path)\n",
        "\n",
        "\n",
        "        print(f\"\\nExtraction complete. Missing files extracted to: {extract_path}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\nError: Zip file not found at {zip_path}\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nExtraction interrupted by user. Some files may not have been extracted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during extraction: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7764de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa7764de",
        "outputId": "32c24146-effd-4df8-e023-d4cd17f600f7"
      },
      "outputs": [],
      "source": [
        "#检查缺失文件\n",
        "import os\n",
        "\n",
        "extract_path = '/content/drive/MyDrive/Colab Notebooks/dl/lecture12-2/words_captcha'\n",
        "expected_files = [f'a{i}.png' for i in range(0, 140000)]\n",
        "\n",
        "existing_files = set(os.listdir(extract_path))\n",
        "\n",
        "missing_files = [f for f in expected_files if f not in existing_files]\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"Found {len(missing_files)} missing files:\")\n",
        "    # Print only the first 100 missing files if there are many\n",
        "    for i, missing_file in enumerate(missing_files):\n",
        "        if i < 100:\n",
        "            print(missing_file)\n",
        "        else:\n",
        "            print(\"...\")\n",
        "            break\n",
        "else:\n",
        "    print(\"No missing files found. All expected files are present.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80bc46d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e80bc46d",
        "outputId": "ef05488a-0628-45a1-f24c-c54a24ce552d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A8JDDvLSfNCW",
      "metadata": {
        "id": "A8JDDvLSfNCW"
      },
      "source": [
        "下载和准备数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622d4bb8",
      "metadata": {
        "id": "622d4bb8"
      },
      "outputs": [],
      "source": [
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7TFNyprWZ7-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TFNyprWZ7-P",
        "outputId": "b8999c20-ce29-4fdf-d9aa-30f8f19d6add"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/dl/lecture12-2/words_captcha/'  # 定义路径变量，便于维护\n",
        "\n",
        "img_name_list = []\n",
        "cap_list = []\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/dl/lecture12-2/spec_train_val.txt') as fin:  # 修改 txt 文件路径\n",
        "    for line in fin:\n",
        "        image_name, caption = line.strip().split()\n",
        "        img_name_list.append(f'{DATA_PATH}{image_name}.png')  # 修改图像路径\n",
        "        cap_list.append('<start> ' + ' '.join(caption) + ' <end>')\n",
        "\n",
        "test_img_name = set(glob.glob(f'{DATA_PATH}*.png')) - set(img_name_list)  # 修改 glob 路径\n",
        "img_name_list += sorted(test_img_name)\n",
        "\n",
        "print(img_name_list[0])\n",
        "print(cap_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0K-4MdSLaQzm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K-4MdSLaQzm",
        "outputId": "c9f8195d-2d4e-4834-8722-a78e3a2de641"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
        "                                                  oov_token='')\n",
        "tokenizer.fit_on_texts(cap_list)\n",
        "cap_seqs = tokenizer.texts_to_sequences(cap_list)\n",
        "\n",
        "cap_seqs = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding='post')\n",
        "max_length = len(cap_seqs[0])\n",
        "\n",
        "print(cap_list[0])\n",
        "print(cap_seqs[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fiyjp2tCfzqv",
      "metadata": {
        "id": "Fiyjp2tCfzqv"
      },
      "outputs": [],
      "source": [
        "img_name_train, img_name_valid = img_name_list[:100000], img_name_list[100000:120000]\n",
        "cap_seqs_train, cap_seqs_valid = cap_seqs[:100000], cap_seqs[100000:]\n",
        "\n",
        "img_name_test = img_name_list[120000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h2Tiup4bk1YI",
      "metadata": {
        "id": "h2Tiup4bk1YI"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (160, 300)\n",
        "BATCH_SIZE = 50\n",
        "BUFFER_SIZE = 5000\n",
        "\n",
        "EMBEDDING_DIM = 256\n",
        "UNITS = 512\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "EPOCHS = 10\n",
        "STEPS = len(img_name_train) // BATCH_SIZE\n",
        "LEARNING_RATE = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7--QFXRxk7RP",
      "metadata": {
        "id": "7--QFXRxk7RP"
      },
      "source": [
        "building data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R8LmE_59k90e",
      "metadata": {
        "id": "R8LmE_59k90e"
      },
      "outputs": [],
      "source": [
        "def map_test(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = img / 255 * 2 - 1\n",
        "    return img, img_path\n",
        "\n",
        "def map_train(img_path, caption):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = img / 255 * 2 - 1 # Normalize to [-1, 1]\n",
        "    return img, caption\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, cap_seqs_train))\\\n",
        "                               .map(map_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
        "                               .shuffle(BUFFER_SIZE)\\\n",
        "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
        "                               .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_valid = tf.data.Dataset.from_tensor_slices((img_name_valid, cap_seqs_valid))\\\n",
        "                               .map(map_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
        "                               .batch(BATCH_SIZE)\\\n",
        "                               .prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Z6A74nGlDUS",
      "metadata": {
        "id": "0Z6A74nGlDUS"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C09wRUz5mraN",
      "metadata": {
        "id": "C09wRUz5mraN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class conv_leaky_relu(layers.Layer):\n",
        "    def __init__(self, filters, size, stride):\n",
        "        super(conv_leaky_relu, self).__init__()\n",
        "        self.conv_2d = layers.Conv2D(filters, size, stride, padding='same')\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "        self.leakey_relu = layers.LeakyReLU(0.1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_2d(inputs)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.leakey_relu(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_AaLSukkmtBd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_AaLSukkmtBd",
        "outputId": "797e783a-71ef-4d69-828c-0920a83da78f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Input, layers, Model\n",
        "\n",
        "inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "x = conv_leaky_relu(64, 7, 2)(inputs)\n",
        "x = layers.MaxPool2D()(x)\n",
        "x = conv_leaky_relu(192, 3, 1)(x)\n",
        "x = layers.MaxPool2D()(x)\n",
        "x = conv_leaky_relu(128, 1, 1)(x)\n",
        "x = conv_leaky_relu(256, 3, 1)(x)\n",
        "x = conv_leaky_relu(256, 1, 1)(x)\n",
        "x = conv_leaky_relu(512, 3, 1)(x)\n",
        "x = layers.MaxPool2D()(x)\n",
        "x = conv_leaky_relu(256, 1, 1)(x)\n",
        "x = conv_leaky_relu(512, 3, 1)(x)\n",
        "x = conv_leaky_relu(256, 1, 1)(x)\n",
        "x = conv_leaky_relu(512, 3, 1)(x)\n",
        "x = conv_leaky_relu(256, 1, 1)(x)\n",
        "x = conv_leaky_relu(512, 3, 1)(x)\n",
        "x = conv_leaky_relu(256, 1, 1)(x)\n",
        "x = conv_leaky_relu(512, 3, 1)(x)\n",
        "x = conv_leaky_relu(512, 1, 1)(x)\n",
        "x = conv_leaky_relu(1024, 3, 1)(x)\n",
        "x = layers.MaxPool2D()(x)\n",
        "x = conv_leaky_relu(512, 1, 1)(x)\n",
        "x = conv_leaky_relu(1024, 3, 1)(x)\n",
        "x = conv_leaky_relu(512, 1, 1)(x)\n",
        "x = conv_leaky_relu(1024, 3, 1)(x)\n",
        "x = conv_leaky_relu(1024, 3, 1)(x)\n",
        "x = conv_leaky_relu(1024, 3, 2)(x)\n",
        "x = conv_leaky_relu(1024, 3, 1)(x)\n",
        "outputs = conv_leaky_relu(1024, 3, 1)(x)\n",
        "\n",
        "feature_extractor = Model(inputs=inputs, outputs=outputs, name='YOLO')\n",
        "\n",
        "feature_extractor.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7TGFExFDlBGf",
      "metadata": {
        "id": "7TGFExFDlBGf"
      },
      "outputs": [],
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v5SAH48NltYf",
      "metadata": {
        "id": "v5SAH48NltYf"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape == (batch_size, 15, embedding_dim)\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # score shape == (batch_size, 15, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, 15, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector == (batch_size, embedding_dim)\n",
        "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xVZpLeqQluhQ",
      "metadata": {
        "id": "xVZpLeqQluhQ"
      },
      "outputs": [],
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # x shape == (batch_size, 1)\n",
        "        # features shape == (batch_size, 15, embedding_dim)\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "\n",
        "        # context_vector shape == (batch_size, embedding_dim)\n",
        "        # attention_weights shape == (batch_size, 15, 1)\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_dim)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        # Try unpacking more values to see what GRU returns\n",
        "        gru_outputs = self.gru(x)\n",
        "        # Assuming it returns more than 2, let's try unpacking the first two\n",
        "        output, state = gru_outputs[0], gru_outputs[1] # Access elements by index\n",
        "\n",
        "\n",
        "        # x shape == (batch_size, 1, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "\n",
        "        # x shape == (batch_size, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2G7JpnJClTju",
      "metadata": {
        "id": "2G7JpnJClTju"
      },
      "outputs": [],
      "source": [
        "encoder = CNN_Encoder(EMBEDDING_DIM)\n",
        "decoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XyLHkV82l2G1",
      "metadata": {
        "id": "XyLHkV82l2G1"
      },
      "source": [
        "training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "he1sGe1Ul3QF",
      "metadata": {
        "id": "he1sGe1Ul3QF"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v2SWHiMxl5nt",
      "metadata": {
        "id": "v2SWHiMxl5nt"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/YOLO/'\n",
        "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
        "                           encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lrdyBoWalVBW",
      "metadata": {
        "id": "lrdyBoWalVBW"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h5-_BC1fl8Ig",
      "metadata": {
        "id": "h5-_BC1fl8Ig"
      },
      "outputs": [],
      "source": [
        "#@tf.function # Temporarily commented out for debugging\n",
        "def train_step(img_tensor, target):\n",
        "    batch_size = img_tensor.shape[0]\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "    hidden = decoder.reset_state(batch_size=batch_size)\n",
        "\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = feature_extractor(img_tensor)\n",
        "        # Explicitly calculate dimensions for reshape\n",
        "        feature_shape = tf.shape(features)\n",
        "        features = tf.reshape(features, (feature_shape[0], feature_shape[1] * feature_shape[2], feature_shape[3]))\n",
        "        features = encoder(features)\n",
        "\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden) # Unpack all three returned values\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    mean_loss = (loss / tf.cast(target.shape[1], tf.float32)) # Cast to float for division\n",
        "\n",
        "    trainable_variables = feature_extractor.trainable_variables + \\\n",
        "        encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KcBDJulGl9Wt",
      "metadata": {
        "id": "KcBDJulGl9Wt"
      },
      "outputs": [],
      "source": [
        "def predict(img_tensor):\n",
        "    batch_size = img_tensor.shape[0]\n",
        "    dec_input = tf.expand_dims(\n",
        "        [tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "\n",
        "    features = feature_extractor(img_tensor)\n",
        "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
        "    features = encoder(features)\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=batch_size)\n",
        "\n",
        "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "    for _ in range(max_length):\n",
        "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
        "        dec_input = tf.expand_dims(predicted_id, 1)\n",
        "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2DzP97uRl--V",
      "metadata": {
        "id": "2DzP97uRl--V"
      },
      "outputs": [],
      "source": [
        "def postprocess(segs):\n",
        "    result_list = []\n",
        "    for seq in segs:\n",
        "        result = ''\n",
        "        for s in seq[1:]:\n",
        "            if s == tokenizer.word_index['<end>']:\n",
        "                break\n",
        "            result += tokenizer.index_word[s]\n",
        "        result_list.append(result)\n",
        "    return result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bkmDNMs0l_7s",
      "metadata": {
        "id": "bkmDNMs0l_7s"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataset_valid):\n",
        "    sample_count = 0\n",
        "    correct_count = 0\n",
        "    for img_tensor, target in dataset_valid:\n",
        "        pred_list = postprocess(predict(img_tensor).numpy())\n",
        "        real_list = postprocess(target.numpy())\n",
        "\n",
        "        for pred, real in zip(pred_list, real_list):\n",
        "            sample_count += 1\n",
        "            if pred == real:\n",
        "                correct_count += 1\n",
        "\n",
        "    return correct_count / sample_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VuY9zBs3mBmP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "VuY9zBs3mBmP",
        "outputId": "8dd743a2-b033-4c6a-c63b-9b161748fd84"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "loss_plot = []\n",
        "start = time.time()\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    loss = 0\n",
        "    pbar = tqdm(dataset_train, total=STEPS, desc=f'Epoch {epoch + 1:2d}')\n",
        "    for (step, (img_tensor, target)) in enumerate(pbar):\n",
        "        loss += train_step(img_tensor, target)\n",
        "        pbar.set_postfix({'loss': loss.numpy() / (step + 1)})\n",
        "\n",
        "    loss_plot.append(loss / STEPS)\n",
        "    ckpt_manager.save()\n",
        "\n",
        "    score = evaluate(dataset_valid)\n",
        "    print(f'Validation accuracy: {score:.2f}')\n",
        "\n",
        "print('Time taken for {} epoch {} sec\\\\n'.format(EPOCHS - start_epoch, time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ESy76IyKmC26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ESy76IyKmC26",
        "outputId": "4eea9329-6399-4c5d-bdd1-85eb0dbcb455"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JILlVNncmE_t",
      "metadata": {
        "id": "JILlVNncmE_t"
      },
      "source": [
        "Predict Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v-OzqOY6mG3N",
      "metadata": {
        "id": "v-OzqOY6mG3N"
      },
      "outputs": [],
      "source": [
        "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
        "                           encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt.restore('./checkpoints/YOLO/ckpt-10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZW1bsDVimIkt",
      "metadata": {
        "id": "ZW1bsDVimIkt"
      },
      "outputs": [],
      "source": [
        "score = evaluate(dataset_valid)\n",
        "print(f'Validation accuracy: {score:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V6iVi9OvmJ1Z",
      "metadata": {
        "id": "V6iVi9OvmJ1Z"
      },
      "outputs": [],
      "source": [
        "def map_test(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = img / 255 * 2 - 1\n",
        "    return img, img_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iz0fJT1tmK01",
      "metadata": {
        "id": "iz0fJT1tmK01"
      },
      "outputs": [],
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((img_name_test))\\\n",
        "                              .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
        "                              .batch(100)\\\n",
        "                              .prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7c91vGkmLxj",
      "metadata": {
        "id": "A7c91vGkmLxj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "with open('./Lab12-2_110062802.txt', 'w') as fout:\n",
        "    for step, (img_tensor, img_path) in enumerate(tqdm(dataset_test)):\n",
        "        pred_list = postprocess(predict(img_tensor).numpy())\n",
        "        for path, pred in zip(img_path, pred_list):\n",
        "            path = path.numpy().decode('utf-8')\n",
        "            name = re.search('(a[0-9]+)', path).group(1)\n",
        "            fout.write(f'{name} {pred}\\\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
