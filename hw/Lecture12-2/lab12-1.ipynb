{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80bc46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume dataset is downloaded to './images/' and spec_train_val.txt is in current dir\n",
    "# If not, add download code here, e.g., using tf.keras.utils.get_file or manual download\n",
    "PATH = './images/'\n",
    "annotation_file = 'spec_train_val.txt'\n",
    "\n",
    "# Read the txt file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Store captions and image names in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    image_name = parts[0]\n",
    "    caption = parts[1]\n",
    "    # Preprocess caption to character level with spaces\n",
    "    caption = '<start> ' + ' '.join(list(caption.lower())) + ' <end>'  # assume lowercase\n",
    "    full_image_path = PATH + image_name\n",
    "    if os.path.exists(full_image_path):\n",
    "        all_img_name_vector.append(full_image_path)\n",
    "        all_captions.append(caption)\n",
    "\n",
    "# No shuffle here, as per assignment order\n",
    "# Train: first 100000, val: next 20000\n",
    "num_train = 100000\n",
    "num_val = 20000\n",
    "train_captions = all_captions[:num_train]\n",
    "img_name_vector = all_img_name_vector[:num_train]\n",
    "val_captions = all_captions[num_train:num_train + num_val]\n",
    "img_name_val = all_img_name_vector[num_train:num_train + num_val]\n",
    "\n",
    "# Also keep original labels for accuracy calculation\n",
    "original_train_labels = [''.join(c.split()[1:-1]) for c in train_captions]  # remove <start> <end>\n",
    "original_val_labels = [''.join(c.split()[1:-1]) for c in val_captions]\n",
    "\n",
    "print(len(train_captions), len(img_name_vector))\n",
    "print(len(val_captions), len(img_name_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique images for train\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                                (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())\n",
    "\n",
    "# Repeat for val if needed, but can do on the fly or cache similarly\n",
    "encode_val = sorted(set(img_name_val))\n",
    "image_dataset_val = tf.data.Dataset.from_tensor_slices(encode_val)\n",
    "image_dataset_val = image_dataset_val.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset_val):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                                (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose top_k large enough for characters (a-z ~26 + tokens)\n",
    "top_k = 100\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions + val_captions)  # fit on all for consistent vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4067ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors for train\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634bf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For val\n",
    "val_seqs = tokenizer.texts_to_sequences(val_captions)\n",
    "cap_val = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the max_length, used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train = img_name_vector\n",
    "# No test_split, val is separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_vector), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 64  # smaller for CAPTCHA\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4795c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d524ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_vector))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f06a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        x = self.fc1(output)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        x = self.fc2(x)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9478665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16629e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20  # Adjust as needed for 90% acc\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_loss = 0\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(dataset), total=num_steps):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss / num_steps))\n",
    "print('Time taken for {} epochs {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b770cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    features = encoder(img_tensor_val)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot((len_result//2)+1, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eff4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on val set for accuracy\n",
    "correct = 0\n",
    "for i in range(len(img_name_val)):\n",
    "    image = img_name_val[i]\n",
    "    real_label = original_val_labels[i]\n",
    "    result, _ = evaluate(image)\n",
    "    predicted_label = ''.join(result[:-1])  # remove <end>\n",
    "    if predicted_label == real_label:\n",
    "        correct += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(f'Processed {i}, Accuracy so far: {correct / (i+1):.4f}')\n",
    "print(f'Validation Accuracy: {correct / len(img_name_val):.4f}')\n",
    "\n",
    "# Example\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_label = original_val_labels[rid]\n",
    "result, attention_plot = evaluate(image)\n",
    "predicted_label = ''.join(result[:-1])\n",
    "print('Real Label:', real_label)\n",
    "print('Prediction Label:', predicted_label)\n",
    "plot_attention(image, result, attention_plot)\n",
    "Image.open(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set, assume images 120001 to 140000\n",
    "img_name_test = [PATH + f'{i}.jpg' for i in range(120001, 140001)]  # adjust if filenames differ\n",
    "\n",
    "# Cache test features if not done\n",
    "encode_test = sorted(set(img_name_test))\n",
    "image_dataset_test = tf.data.Dataset.from_tensor_slices(encode_test)\n",
    "image_dataset_test = image_dataset_test.map(\n",
    "    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset_test):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                                (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32164daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test\n",
    "predictions = []\n",
    "for image in img_name_test:\n",
    "    result, _ = evaluate(image)\n",
    "    predicted_label = ''.join(result[:-1])\n",
    "    image_name = os.path.basename(image)\n",
    "    predictions.append(f'{image_name} {predicted_label}')\n",
    "\n",
    "# Write to file\n",
    "student_id = 'your_id'  # replace with your student id\n",
    "with open(f'Lab12-2_{student_id}.txt', 'w') as f:\n",
    "    f.write('\\n'.join(predictions))\n",
    "print('Test predictions saved to Lab12-2_{student_id}.txt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
