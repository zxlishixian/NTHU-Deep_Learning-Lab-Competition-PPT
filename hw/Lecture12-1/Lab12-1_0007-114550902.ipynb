{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34k62fvb1VM-",
        "outputId": "03579b51-53e4-476e-a2ad-39621bd25e59"
      },
      "id": "34k62fvb1VM-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "021ab01a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "021ab01a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f1e55b03",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1e55b03",
        "outputId": "ed362b2b-b459-4271-d359-67e5df54b9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Restrict TensorFlow to only use the first GPU\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8ccf6850",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8ccf6850"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "movie_reviews = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dl/lecture12-1/data/IMDB Dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135d63ed",
      "metadata": {
        "id": "135d63ed"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8ec35f9d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ec35f9d",
        "outputId": "b352d1bb-1d1d-4666-c6fc-c39c16b29213"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.False_"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# check if there is any null value in the dataset\n",
        "movie_reviews.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1d34d6ba",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d34d6ba",
        "outputId": "561b7863-6ae2-44bc-b55c-779ed5b8a040"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# show the size of the dataset\n",
        "movie_reviews.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fce04a72",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fce04a72",
        "outputId": "3045c197-fc64-4b9d-c33b-51e70eba863d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c264d2f6-ab20-4867-842b-612e4b36dc41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c264d2f6-ab20-4867-842b-612e4b36dc41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c264d2f6-ab20-4867-842b-612e4b36dc41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c264d2f6-ab20-4867-842b-612e4b36dc41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f897c899-5ffa-4237-99a0-e71f73f5fc50\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f897c899-5ffa-4237-99a0-e71f73f5fc50')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f897c899-5ffa-4237-99a0-e71f73f5fc50 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "movie_reviews",
              "summary": "{\n  \"name\": \"movie_reviews\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# show the first five data in the dataset\n",
        "movie_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "30b141b7",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "30b141b7",
        "outputId": "d9c3fef8-f037-4576-d448-65e6e693b65f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "movie_reviews[\"review\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ad5429fc",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ad5429fc"
      },
      "outputs": [],
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "\n",
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6bf6ca05",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6bf6ca05"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "sentences = list(movie_reviews['review'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "# replace the positive with 1, replace the negative with 0\n",
        "y = movie_reviews['sentiment']\n",
        "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8e29f819",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e29f819",
        "outputId": "fffcd59c-2889-407a-9d9b-147d0222c287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training data: 40000\n",
            "# test data: 10000\n"
          ]
        }
      ],
      "source": [
        "# Split the training dataset and test dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "53af5d30",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "53af5d30"
      },
      "outputs": [],
      "source": [
        "# 3. Tokenizer（必須在轉序列前！）\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)  # ← 必須是字符串！\n",
        "\n",
        "# 4. 轉成數字序列\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# 5. Padding\n",
        "max_len = 100\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0b95c6a3",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b95c6a3",
        "outputId": "8a1f3197-18b5-45fb-e0d0-0f155c4df380"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,  296,  140, 2854,    2,  405,  614,    1,  263,    5, 3514,\n",
              "        977,    4,   25,   37,   11, 1237,  215,   62,    2,   35,    5,\n",
              "         27,  217,   24,  189, 1430,    7, 1068,   15, 4868,   81,    1,\n",
              "        221,   63,  351,   64,   52,   24,    4, 3547,   13,    6,   19,\n",
              "        192,    4, 8148,  859, 3430, 1720,   17,   23,    4,  158,  194,\n",
              "        175,  106,    9, 1604,  461,   71,  218,    4,  321,    2, 3431,\n",
              "         31,   20,   47,   68, 1844, 4668,   11,    6, 1365,    8,   16,\n",
              "          5, 3475, 1990,   14,   59,    1, 2380,  460,  518,    2,  170,\n",
              "       2524, 2698, 1745,    4,  573,    6,   33,    1, 3750,  198,  345,\n",
              "       3812], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# show the preprocessed data\n",
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "07394828",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07394828",
        "outputId": "e975452e-bffc-4b06-cbfd-a02b6458002b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 100]), TensorShape([128]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "# only reserve 10000 words\n",
        "vocab_size = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "63f3e12e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "63f3e12e"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_activation='sigmoid',\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # x is the training data with shape == (batch_size，max_length)  -> (128, 100)\n",
        "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
        "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
        "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
        "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
        "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
        "        # Unpack the output based on the observed behavior (output sequence, then individual states)\n",
        "        gru_output = self.gru(x, initial_state=hidden)\n",
        "        output = gru_output[0]\n",
        "        state = gru_output[-1]\n",
        "\n",
        "\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "178e2229",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178e2229",
        "outputId": "dda79f44-361f-4bd4-8c25-cfbae8d705a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 100, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (1024,)\n",
            "tf.Tensor(False, shape=(), dtype=bool)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "# the output and the hidden state of GRU is equal\n",
        "print(tf.reduce_all(sample_output[:, -1, :] == sample_hidden))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2381fda1",
        "outputId": "399f6a7d-8333-47aa-f298-e21cb3b723e7"
      },
      "source": [
        "# Test the GRU layer directly\n",
        "gru_test = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')\n",
        "\n",
        "# Create dummy input with expected shape (batch_size, max_len, embedding_dim)\n",
        "dummy_input = tf.random.normal((BATCH_SIZE, max_len, embedding_dim))\n",
        "\n",
        "# Create dummy initial state with expected shape (batch_size, units)\n",
        "dummy_state = tf.zeros((BATCH_SIZE, units))\n",
        "\n",
        "try:\n",
        "    test_output, test_state = gru_test(dummy_input, initial_state=dummy_state)\n",
        "    print(\"GRU test successful!\")\n",
        "    print('GRU output shape: (batch size, sequence length, units) {}'.format(test_output.shape))\n",
        "    print('GRU Hidden state shape: (batch size, units) {}'.format(test_state.shape))\n",
        "except Exception as e:\n",
        "    print(f\"GRU test failed: {e}\")"
      ],
      "id": "2381fda1",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU test failed: too many values to unpack (expected 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "4c37ae59",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4c37ae59"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)  # W_a\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query: (batch, hidden) -> (batch, hidden)\n",
        "        proj_query = self.Wa(query)\n",
        "        # (batch, hidden) -> (batch, hidden, 1) 以便与 values 做时间步矩阵乘\n",
        "        proj_query = tf.expand_dims(proj_query, -1)\n",
        "        # scores[b, t, 1] = h_i^T (W_a s_t) = s_t^T W_a h_i\n",
        "        scores = tf.matmul(values, proj_query)               # (batch, max_len, 1)\n",
        "        attn   = tf.nn.softmax(scores, axis=1)               # (batch, max_len, 1)\n",
        "        context = tf.reduce_sum(attn * values, axis=1)       # (batch, hidden)\n",
        "        return context, tf.squeeze(attn, -1)                 # (batch, hidden), (batch, max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "24ca63f4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "24ca63f4"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "\n",
        "        # pass through four fully connected layers, the model will return\n",
        "        # the probability of the positivity of the sentence\n",
        "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
        "        self.fc_2 = tf.keras.layers.Dense(512)\n",
        "        self.fc_3 = tf.keras.layers.Dense(64)\n",
        "        self.fc_4 = tf.keras.layers.Dense(1)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = LuongAttention(self.dec_units)\n",
        "\n",
        "    def call(self, hidden, enc_output):\n",
        "        # Ensure hidden state has the correct shape (batch_size, units)\n",
        "        if hidden.shape.rank == 1:\n",
        "             hidden = tf.reshape(hidden, (-1, self.dec_units))\n",
        "\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        output = self.fc_1(context_vector)\n",
        "        output = self.fc_2(output)\n",
        "        output = self.fc_3(output)\n",
        "        output = self.fc_4(output)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(units, BATCH_SIZE)\n",
        "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
        "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtjRIYGG9DuG",
        "outputId": "c69cb4b2-e4f6-4f9f-84d8-0d57b04646b3"
      },
      "id": "HtjRIYGG9DuG",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5588631c",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "5588631c"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    loss_ = loss_object(real, pred)\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fff425f8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "fff425f8"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './checkpoints/sentiment-analysis'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "6fac894e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6fac894e"
      },
      "outputs": [],
      "source": [
        "#@tf.function  # Remove the decorator for debugging\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, _ = decoder(enc_hidden, enc_output)\n",
        "\n",
        "        loss = loss_function(targ, predictions)\n",
        "\n",
        "    # collect all trainable variables\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # calculate the gradients for the whole variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # apply the gradients on the variables\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "dc7466d8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc7466d8",
        "outputId": "98fa1cb0-1c8b-4cc1-cfa4-dad3e114b7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.6931\n",
            "Epoch 1 Batch 100 Loss 0.4364\n",
            "Epoch 1 Batch 200 Loss 0.4661\n",
            "Epoch 1 Batch 300 Loss 0.4933\n",
            "Epoch 1 Loss 0.4661\n",
            "Time taken for 1 epoch 80.31165051460266 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2594\n",
            "Epoch 2 Batch 100 Loss 0.2756\n",
            "Epoch 2 Batch 200 Loss 0.2727\n",
            "Epoch 2 Batch 300 Loss 0.3360\n",
            "Epoch 2 Loss 0.3395\n",
            "Time taken for 1 epoch 82.26875066757202 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2093\n",
            "Epoch 3 Batch 100 Loss 0.1859\n",
            "Epoch 3 Batch 200 Loss 0.1786\n",
            "Epoch 3 Batch 300 Loss 0.4830\n",
            "Epoch 3 Loss 0.2622\n",
            "Time taken for 1 epoch 77.13653659820557 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1764\n",
            "Epoch 4 Batch 100 Loss 0.1686\n",
            "Epoch 4 Batch 200 Loss 0.2186\n",
            "Epoch 4 Batch 300 Loss 0.2260\n",
            "Epoch 4 Loss 0.2003\n",
            "Time taken for 1 epoch 77.36123013496399 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1394\n",
            "Epoch 5 Batch 100 Loss 0.1037\n",
            "Epoch 5 Batch 200 Loss 0.1679\n",
            "Epoch 5 Batch 300 Loss 0.2204\n",
            "Epoch 5 Loss 0.1515\n",
            "Time taken for 1 epoch 79.40586066246033 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0943\n",
            "Epoch 6 Batch 100 Loss 0.0975\n",
            "Epoch 6 Batch 200 Loss 0.0440\n",
            "Epoch 6 Batch 300 Loss 0.1516\n",
            "Epoch 6 Loss 0.1077\n",
            "Time taken for 1 epoch 77.6571991443634 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0468\n",
            "Epoch 7 Batch 100 Loss 0.0380\n",
            "Epoch 7 Batch 200 Loss 0.0847\n",
            "Epoch 7 Batch 300 Loss 0.0859\n",
            "Epoch 7 Loss 0.0694\n",
            "Time taken for 1 epoch 76.62860560417175 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0788\n",
            "Epoch 8 Batch 100 Loss 0.0223\n",
            "Epoch 8 Batch 200 Loss 0.0273\n",
            "Epoch 8 Batch 300 Loss 0.0622\n",
            "Epoch 8 Loss 0.0500\n",
            "Time taken for 1 epoch 76.96329808235168 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0079\n",
            "Epoch 9 Batch 100 Loss 0.0497\n",
            "Epoch 9 Batch 200 Loss 0.0244\n",
            "Epoch 9 Batch 300 Loss 0.0462\n",
            "Epoch 9 Loss 0.0368\n",
            "Time taken for 1 epoch 76.6710307598114 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0104\n",
            "Epoch 10 Batch 100 Loss 0.0177\n",
            "Epoch 10 Batch 200 Loss 0.0024\n",
            "Epoch 10 Batch 300 Loss 0.0324\n",
            "Epoch 10 Loss 0.0281\n",
            "Time taken for 1 epoch 76.96258616447449 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# set the epochs for training\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # get the initial hidden state of gru\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "76c444f6",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76c444f6",
        "outputId": "58ab3a18-04d2-4db8-c38a-d0c4b802f0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./checkpoints/sentiment-analysis/ckpt-5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7b519fbeee10>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "585a9255",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "585a9255"
      },
      "outputs": [],
      "source": [
        "#@tf.function # Remove the decorator for debugging\n",
        "def test_step(inp, enc_hidden):\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
        "    return predictions, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f80770f3",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "f80770f3"
      },
      "outputs": [],
      "source": [
        "def evaluate(test_data):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "    for batch, (inp, targ) in enumerate(test_data):\n",
        "        if len(inp) != BATCH_SIZE:\n",
        "            enc_hidden = tf.zeros((len(inp), units))\n",
        "        # make prediction\n",
        "        if batch == 0:\n",
        "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
        "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
        "        else:\n",
        "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
        "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
        "            predictions = np.concatenate((predictions, _predictions))\n",
        "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
        "\n",
        "    predictions = np.squeeze(predictions)\n",
        "    attention_weights = np.squeeze(attention_weights)\n",
        "    predictions[np.where(predictions < 0.5)] = 0\n",
        "    predictions[np.where(predictions >= 0.5)] = 1\n",
        "    return predictions, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2e62e67c",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "2e62e67c"
      },
      "outputs": [],
      "source": [
        "y_pred, attention_weights = evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "8b45ae83",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b45ae83",
        "outputId": "4625182b-5b2d-4a3e-dd88-57695968ce9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.83\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy: ', (y_pred == y_test).sum() / len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "e43471c6",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e43471c6",
        "outputId": "ad8d6834-794e-4ee1-d2d4-5b5939b4e3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true: 1\n",
            "y_predict: 0\n",
            "changed it was terrible main event \u001b[31mjust\u001b[0m \u001b[31mlike\u001b[0m every match \u001b[31mis\u001b[0m \u001b[31min\u001b[0m \u001b[31mis\u001b[0m terrible other matches on the \u001b[31mcard\u001b[0m were razor ramon vs ted brothers vs bodies shawn michaels vs this was the event where shawn named his big monster of body guard vs kid \u001b[31mhart\u001b[0m first takes on then takes on jerry and stuff with the and was always very interesting then destroyed marty undertaker took on giant in another terrible match the \u001b[31msmoking\u001b[0m and took on bam bam and the and the world title against lex this match was \u001b[31mboring\u001b[0m and it has terrible ending however it \u001b[31mdeserves\u001b[0m \n",
            "\n",
            "\n",
            "y_true: 1\n",
            "y_predict: 1\n",
            "of subject matter as are and broken in many \u001b[31mways\u001b[0m on many many issues happened to \u001b[31msee\u001b[0m the pilot premiere in \u001b[31mpassing\u001b[0m and just had to keep in after that to see if would ever get the \u001b[31mgirl\u001b[0m after \u001b[31mseeing\u001b[0m them all on television was delighted to see them available on dvd have to admit that it was the only thing that \u001b[31mkept\u001b[0m me sane whilst had to do hour night shift and developed insomnia farscape was the only thing to get me through those \u001b[31mextremely\u001b[0m long \u001b[31mnights\u001b[0m do yourself favour watch the pilot and \u001b[31msee\u001b[0m \u001b[31mwhat\u001b[0m mean farscape comet \n",
            "\n",
            "\n",
            "y_true: 0\n",
            "y_predict: 0\n",
            "destruction the first really bad thing is the guy steven seagal \u001b[31mwould\u001b[0m have been beaten to \u001b[31mpulp\u001b[0m by seagal \u001b[31mdriving\u001b[0m but that probably would have ended the whole premise for the movie it seems like they decided \u001b[31mto\u001b[0m make all kinds of changes in the movie plot so just plan \u001b[31mto\u001b[0m \u001b[31menjoy\u001b[0m the action and do not expect coherent plot turn any sense of logic \u001b[31myou\u001b[0m may have it will your chance of getting headache does give me some hope that steven seagal \u001b[31mis\u001b[0m trying to move back towards the type of \u001b[31mcharacters\u001b[0m he \u001b[31mportrayed\u001b[0m in his more popular movies \n",
            "\n",
            "\n",
            "y_true: 1\n",
            "y_predict: 1\n",
            "jane austen would \u001b[31mdefinitely\u001b[0m of this one paltrow does an \u001b[31mawesome\u001b[0m \u001b[31mjob\u001b[0m capturing the \u001b[31mattitude\u001b[0m of \u001b[31memma\u001b[0m she is funny without being silly yet elegant she puts on very convincing british accent not being british myself maybe m not the best judge but \u001b[31mshe\u001b[0m fooled me she was also excellent in doors sometimes forget she american also brilliant are \u001b[31mjeremy\u001b[0m northam and sophie thompson and law emma thompson sister and mother as the bates women they nearly \u001b[31msteal\u001b[0m the show and ms law doesn even have any lines \u001b[31mhighly\u001b[0m recommended \n",
            "\n",
            "\n",
            "y_true: 0\n",
            "y_predict: 0\n",
            "reaches the point where they become \u001b[31mobnoxious\u001b[0m and simply frustrating touch football puzzle \u001b[31mfamily\u001b[0m and \u001b[31mtalent\u001b[0m shows are not how actual people behave it almost sickening another big \u001b[31mflaw\u001b[0m \u001b[31mis\u001b[0m the woman carell is supposed to be falling for her in her first scene with steve carell is like watching \u001b[31mstroke\u001b[0m victim trying to be what \u001b[31mimagine\u001b[0m is supposed to be unique and original \u001b[31min\u001b[0m this woman comes off as mildly retarded it makes me think that this movie is taking place on another planet left the theater wondering what \u001b[31mjust\u001b[0m \u001b[31msaw\u001b[0m after thinking further don think it was much \n",
            "\n",
            "\n",
            "y_true: 1\n",
            "y_predict: 1\n",
            "the pace \u001b[31mquick\u001b[0m and energetic but most importantly he knows how to make comedy funny he doesn the jokes and he understands that funny actors know what they re doing and he allows them to do it but segal goes step further he \u001b[31mgives\u001b[0m \u001b[31mtommy\u001b[0m \u001b[31mboy\u001b[0m \u001b[31mfriendly\u001b[0m almost nostalgic tone \u001b[31mthat\u001b[0m \u001b[31mboth\u001b[0m the genuinely and the critics didn like tommy \u001b[31mboy\u001b[0m shame on them movie doesn have to be super sophisticated or \u001b[31mintellectual\u001b[0m to be funny god farley and spade \u001b[31mwere\u001b[0m forced to do muted comedy la the office this is great movie and one of my all time favorites \n",
            "\n",
            "\n",
            "y_true: 1\n",
            "y_predict: 1\n",
            "for once story of hope over the tragic reality our youth face rising draws one into scary and unfair world and \u001b[31mshows\u001b[0m through beautiful color \u001b[31mand\u001b[0m \u001b[31mmoving\u001b[0m \u001b[31mmusic\u001b[0m how one man and his \u001b[31mdedicated\u001b[0m friends choose not to accept that world and change it through action and art \u001b[31man\u001b[0m \u001b[31mentertaining\u001b[0m \u001b[31minteresting\u001b[0m \u001b[31memotional\u001b[0m beautiful film showed this film to numerous high school students as well who all live in with poverty and and \u001b[31mgun\u001b[0m violence and they were with anderson the protagonist recommend this film to all ages over due to subtitles and some images of death from all backgrounds \n",
            "\n",
            "\n",
            "y_true: 1\n",
            "y_predict: 1\n",
            "people and sleeping around that he kept \u001b[31msecret\u001b[0m \u001b[31mfrom\u001b[0m \u001b[31mmost\u001b[0m people he feels free to have an affair with quasi because he kevin he figures out that \u001b[31mhe\u001b[0m can \u001b[31mfool\u001b[0m some people with cards like hotel \u001b[31mbut\u001b[0m it won get him out of those the of \u001b[31mheaven\u001b[0m are keeping track of him \u001b[31mand\u001b[0m \u001b[31meverything\u001b[0m he does after reading all the theories on though it seems like identity is reminder of the different paths tony could ve taken in his life possibly along with the car joke involving that made no sense to me otherwise at that point my \u001b[31mbrain\u001b[0m out \n",
            "\n",
            "\n",
            "y_true: 0\n",
            "y_predict: 0\n",
            "over again can remember how many \u001b[31mtimes\u001b[0m he said the universe is made out of tiny little strings it like they were trying to us into just accepting \u001b[31mare\u001b[0m the best thing since bread \u001b[31mfinally\u001b[0m \u001b[31mthe\u001b[0m show ended off with an unpleasant sense of competition between \u001b[31mand\u001b[0m clearly \u001b[31mbiased\u001b[0m towards this is supposed \u001b[31mto\u001b[0m be an educational program about quantum physics not about whether the us is better than europe or vice versa also felt that \u001b[31mwas\u001b[0m \u001b[31mpart\u001b[0m of the audiences need to see some conflict to remain interested please give me little more credit than that \u001b[31moverall\u001b[0m thumbs down \n",
            "\n",
            "\n",
            "y_true: 0\n",
            "y_predict: 0\n",
            "the scenes involving joe character in particular the scenes in the terribly clich but still funny rich but screwed \u001b[31mup\u001b[0m characters house where the story towards it final moments can see how was \u001b[31mgreat\u001b[0m stage play and while the film \u001b[31mmakers\u001b[0m did their best to translate this to \u001b[31mcelluloid\u001b[0m it \u001b[31msimply\u001b[0m didn work and while laughed out loud at some of scenes and one liners think the first minutes \u001b[31mmy\u001b[0m senses and expectations to such \u001b[31mdegree\u001b[0m would have \u001b[31mlaughed\u001b[0m \u001b[31mat\u001b[0m anything unless you re stuck for \u001b[31mnovelty\u001b[0m coffee coaster don pick this up if you see it in bargain bucket \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from termcolor import colored\n",
        "for idx, data in enumerate(X_test[:10]):\n",
        "    print('y_true: {:d}'.format(y_test[idx]))\n",
        "    print('y_predict: {:.0f}'.format(y_pred[idx]))\n",
        "\n",
        "    # get the twenty most largest attention weights\n",
        "    large_weights_idx = np.argsort(attention_weights[idx])[::-1][:10]\n",
        "\n",
        "    for _idx in range(len(data)):\n",
        "        word_idx = data[_idx]\n",
        "        if word_idx != 0:\n",
        "            if _idx in large_weights_idx:\n",
        "                #print(colored(tokenizer.index_word[word_idx], 'red'), end=' ')\n",
        "                # try this if termcolor is not working properly\n",
        "                print(f'\\033[31m{tokenizer.index_word[word_idx]}\\033[0m', end=' ')\n",
        "            else:\n",
        "                print(tokenizer.index_word[word_idx], end=' ')\n",
        "    print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}